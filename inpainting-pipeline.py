"""
Inpainting Pipeline for Face Composition
ìë™ ì–¼êµ´ í•©ì„± - ë§ˆìŠ¤í¬ ìë™ ìƒì„± (ë¨¸ë¦¬ì¹´ë½ í¬í•¨)
í•„ìš”í•œ ê²ƒ: ë°°ê²½ ì´ë¯¸ì§€ + í•©ì„±í•  ì–¼êµ´ ì´ë¯¸ì§€ (2ê°œë§Œ!)

v2: BiSeNet ê¸°ë°˜ face+hair ë§ˆìŠ¤í‚¹ ì¶”ê°€
v3: IP-Adapter FaceID ì§€ì› (ì •ì²´ì„± ë³´ì¡´)
v4: Dual IP-Adapter ì‹œë„ (ì‹¤íŒ¨ - diffusers í•œê³„)
v5: CLIP Blending ëª¨ë“œ ì¶”ê°€ - ì–¼êµ´/ë¨¸ë¦¬ì¹´ë½ CLIP ì„ë² ë”© ë¸”ë Œë”©
"""

import torch
from diffusers import AutoPipelineForInpainting
from PIL import Image, ImageFilter, ImageOps
import numpy as np
import cv2
import argparse
import os
import shutil
import sys
import random
from datetime import datetime


def get_input_path(input_path: str) -> str:
    """ì…ë ¥ ê²½ë¡œ ì²˜ë¦¬ - inputs/ í´ë” ìë™ í™•ì¸

    íŒŒì¼ì´ í˜„ì¬ ìœ„ì¹˜ì— ì—†ìœ¼ë©´ inputs/ í´ë”ì—ì„œ ì°¾ìŒ
    """
    # ì´ë¯¸ ì¡´ì¬í•˜ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    if os.path.exists(input_path):
        return input_path

    # ì ˆëŒ€ ê²½ë¡œë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
    if os.path.isabs(input_path):
        return input_path

    # ìŠ¤í¬ë¦½íŠ¸ ìœ„ì¹˜ ê¸°ì¤€ìœ¼ë¡œ inputs í´ë” í™•ì¸
    script_dir = os.path.dirname(os.path.abspath(__file__))
    inputs_dir = os.path.join(script_dir, "inputs")
    inputs_path = os.path.join(inputs_dir, input_path)

    if os.path.exists(inputs_path):
        return inputs_path

    # ëª» ì°¾ìœ¼ë©´ ì›ë³¸ ê²½ë¡œ ë°˜í™˜ (ì—ëŸ¬ëŠ” ë‚˜ì¤‘ì— ì²˜ë¦¬)
    return input_path


def setup_run_folder(run_name: str = None) -> str:
    """ì‹¤í–‰ í´ë” ìƒì„±

    ê° ì‹¤í–‰ë§ˆë‹¤ í•˜ë‚˜ì˜ í´ë”ê°€ ìƒì„±ë¨:
    - outputs/run_name_timestamp/ ë˜ëŠ”
    - outputs/timestamp/ (run_name ë¯¸ì§€ì •ì‹œ)

    Returns:
        í´ë” ê²½ë¡œ
    """
    script_dir = os.path.dirname(os.path.abspath(__file__))
    outputs_dir = os.path.join(script_dir, "outputs")

    # íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„± (YYYYMMDD_HHMMSS)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # í´ë”ëª… ê²°ì •
    if run_name:
        # í™•ì¥ì ì œê±° (ì‚¬ìš©ìê°€ .png ë“±ì„ ë¶™ì˜€ì„ ê²½ìš°)
        if '.' in run_name:
            run_name = run_name.rsplit('.', 1)[0]
        folder_name = f"{run_name}_{timestamp}"
    else:
        folder_name = f"run_{timestamp}"

    run_folder = os.path.join(outputs_dir, folder_name)
    os.makedirs(run_folder, exist_ok=True)

    return run_folder


def save_run_params(run_folder: str, args, command: str, actual_seed: int,
                    background_path: str, face_path: str, actual_prompt: str = None):
    """ì‹¤í–‰ íŒŒë¼ë¯¸í„°ë¥¼ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥

    Args:
        run_folder: ì‹¤í–‰ í´ë” ê²½ë¡œ
        args: argparse ê²°ê³¼
        command: ì‹¤ì œ ì‹¤í–‰í•œ ëª…ë ¹ì–´
        actual_seed: ì‹¤ì œ ì‚¬ìš©ëœ ì‹œë“œ (ëœë¤ ìƒì„±ëœ ê²½ìš° í¬í•¨)
        background_path: ë°°ê²½ ì´ë¯¸ì§€ ê²½ë¡œ
        face_path: ì–¼êµ´ ì´ë¯¸ì§€ ê²½ë¡œ
        actual_prompt: ì‹¤ì œ ì‚¬ìš©ëœ í”„ë¡¬í”„íŠ¸ (auto-prompt ì‹œ ìƒì„±ëœ í”„ë¡¬í”„íŠ¸)
    """
    params_path = os.path.join(run_folder, "params.txt")

    with open(params_path, 'w', encoding='utf-8') as f:
        f.write("=" * 70 + "\n")
        f.write("Inpainting Pipeline - Run Parameters\n")
        f.write("=" * 70 + "\n\n")

        # ì‹¤í–‰ ì‹œê°„
        f.write(f"ì‹¤í–‰ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")

        # ì‹¤ì œ ëª…ë ¹ì–´
        f.write("[ ì‹¤í–‰ ëª…ë ¹ì–´ ]\n")
        f.write(f"{command}\n\n")

        # ì…ë ¥ íŒŒì¼
        f.write("[ ì…ë ¥ íŒŒì¼ ]\n")
        f.write(f"ë°°ê²½ ì´ë¯¸ì§€: {background_path}\n")
        f.write(f"ì–¼êµ´ ì´ë¯¸ì§€: {face_path}\n\n")

        # ëª¨ë“  íŒŒë¼ë¯¸í„°
        f.write("[ íŒŒë¼ë¯¸í„° ]\n")
        f.write(f"seed: {actual_seed}  # ì¬í˜„ì— í•„ìˆ˜!\n")
        f.write(f"face_strength: {args.face_strength}\n")
        f.write(f"denoising: {args.denoising}\n")
        f.write(f"guidance: {args.guidance}\n")
        f.write(f"steps: {args.steps}\n")
        f.write(f"mask_expand: {args.mask_expand}\n")
        f.write(f"mask_blur: {args.mask_blur}\n")
        f.write(f"mask_padding: {args.mask_padding}\n")
        # ì‹¤ì œ ì‚¬ìš©ëœ í”„ë¡¬í”„íŠ¸ ì €ì¥ (auto-promptë©´ ìƒì„±ëœ ê²ƒ, ì•„ë‹ˆë©´ ì›ë³¸)
        used_prompt = actual_prompt if actual_prompt else args.prompt
        f.write(f"prompt: {used_prompt}\n\n")

        # ëª¨ë“œ ì„¤ì •
        f.write("[ ëª¨ë“œ ì„¤ì • ]\n")
        f.write(f"use_faceid: {args.use_faceid}\n")
        f.write(f"use_faceid_plus: {args.use_faceid_plus}\n")
        f.write(f"use_dual_adapter: {args.use_dual_adapter}\n")
        f.write(f"use_clip_blend: {args.use_clip_blend}\n")
        f.write(f"detection: {args.detection}\n")
        f.write(f"no_bisenet: {args.no_bisenet}\n")
        f.write(f"no_hair: {args.no_hair}\n")
        f.write(f"include_neck: {args.include_neck}\n")
        f.write(f"no_gender_detect: {args.no_gender_detect}\n")
        f.write(f"use_background_size: {args.use_background_size}\n")
        f.write(f"stop_at: {args.stop_at}\n")
        f.write(f"auto_prompt: {args.auto_prompt}\n\n")

        # CLIP Blending íŒŒë¼ë¯¸í„°
        f.write("[ CLIP Blending ]\n")
        f.write(f"face_blend_weight: {args.face_blend_weight}\n")
        f.write(f"hair_blend_weight: {args.hair_blend_weight}\n\n")

        # ì¬í˜„ ëª…ë ¹ì–´
        f.write("=" * 70 + "\n")
        f.write("[ ì¬í˜„ ëª…ë ¹ì–´ ]\n")
        f.write("=" * 70 + "\n")
        reproduce_cmd = (
            f"python inpainting-pipeline.py {os.path.basename(background_path)} {os.path.basename(face_path)} "
            f"--face-strength {args.face_strength} "
            f"--denoising {args.denoising} "
            f"--guidance {args.guidance} "
            f"--steps {args.steps} "
            f"--mask-padding {args.mask_padding} "
            f"--seed {actual_seed} "
            f"--prompt \"{args.prompt}\""
        )
        if args.use_faceid_plus:
            reproduce_cmd += " --use-faceid-plus"
        elif args.use_faceid:
            reproduce_cmd += " --use-faceid"
        if args.no_gender_detect:
            reproduce_cmd += " --no-gender-detect"
        if args.no_hair:
            reproduce_cmd += " --no-hair"
        if args.include_neck:
            reproduce_cmd += " --include-neck"
        if args.stop_at < 1.0:
            reproduce_cmd += f" --stop-at {args.stop_at}"
        if args.auto_prompt:
            reproduce_cmd += " --auto-prompt"

        f.write(f"{reproduce_cmd}\n")

    print(f"   íŒŒë¼ë¯¸í„° ì €ì¥: {params_path}")

# BiSeNet face parser (optional, for hair-inclusive masks)
try:
    from face_parsing import FaceParser
    HAS_FACE_PARSER = True
except ImportError:
    HAS_FACE_PARSER = False
    print("face_parsing.py not found. BiSeNet hair masking unavailable.")

# FaceID module (optional, for better identity preservation)
try:
    from face_id import FaceIDExtractor, FaceIDIPAdapter, check_insightface_available
    HAS_FACEID = check_insightface_available()
    if not HAS_FACEID:
        print("InsightFace not installed. FaceID mode unavailable.")
        print("Install: pip install insightface onnxruntime")
except ImportError:
    HAS_FACEID = False
    print("face_id.py not found. FaceID mode unavailable.")

# Gemini Vision í”„ë¡¬í”„íŠ¸ ìƒì„±ê¸° (optional)
try:
    from prompt_generator import generate_prompt_from_face_image
    HAS_PROMPT_GENERATOR = True
except ImportError:
    HAS_PROMPT_GENERATOR = False


def get_device():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ìµœì ì˜ ë””ë°”ì´ìŠ¤ ë°˜í™˜"""
    if torch.cuda.is_available():
        return "cuda"
    elif torch.backends.mps.is_available():
        return "mps"
    else:
        return "cpu"


class AutoIDPhotoCompositor:
    """ìë™ ì–¼êµ´ ê°ì§€ + í•©ì„± (ë¨¸ë¦¬ì¹´ë½ í¬í•¨, FaceID ì§€ì›, CLIP Blending)"""

    def __init__(self, detection_method='opencv', use_bisenet=True, use_faceid=False,
                 use_dual_adapter=False, use_clip_blend=False, use_faceid_plus=False):
        """
        íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”

        Args:
            detection_method: 'opencv' or 'mediapipe'
            use_bisenet: BiSeNet ì‚¬ìš© ì—¬ë¶€ (ë¨¸ë¦¬ì¹´ë½ ë§ˆìŠ¤í‚¹)
            use_faceid: FaceID ëª¨ë“œ ì‚¬ìš© ì—¬ë¶€ (ì •ì²´ì„± ë³´ì¡´ í–¥ìƒ)
            use_dual_adapter: Dual IP-Adapter ëª¨ë“œ (FaceID + CLIP for hair transfer)
            use_clip_blend: CLIP Blending ëª¨ë“œ (ì–¼êµ´+ë¨¸ë¦¬ì¹´ë½ CLIP ì„ë² ë”© ë¸”ë Œë”©)
        """
        print("=" * 70)
        print("Inpainting Pipeline v5")
        print("=" * 70)

        # ë””ë°”ì´ìŠ¤ ê°ì§€
        self.device = get_device()
        print(f"ë””ë°”ì´ìŠ¤: {self.device}")

        # ëª¨ë“œ ì„¤ì •
        # Dual adapter requires both FaceID and CLIP
        self.use_dual_adapter = use_dual_adapter and HAS_FACEID
        self.use_faceid = (use_faceid or use_dual_adapter or use_faceid_plus) and HAS_FACEID
        self.use_faceid_plus = use_faceid_plus and HAS_FACEID  # FaceID Plus v2 (ì–¼êµ´+ë¨¸ë¦¬ìŠ¤íƒ€ì¼)
        self.use_clip_blend = use_clip_blend  # CLIP Blending mode

        if use_clip_blend:
            self.ip_adapter_mode = "clip_blend"  # CLIP embedding blending
        elif use_faceid_plus:
            self.ip_adapter_mode = "faceid_plus"  # FaceID Plus v2 (InsightFace + CLIP)
        elif use_dual_adapter:
            self.ip_adapter_mode = "dual"  # FaceID + CLIP
        elif self.use_faceid:
            self.ip_adapter_mode = "faceid"
        else:
            self.ip_adapter_mode = "standard"

        if (use_faceid or use_dual_adapter) and not HAS_FACEID:
            print("FaceID ìš”ì²­ë˜ì—ˆìœ¼ë‚˜ InsightFace ë¯¸ì„¤ì¹˜. Standard ëª¨ë“œë¡œ ì „í™˜.")

        # BiSeNet face parser ì´ˆê¸°í™” (ë¨¸ë¦¬ì¹´ë½ ë§ˆìŠ¤í‚¹ìš©)
        self.face_parser = None
        self.use_bisenet = use_bisenet and HAS_FACE_PARSER
        if self.use_bisenet:
            try:
                self.face_parser = FaceParser(device=self.device)
                print("BiSeNet face parser ì¤€ë¹„ ì™„ë£Œ")
            except Exception as e:
                print(f"BiSeNet ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.use_bisenet = False

        # FaceID extractor ì´ˆê¸°í™” (ì •ì²´ì„± ë³´ì¡´ìš©)
        self.face_id_extractor = None
        if self.use_faceid:
            try:
                self.face_id_extractor = FaceIDExtractor(device=self.device)
                if self.face_id_extractor.load():
                    print("FaceID extractor ì¤€ë¹„ ì™„ë£Œ (InsightFace)")
                else:
                    print("FaceID ë¡œë”© ì‹¤íŒ¨, Standard ëª¨ë“œë¡œ ì „í™˜")
                    self.use_faceid = False
                    self.ip_adapter_mode = "standard"
            except Exception as e:
                print(f"FaceID ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.use_faceid = False
                self.ip_adapter_mode = "standard"

        # dtype ì„¤ì • (CPUëŠ” float32 ì‚¬ìš©)
        self.dtype = torch.float32 if self.device == "cpu" else torch.float16

        # Inpainting íŒŒì´í”„ë¼ì¸
        print("\nSDXL Inpainting ëª¨ë¸ ë¡œë”© ì¤‘...")
        self.pipeline = AutoPipelineForInpainting.from_pretrained(
            "diffusers/stable-diffusion-xl-1.0-inpainting-0.1",
            torch_dtype=self.dtype,
            variant="fp16" if self.dtype == torch.float16 else None
        )

        # IP-Adapter ë¡œë“œ (ëª¨ë“œì— ë”°ë¼ ë‹¤ë¥¸ ì–´ëŒ‘í„°)
        self.has_ip_adapter = self._load_ip_adapter()
        if not self.has_ip_adapter:
            return

        self.pipeline.to(self.device)

        # xFormersëŠ” CUDAì—ì„œë§Œ ì‚¬ìš©
        if self.device == "cuda":
            try:
                self.pipeline.enable_xformers_memory_efficient_attention()
                print("xFormers ë©”ëª¨ë¦¬ ìµœì í™” í™œì„±í™”")
            except:
                pass

        # ì–¼êµ´ ê°ì§€ ì´ˆê¸°í™”
        self.detection_method = detection_method
        self._init_face_detection()

        # ëª¨ë“œ ì •ë³´ ì¶œë ¥
        print(f"\ní˜„ì¬ ëª¨ë“œ: {self.ip_adapter_mode.upper()}")
        if self.use_clip_blend:
            print("  - CLIP Blending Mode")
            print("  - ì–¼êµ´/ë¨¸ë¦¬ì¹´ë½ ì˜ì—­ ë³„ë„ CLIP ì¸ì½”ë”©")
            print("  - ê°€ì¤‘ì¹˜ ë¸”ë Œë”©ìœ¼ë¡œ ë‘ íŠ¹ì„± ë™ì‹œ ë°˜ì˜")
            print("  - ì •ì²´ì„± + ë¨¸ë¦¬ì¹´ë½ ìŠ¤íƒ€ì¼ ì „ì´")
        elif self.use_faceid_plus:
            print("  - IP-Adapter FaceID Plus v2")
            print("  - InsightFace 512-dim ì–¼êµ´ ì„ë² ë”© (ì •ì²´ì„±)")
            print("  - CLIP 1024-dim ì´ë¯¸ì§€ ì„ë² ë”© (ë¨¸ë¦¬ìŠ¤íƒ€ì¼)")
            print("  - ì–¼êµ´ + ë¨¸ë¦¬ìŠ¤íƒ€ì¼ ë™ì‹œ ë°˜ì˜!")
        elif self.use_dual_adapter:
            print("  - Dual IP-Adapter (FaceID + CLIP)")
            print("  - InsightFace 512-dim ì–¼êµ´ ì„ë² ë”© (ì •ì²´ì„±)")
            print("  - CLIP ë¨¸ë¦¬ì¹´ë½ ì´ë¯¸ì§€ ì„ë² ë”© (ìŠ¤íƒ€ì¼)")
            print("  - ì–¼êµ´ ì •ì²´ì„± + ë¨¸ë¦¬ì¹´ë½ ìŠ¤íƒ€ì¼ ë™ì‹œ ì „ì´")
        elif self.use_faceid:
            print("  - IP-Adapter FaceID (InsightFace)")
            print("  - InsightFace 512-dim ì–¼êµ´ ì„ë² ë”© ì‚¬ìš©")
            print("  - ì •ì²´ì„± ë³´ì¡´ í–¥ìƒ (ë¨¸ë¦¬ìŠ¤íƒ€ì¼ ë¯¸ë°˜ì˜)")
        else:
            print("  - Standard IP-Adapter")
            print("  - CLIP ì„ë² ë”©ë§Œ ì‚¬ìš©")
            print("  - ì •ì²´ì„± ë³´ì¡´ ì œí•œì ")
        print("=" * 70)

    def _load_ip_adapter(self) -> bool:
        """IP-Adapter ë¡œë“œ (ëª¨ë“œì— ë”°ë¼ Standard, FaceID, Dual, ë˜ëŠ” CLIP Blend)"""
        try:
            if self.use_clip_blend:
                # CLIP Blending: Standard IP-Adapter ë¡œë“œ + CLIP ì¸ì½”ë” ì €ì¥
                print("CLIP Blending ëª¨ë“œ: Standard IP-Adapter ë¡œë”© ì¤‘...")
                self.pipeline.load_ip_adapter(
                    "h94/IP-Adapter",
                    subfolder="sdxl_models",
                    weight_name="ip-adapter_sdxl.bin"
                )
                # CLIP ì¸ì½”ë” ì €ì¥ (ìˆ˜ë™ ì„ë² ë”© ì¶”ì¶œìš©)
                self.clip_image_encoder = self.pipeline.image_encoder
                self.clip_image_processor = self.pipeline.feature_extractor
                print("CLIP Blending: Standard IP-Adapter + CLIP ì¸ì½”ë” ì¤€ë¹„ ì™„ë£Œ!")

            elif self.use_dual_adapter:
                # Dual IP-Adapter: Standard (ë¨¸ë¦¬ì¹´ë½ CLIP) + FaceID (ì–¼êµ´)
                # diffusersëŠ” ë¦¬ìŠ¤íŠ¸ë¡œ multiple IP-Adapter ë¡œë”© ì§€ì›
                print("Dual IP-Adapter ë¡œë”© ì¤‘ (Standard + FaceID)...")

                # ë‘ ì–´ëŒ‘í„°ë¥¼ í•œ ë²ˆì— ë¡œë“œ (ë¦¬ìŠ¤íŠ¸ í˜•ì‹)
                self.pipeline.load_ip_adapter(
                    ["h94/IP-Adapter", "h94/IP-Adapter-FaceID"],
                    subfolder=["sdxl_models", ""],
                    weight_name=["ip-adapter_sdxl.bin", "ip-adapter-faceid_sdxl.bin"],
                )
                print("  [1] Standard IP-Adapter (CLIP) ë¡œë”©")
                print("  [2] IP-Adapter FaceID ë¡œë”©")

                # CLIP image encoder ì €ì¥
                self.clip_image_encoder = self.pipeline.image_encoder
                self.clip_image_processor = self.pipeline.feature_extractor

                # ë‘ ì–´ëŒ‘í„°ì˜ ìŠ¤ì¼€ì¼ ì„¤ì • [Standard(hair), FaceID(face)]
                self.pipeline.set_ip_adapter_scale([0.3, 0.6])
                print("Dual IP-Adapter ë¡œë”© ì™„ë£Œ! (scales: hair=0.3, face=0.6)")

            elif self.use_faceid_plus:
                # FaceID Plus v2: InsightFace + CLIP ì´ë¯¸ì§€ ì„ë² ë”© (ë¨¸ë¦¬ìŠ¤íƒ€ì¼ í¬í•¨)
                print("IP-Adapter FaceID Plus v2 ë¡œë”© ì¤‘...")

                # CLIP ì´ë¯¸ì§€ ì¸ì½”ë” ë¡œë“œ (Plus v2 í•„ìˆ˜)
                from transformers import CLIPVisionModelWithProjection
                self.clip_image_encoder = CLIPVisionModelWithProjection.from_pretrained(
                    "laion/CLIP-ViT-H-14-laion2B-s32B-b79K",
                    torch_dtype=self.dtype,
                ).to(self.device)
                print("  CLIP ì´ë¯¸ì§€ ì¸ì½”ë” ë¡œë“œ ì™„ë£Œ")

                # IP-Adapter FaceID Plus v2 ë¡œë“œ
                self.pipeline.load_ip_adapter(
                    "h94/IP-Adapter-FaceID",
                    subfolder="",
                    weight_name="ip-adapter-faceid-plusv2_sdxl.bin",
                    image_encoder_folder=None,  # ì´ë¯¸ ë³„ë„ë¡œ ë¡œë“œí•¨
                )

                # shortcut ì„¤ì • (Plus v2 í•„ìˆ˜)
                self.pipeline.unet.encoder_hid_proj.image_projection_layers[0].shortcut = True
                print("IP-Adapter FaceID Plus v2 ë¡œë”© ì™„ë£Œ! (ì–¼êµ´+ë¨¸ë¦¬ìŠ¤íƒ€ì¼)")
            elif self.use_faceid:
                # FaceID (non-Plus): InsightFace ì„ë² ë”©ë§Œ ì‚¬ìš©
                print("IP-Adapter FaceID ë¡œë”© ì¤‘...")
                self.pipeline.load_ip_adapter(
                    "h94/IP-Adapter-FaceID",
                    subfolder="",
                    weight_name="ip-adapter-faceid_sdxl.bin",
                    image_encoder_folder=None,
                )
                print("IP-Adapter FaceID ë¡œë”© ì™„ë£Œ!")
            else:
                # Standard IP-Adapter (CLIP only)
                print("Standard IP-Adapter ë¡œë”© ì¤‘...")
                self.pipeline.load_ip_adapter(
                    "h94/IP-Adapter",
                    subfolder="sdxl_models",
                    weight_name="ip-adapter_sdxl.bin"
                )
                print("Standard IP-Adapter ë¡œë”© ì™„ë£Œ!")
            return True
        except Exception as e:
            print(f"IP-Adapter ë¡œë”© ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return False

    def switch_to_faceid(self, scale: float = 0.85) -> bool:
        """
        FaceID ëª¨ë“œë¡œ ì „í™˜ (ëŸ°íƒ€ì„ ì „í™˜)

        Args:
            scale: IP-Adapter scale

        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        if not HAS_FACEID:
            print("InsightFaceê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return False

        if self.ip_adapter_mode == "faceid":
            print("ì´ë¯¸ FaceID ëª¨ë“œì…ë‹ˆë‹¤.")
            return True

        try:
            # FaceID extractor ì´ˆê¸°í™”
            if self.face_id_extractor is None:
                self.face_id_extractor = FaceIDExtractor(device=self.device)
                if not self.face_id_extractor.load():
                    return False

            # IP-Adapter êµì²´ (FaceID)
            print("IP-Adapter FaceIDë¡œ ì „í™˜ ì¤‘...")
            self.pipeline.load_ip_adapter(
                "h94/IP-Adapter-FaceID",
                subfolder="",
                weight_name="ip-adapter-faceid_sdxl.bin",
            )
            self.pipeline.set_ip_adapter_scale(scale)

            self.use_faceid = True
            self.ip_adapter_mode = "faceid"
            print("FaceID Plus v2 ëª¨ë“œë¡œ ì „í™˜ ì™„ë£Œ!")
            return True

        except Exception as e:
            print(f"FaceID ì „í™˜ ì‹¤íŒ¨: {e}")
            return False

    def switch_to_standard(self, scale: float = 0.85) -> bool:
        """
        Standard ëª¨ë“œë¡œ ì „í™˜ (ëŸ°íƒ€ì„ ì „í™˜)

        Args:
            scale: IP-Adapter scale

        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        if self.ip_adapter_mode == "standard":
            print("ì´ë¯¸ Standard ëª¨ë“œì…ë‹ˆë‹¤.")
            return True

        try:
            print("Standard IP-Adapterë¡œ ì „í™˜ ì¤‘...")
            self.pipeline.load_ip_adapter(
                "h94/IP-Adapter",
                subfolder="sdxl_models",
                weight_name="ip-adapter_sdxl.bin"
            )
            self.pipeline.set_ip_adapter_scale(scale)

            self.use_faceid = False
            self.ip_adapter_mode = "standard"
            print("Standard ëª¨ë“œë¡œ ì „í™˜ ì™„ë£Œ!")
            return True

        except Exception as e:
            print(f"Standard ì „í™˜ ì‹¤íŒ¨: {e}")
            return False

    def get_current_mode(self) -> str:
        """í˜„ì¬ IP-Adapter ëª¨ë“œ ë°˜í™˜"""
        return self.ip_adapter_mode

    def _create_face_hair_composite(
        self,
        source_face: Image.Image,
        hair_region: Image.Image,
        face_weight: float = 0.6,
        hair_weight: float = 0.4
    ) -> Image.Image:
        """
        ì–¼êµ´ê³¼ ë¨¸ë¦¬ì¹´ë½ ì˜ì—­ì„ ê°€ì¤‘ì¹˜ ë¸”ë Œë”©í•˜ì—¬ í•©ì„± ì´ë¯¸ì§€ ìƒì„±

        BiSeNetìœ¼ë¡œ ì¶”ì¶œí•œ ì–¼êµ´/ë¨¸ë¦¬ì¹´ë½ ì˜ì—­ì„ ê¸°ë°˜ìœ¼ë¡œ
        ì–¼êµ´ íŠ¹ì§•ê³¼ ë¨¸ë¦¬ì¹´ë½ ìŠ¤íƒ€ì¼ì„ ëª¨ë‘ ê°•ì¡°í•œ í•©ì„± ì´ë¯¸ì§€ ìƒì„±

        Args:
            source_face: ì›ë³¸ ì–¼êµ´ ì´ë¯¸ì§€
            hair_region: BiSeNetìœ¼ë¡œ ì¶”ì¶œí•œ ë¨¸ë¦¬ì¹´ë½ ì˜ì—­ ì´ë¯¸ì§€
            face_weight: ì–¼êµ´ ì˜ì—­ ê°€ì¤‘ì¹˜ (ê¸°ë³¸: 0.6)
            hair_weight: ë¨¸ë¦¬ì¹´ë½ ì˜ì—­ ê°€ì¤‘ì¹˜ (ê¸°ë³¸: 0.4)

        Returns:
            í•©ì„±ëœ ì´ë¯¸ì§€ (PIL Image)
        """
        # numpy ë°°ì—´ë¡œ ë³€í™˜
        face_array = np.array(source_face).astype(np.float32)
        hair_array = np.array(hair_region).astype(np.float32)

        # ë¨¸ë¦¬ì¹´ë½ ì˜ì—­ ë§ˆìŠ¤í¬ ìƒì„± (íšŒìƒ‰(128)ì´ ì•„ë‹Œ ì˜ì—­ì´ ë¨¸ë¦¬ì¹´ë½)
        hair_mask = np.any(np.abs(hair_array - 128) > 20, axis=2).astype(np.float32)

        # ê°€ì¤‘ì¹˜ ì •ê·œí™” (hair_weightë§Œ ì‚¬ìš©)
        total = face_weight + hair_weight
        hair_w = hair_weight / total

        # ë§ˆìŠ¤í¬ í™•ì¥ (3ì±„ë„)
        hair_mask_3d = hair_mask[:, :, np.newaxis]

        # ê°€ì¤‘ì¹˜ ë¸”ë Œë”©:
        # - ë¨¸ë¦¬ì¹´ë½ ì˜ì—­ì—ì„œ ë¨¸ë¦¬ì¹´ë½ ì´ë¯¸ì§€ì˜ ìƒ‰ìƒ/ì§ˆê°ì„ ë” ë°˜ì˜
        # - ì–¼êµ´ ì˜ì—­ì€ ì›ë³¸ ìœ ì§€
        blended = face_array.copy()
        blended = blended * (1 - hair_mask_3d * hair_w) + hair_array * (hair_mask_3d * hair_w)

        # í´ë¦¬í•‘ ë° ë³€í™˜
        blended = np.clip(blended, 0, 255).astype(np.uint8)

        return Image.fromarray(blended)

    def _blend_clip_embeddings(
        self,
        face_embeds: torch.Tensor,
        hair_embeds: torch.Tensor,
        face_weight: float = 0.6,
        hair_weight: float = 0.4
    ) -> torch.Tensor:
        """
        ì–¼êµ´ê³¼ ë¨¸ë¦¬ì¹´ë½ CLIP ì„ë² ë”© ë¸”ë Œë”©

        Args:
            face_embeds: ì–¼êµ´ CLIP ì„ë² ë”©
            hair_embeds: ë¨¸ë¦¬ì¹´ë½ CLIP ì„ë² ë”©
            face_weight: ì–¼êµ´ ê°€ì¤‘ì¹˜
            hair_weight: ë¨¸ë¦¬ì¹´ë½ ê°€ì¤‘ì¹˜

        Returns:
            ë¸”ë Œë”©ëœ ì„ë² ë”©
        """
        # ê°€ì¤‘ì¹˜ ì •ê·œí™”
        total = face_weight + hair_weight
        face_w = face_weight / total
        hair_w = hair_weight / total

        # ë¸”ë Œë”©
        blended = face_embeds * face_w + hair_embeds * hair_w

        return blended

    def _init_face_detection(self):
        """ì–¼êµ´ ê°ì§€ ì´ˆê¸°í™”"""
        if self.detection_method == 'mediapipe':
            try:
                import mediapipe as mp
                self.mp_face_detection = mp.solutions.face_detection
                self.face_detection = self.mp_face_detection.FaceDetection(
                    model_selection=1,
                    min_detection_confidence=0.5
                )
                print("âœ… MediaPipe ì–¼êµ´ ê°ì§€ ì¤€ë¹„ ì™„ë£Œ")
            except ImportError:
                print("âš ï¸ MediaPipe ì—†ìŒ. OpenCVë¡œ ì „í™˜...")
                self.detection_method = 'opencv'

        if self.detection_method == 'opencv':
            cascade_path = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
            self.face_cascade = cv2.CascadeClassifier(cascade_path)
            print("âœ… OpenCV ì–¼êµ´ ê°ì§€ ì¤€ë¹„ ì™„ë£Œ")

    def detect_face(self, image_path):
        """
        ì–¼êµ´ ê°ì§€

        Returns:
            (x, y, w, h) ë˜ëŠ” None
        """
        image = cv2.imread(image_path)
        if image is None:
            return None

        if self.detection_method == 'mediapipe':
            rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            results = self.face_detection.process(rgb_image)

            if not results.detections:
                return None

            detection = results.detections[0]
            bbox = detection.location_data.relative_bounding_box

            h, w = image.shape[:2]
            x = int(bbox.xmin * w)
            y = int(bbox.ymin * h)
            box_w = int(bbox.width * w)
            box_h = int(bbox.height * h)

            return (x, y, box_w, box_h)

        else:  # opencv
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            faces = self.face_cascade.detectMultiScale(
                gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30)
            )

            if len(faces) == 0:
                return None

            # ê°€ì¥ í° ì–¼êµ´
            largest_face = max(faces, key=lambda f: f[2] * f[3])
            return tuple(largest_face)

    def create_face_mask(self, image_path, expand_ratio=0.3, feather=15, include_hair=True, include_neck=False):
        """
        ì´ë¯¸ì§€ì—ì„œ ì–¼êµ´ ìë™ ê°ì§€ í›„ ë§ˆìŠ¤í¬ ìƒì„±

        Args:
            image_path: ì´ë¯¸ì§€ ê²½ë¡œ
            expand_ratio: ì–¼êµ´ ì˜ì—­ í™•ì¥
            feather: ê²½ê³„ ë¸”ëŸ¬
            include_hair: ë¨¸ë¦¬ì¹´ë½ í¬í•¨ ì—¬ë¶€ (BiSeNet ì‚¬ìš© ì‹œ)
            include_neck: ëª© í¬í•¨ ì—¬ë¶€ (BiSeNet ì‚¬ìš© ì‹œ)

        Returns:
            ë§ˆìŠ¤í¬ (PIL Image) ë˜ëŠ” None
        """
        print(f"ì–¼êµ´ ê°ì§€ ì¤‘: {os.path.basename(image_path)}")

        # BiSeNetìœ¼ë¡œ ë¨¸ë¦¬ì¹´ë½ í¬í•¨ ë§ˆìŠ¤í¬ ìƒì„± ì‹œë„
        if self.use_bisenet and self.face_parser is not None:
            try:
                image_pil = Image.open(image_path).convert("RGB")
                bisenet_mask = self.face_parser.get_face_hair_mask(
                    image_pil,
                    target_size=image_pil.size,
                    include_hair=include_hair,
                    include_neck=include_neck,
                    blur_radius=feather,
                    expand_ratio=1.0 + expand_ratio  # 1.3 for 0.3 expand
                )
                if bisenet_mask is not None:
                    parts = []
                    if include_hair:
                        parts.append("ë¨¸ë¦¬ì¹´ë½")
                    if include_neck:
                        parts.append("ëª©")
                    parts_str = "+".join(parts) if parts else "ì–¼êµ´ë§Œ"
                    print(f"BiSeNet ë§ˆìŠ¤í¬ ìƒì„± ì™„ë£Œ ({parts_str})")
                    return bisenet_mask
                else:
                    print("BiSeNet ë§ˆìŠ¤í¬ ì‹¤íŒ¨, íƒ€ì› ë§ˆìŠ¤í¬ë¡œ ì „í™˜")
            except Exception as e:
                print(f"BiSeNet ì˜¤ë¥˜: {e}, íƒ€ì› ë§ˆìŠ¤í¬ë¡œ ì „í™˜")

        # Fallback: íƒ€ì›í˜• ë§ˆìŠ¤í¬
        face_bbox = self.detect_face(image_path)

        if face_bbox is None:
            print("ì–¼êµ´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
            return None

        x, y, box_w, box_h = face_bbox
        print(f"ì–¼êµ´ ë°œê²¬: x={x}, y={y}, w={box_w}, h={box_h}")

        # ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°
        image = cv2.imread(image_path)
        h, w = image.shape[:2]

        # ë§ˆìŠ¤í¬ ìƒì„±
        mask = np.zeros((h, w), dtype=np.uint8)

        # ì˜ì—­ í™•ì¥ (ë¨¸ë¦¬ì¹´ë½ í¬í•¨ ì‹œ ë” í¬ê²Œ)
        hair_expand_multiplier = 1.5 if include_hair else 1.0
        expand_w = int(box_w * expand_ratio * hair_expand_multiplier)
        expand_h = int(box_h * expand_ratio * hair_expand_multiplier)

        # ë¨¸ë¦¬ì¹´ë½ í¬í•¨ ì‹œ ìœ„ìª½ìœ¼ë¡œ ë” í™•ì¥
        expand_h_up = int(expand_h * 1.8) if include_hair else expand_h

        x1 = max(0, x - expand_w)
        y1 = max(0, y - expand_h_up)  # ìœ„ìª½ í™•ì¥ (ë¨¸ë¦¬ì¹´ë½)
        x2 = min(w, x + box_w + expand_w)
        y2 = min(h, y + box_h + expand_h)

        # íƒ€ì›í˜• ë§ˆìŠ¤í¬
        center_x = (x1 + x2) // 2
        center_y = (y1 + y2) // 2
        axes_x = (x2 - x1) // 2
        axes_y = (y2 - y1) // 2

        cv2.ellipse(mask, (center_x, center_y), (axes_x, axes_y),
                   0, 0, 360, 255, -1)

        # ê²½ê³„ ë¸”ëŸ¬
        if feather > 0:
            mask = cv2.GaussianBlur(mask, (feather*2+1, feather*2+1), 0)

        return Image.fromarray(mask)

    def composite_face_auto(
        self,
        background_path,
        source_face_path,
        prompt="professional portrait, natural expression",
        output_path="output.png",
        face_strength=0.85,
        denoising_strength=0.92,
        num_inference_steps=50,
        guidance_scale=7.5,
        mask_expand=0.3,
        mask_blur=15,
        seed=None,
        save_mask=False,
        use_source_size=True,
        include_hair=True,
        include_neck=False,
        auto_detect_gender=True,
        face_blend_weight=0.6,
        hair_blend_weight=0.4,
        mask_padding=0,
        run_folder=None,
        stop_at=1.0,
        save_preview=False
    ):
        """
        ìë™ ì–¼êµ´ í•©ì„± (ë¨¸ë¦¬ì¹´ë½/ëª© í¬í•¨)

        Args:
            background_path: ë ˆí¼ëŸ°ìŠ¤ ë°°ê²½ (ì–¼êµ´ì´ ìˆëŠ” ì¦ëª…ì‚¬ì§„)
            source_face_path: í•©ì„±í•  ì–¼êµ´ ì´ë¯¸ì§€
            prompt: í”„ë¡¬í”„íŠ¸
            output_path: ì¶œë ¥ ê²½ë¡œ
            face_strength: ì–¼êµ´ ë°˜ì˜ ê°•ë„
            denoising_strength: ìƒì„± ê°•ë„
            num_inference_steps: ìƒì„± ìŠ¤í…
            guidance_scale: ê°€ì´ë˜ìŠ¤
            mask_expand: ë§ˆìŠ¤í¬ í™•ì¥ ë¹„ìœ¨
            mask_blur: ë§ˆìŠ¤í¬ ë¸”ëŸ¬
            seed: ëœë¤ ì‹œë“œ
            save_mask: ë§ˆìŠ¤í¬ ì €ì¥ ì—¬ë¶€
            use_source_size: ì›ë³¸ ì–¼êµ´ ì´ë¯¸ì§€ í¬ê¸° ì‚¬ìš© (True=ì›ë³¸ í¬ê¸° ìœ ì§€)
            include_hair: ë¨¸ë¦¬ì¹´ë½ í¬í•¨ ë§ˆìŠ¤í‚¹ (BiSeNet ì‚¬ìš©)
            include_neck: ëª© í¬í•¨ ë§ˆìŠ¤í‚¹ (BiSeNet ì‚¬ìš©)
            auto_detect_gender: ë¨¸ë¦¬ì¹´ë½ìœ¼ë¡œ ì„±ë³„ íŒíŠ¸ ìë™ ê°ì§€
            face_blend_weight: CLIP Blending ì‹œ ì–¼êµ´ ê°€ì¤‘ì¹˜ (ê¸°ë³¸: 0.6)
            hair_blend_weight: CLIP Blending ì‹œ ë¨¸ë¦¬ì¹´ë½ ê°€ì¤‘ì¹˜ (ê¸°ë³¸: 0.4)
            mask_padding: ë§ˆìŠ¤í¬ íŒ¨ë”© í”½ì…€ (ì–‘ìˆ˜=í™•ì¥, ìŒìˆ˜=ì¶•ì†Œ)
            stop_at: FaceID ì ìš© ì¤‘ë‹¨ ì‹œì  (0.0~1.0, ê¸°ë³¸: 1.0=ëê¹Œì§€)

        Returns:
            í•©ì„±ëœ ì´ë¯¸ì§€ (PIL Image)
        """
        if not self.has_ip_adapter:
            print("IP-Adapterê°€ í•„ìš”í•©ë‹ˆë‹¤!")
            return None

        # Preview ì„¤ì •
        self.save_preview = save_preview
        if save_preview:
            # Preview íŒŒì¼ ê²½ë¡œ ì„¤ì •
            base_path = output_path.replace('.png', '')
            self.preview_path = f"{base_path}_preview.png"

        print("=" * 70)
        print("ìë™ ì–¼êµ´ í•©ì„± (ë¨¸ë¦¬ì¹´ë½ í¬í•¨)" if include_hair else "ìë™ ì–¼êµ´ í•©ì„±")
        print("=" * 70)

        # 1. ì›ë³¸ ì–¼êµ´ ì´ë¯¸ì§€ ë¡œë“œ (í¬ê¸° ê²°ì •ìš©)
        print("\nì›ë³¸ ì–¼êµ´ ì´ë¯¸ì§€ ë¡œë”©...")
        source_face = Image.open(source_face_path).convert("RGB")
        src_w, src_h = source_face.size
        print(f"   ì›ë³¸ ì–¼êµ´ í¬ê¸°: {src_w}x{src_h}")

        # 1.5. ì„±ë³„ íŒíŠ¸ ìë™ ê°ì§€ (ë¨¸ë¦¬ì¹´ë½ ê¸°ë°˜)
        gender_hint = ""
        if auto_detect_gender and self.use_bisenet and self.face_parser is not None:
            try:
                gender_hint = self.face_parser.detect_gender_from_hair(source_face)
                print(f"   ë¨¸ë¦¬ì¹´ë½ ë¶„ì„: {gender_hint}")

                # í”„ë¡¬í”„íŠ¸ì— ì„±ë³„ íŒíŠ¸ ì¶”ê°€
                if "female" in gender_hint.lower():
                    gender_hint = "woman, "
                elif "male" in gender_hint.lower():
                    gender_hint = "man, "
                else:
                    gender_hint = ""
            except Exception as e:
                print(f"   ì„±ë³„ ê°ì§€ ì‹¤íŒ¨: {e}")
                gender_hint = ""

        # 2. ë°°ê²½ ì´ë¯¸ì§€ ë¡œë“œ ë° í¬ê¸° ì¡°ì •
        print("\nğŸ“‚ ë°°ê²½ ì´ë¯¸ì§€ ë¡œë”©...")
        background_img = Image.open(background_path).convert("RGB")
        bg_w, bg_h = background_img.size

        # ë°°ê²½ ì´ë¯¸ì§€ ë¹„ìœ¨ ê¸°ì¤€ + SDXL ìµœì í™” (1024/8ë°°ìˆ˜)
        max_side = 1024
        aspect_ratio = bg_w / bg_h

        if bg_w > bg_h:
            target_w = max_side
            target_h = int(max_side / aspect_ratio)
        else:
            target_h = max_side
            target_w = int(max_side * aspect_ratio)

        # 8ì˜ ë°°ìˆ˜ë¡œ ì¡°ì • (SDXL í•„ìˆ˜ ì¡°ê±´)
        target_w = (target_w // 8) * 8
        target_h = (target_h // 8) * 8
        target_size = (target_w, target_h)

        print(f"   âœ¨ ë°°ê²½ ë¹„ìœ¨ ìœ ì§€ ìŠ¤ì¼€ì¼ì—…: ({bg_w}, {bg_h}) -> {target_size}")

        # ë°°ê²½ ì´ë¯¸ì§€ë¥¼ ë¹„ìœ¨ ìœ ì§€í•˜ë©° ë¦¬ì‚¬ì´ì¦ˆ (crop ì—†ì´)
        background_img = background_img.resize(target_size, Image.Resampling.LANCZOS)

        # 3. ë°°ê²½ì—ì„œ ì–¼êµ´ ìë™ ê°ì§€ + ë§ˆìŠ¤í¬ ìƒì„±
        print("\në°°ê²½ì—ì„œ ì–¼êµ´ ë§ˆìŠ¤í¬ ìë™ ìƒì„±...")
        if include_hair:
            print("   (ë¨¸ë¦¬ì¹´ë½ ì˜ì—­ í¬í•¨)")

        # ì„ì‹œë¡œ ë°°ê²½ì„ ë¦¬ì‚¬ì´ì¦ˆëœ í¬ê¸°ë¡œ ì €ì¥ (ë§ˆìŠ¤í¬ ìƒì„±ìš©)
        import tempfile
        with tempfile.NamedTemporaryFile(suffix='.jpg', delete=False) as tmp:
            background_img.save(tmp.name)
            temp_bg_path = tmp.name

        face_mask = self.create_face_mask(
            temp_bg_path,
            expand_ratio=mask_expand,
            feather=mask_blur,
            include_hair=include_hair,
            include_neck=include_neck
        )

        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        os.unlink(temp_bg_path)

        if face_mask is None:
            print("ë°°ê²½ì—ì„œ ì–¼êµ´ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤!")
            print("TIP: ì •ë©´ ì–¼êµ´ì´ ëª…í™•í•œ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”")
            return None

        # ë§ˆìŠ¤í¬ íŒ¨ë”© ì ìš© (ì–‘ìˆ˜=í™•ì¥, ìŒìˆ˜=ì¶•ì†Œ)
        if mask_padding != 0:
            mask_array = np.array(face_mask.convert('L'))
            kernel_size = abs(mask_padding)
            kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (kernel_size * 2 + 1, kernel_size * 2 + 1))

            if mask_padding > 0:
                # í™•ì¥ (dilate)
                mask_array = cv2.dilate(mask_array, kernel, iterations=1)
                print(f"   ë§ˆìŠ¤í¬ í™•ì¥: +{mask_padding}px")
            else:
                # ì¶•ì†Œ (erode)
                mask_array = cv2.erode(mask_array, kernel, iterations=1)
                print(f"   ë§ˆìŠ¤í¬ ì¶•ì†Œ: {mask_padding}px")

            face_mask = Image.fromarray(mask_array)

        # ë§ˆìŠ¤í¬ ë° ì¤‘ê°„ ê²°ê³¼ ì €ì¥ (ì˜µì…˜)
        if save_mask:
            # run_folderê°€ ìˆìœ¼ë©´ í•´ë‹¹ í´ë”ì— ì €ì¥, ì—†ìœ¼ë©´ ê¸°ì¡´ ë°©ì‹
            if run_folder:
                save_dir = run_folder
            else:
                save_dir = os.path.dirname(output_path) or '.'

            mask_array = np.array(face_mask.convert('L'))
            bg_array = np.array(background_img)
            mask_bool = mask_array > 127

            # 3. ë§ˆìŠ¤í¬ ì˜¤ë²„ë ˆì´ (ë°°ê²½ì— ë§ˆìŠ¤í¬ ì˜ì—­ ë¹¨ê°„ìƒ‰ í‘œì‹œ) - ê°€ì¥ ì¤‘ìš”!
            overlay_path = os.path.join(save_dir, "3_mask_overlay.png")
            overlay = bg_array.copy()
            overlay[mask_bool, 0] = np.clip(overlay[mask_bool, 0] * 0.5 + 127, 0, 255)  # Red
            overlay[mask_bool, 1] = (overlay[mask_bool, 1] * 0.5).astype(np.uint8)
            overlay[mask_bool, 2] = (overlay[mask_bool, 2] * 0.5).astype(np.uint8)
            Image.fromarray(overlay).save(overlay_path)
            print(f"   ë§ˆìŠ¤í¬ ì˜¤ë²„ë ˆì´ ì €ì¥: {os.path.basename(overlay_path)}")

            # 4. Inpainting ì…ë ¥ ì‹œê°í™” (ë§ˆìŠ¤í¬ ì˜ì—­ ê²€ì •ìƒ‰ìœ¼ë¡œ í‘œì‹œ)
            inpaint_input_path = os.path.join(save_dir, "4_inpaint_input.png")
            inpaint_vis = bg_array.copy()
            inpaint_vis[mask_bool] = 0  # ë§ˆìŠ¤í¬ ì˜ì—­ ê²€ì •ìƒ‰
            Image.fromarray(inpaint_vis).save(inpaint_input_path)
            print(f"   Inpainting ì…ë ¥ ì €ì¥: {os.path.basename(inpaint_input_path)}")

        # 4. ë¨¸ë¦¬ì¹´ë½ ì˜ì—­ ì¶”ì¶œ (IP-Adapter ì…ë ¥ìš©)
        hair_region = None
        if include_hair and self.use_bisenet and self.face_parser is not None:
            try:
                hair_region = self.face_parser.extract_hair_region(source_face)
                if hair_region is not None:
                    print(f"   ë¨¸ë¦¬ì¹´ë½ ì˜ì—­ ì¶”ì¶œ ì™„ë£Œ (IP-Adapter ì…ë ¥ìš©)")
            except Exception as e:
                print(f"   ë¨¸ë¦¬ì¹´ë½ ì¶”ì¶œ ì‹¤íŒ¨: {e}")

        # 5. ìµœì¢… í¬ê¸° í™•ì¸
        print(f"\nìµœì¢… ì¶œë ¥ í¬ê¸°: {target_size}")
        # 6. í”„ë¡¬í”„íŠ¸ ì¤€ë¹„ (ì„±ë³„ íŒíŠ¸ í¬í•¨)
        full_prompt = (
            f"professional ID photo, passport style photograph, "
            f"{gender_hint}"
            f"neutral background, studio lighting, front-facing portrait, "
            f"sharp focus, high quality, even lighting, formal photograph, "
            f"{prompt}"
        )

        print(f"\ní”„ë¡¬í”„íŠ¸: {gender_hint}{prompt}")

        negative_prompt = (
            "bad quality, blurry, distorted, deformed, ugly, bad anatomy, "
            "wrong face, disfigured, mutation, low resolution, pixelated, "
            "artifacts, watermark, multiple faces, cropped face, "
            "side view, profile, looking away, tilted head"
        )

        # 6. Generator ì„¤ì •
        generator = None
        if seed is not None:
            generator = torch.Generator(self.device).manual_seed(seed)
            print(f"ì‹œë“œ: {seed}")

        print(f"\nâš™ï¸ ì„¤ì •:")
        print(f"   ì–¼êµ´ ë°˜ì˜ ê°•ë„: {face_strength}")
        print(f"   ìƒì„± ê°•ë„: {denoising_strength}")
        print(f"   ìƒì„± ìŠ¤í…: {num_inference_steps}")

        # 8. IP-Adapter ì„¤ì • ë° ì´ë¯¸ì§€/ì„ë² ë”© ì¤€ë¹„
        print(f"   IP-Adapter ëª¨ë“œ: {self.ip_adapter_mode.upper()}")

        ip_adapter_kwargs = {}

        if self.use_clip_blend:
            # CLIP Blending ëª¨ë“œ: í”½ì…€ ë ˆë²¨ ì–¼êµ´/ë¨¸ë¦¬ì¹´ë½ ë¸”ë Œë”© í›„ CLIP ì¸ì½”ë”©
            print("   CLIP Blending: í”½ì…€ ë ˆë²¨ ë¸”ë Œë”©...")
            self.pipeline.set_ip_adapter_scale(face_strength)

            if hair_region is not None:
                # ì–¼êµ´ + ë¨¸ë¦¬ì¹´ë½ í•©ì„± ì´ë¯¸ì§€ ìƒì„±
                composite_image = self._create_face_hair_composite(
                    source_face, hair_region,
                    face_weight=face_blend_weight,
                    hair_weight=hair_blend_weight
                )
                print(f"   [Face+Hair] í•©ì„± ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ")
                print(f"   ë¸”ë Œë”© ê°€ì¤‘ì¹˜: face={face_blend_weight:.0%}, hair={hair_blend_weight:.0%}")

                # í•©ì„± ì´ë¯¸ì§€ë¥¼ IP-Adapter ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©
                ip_adapter_kwargs["ip_adapter_image"] = composite_image

            else:
                # ë¨¸ë¦¬ì¹´ë½ ì˜ì—­ì´ ì—†ìœ¼ë©´ ì›ë³¸ ì–¼êµ´ ì‚¬ìš©
                print("   [Warning] ë¨¸ë¦¬ì¹´ë½ ì˜ì—­ ì—†ìŒ, ì›ë³¸ ì–¼êµ´ ì‚¬ìš©")
                ip_adapter_kwargs["ip_adapter_image"] = source_face

        elif self.use_dual_adapter and self.face_id_extractor is not None:
            # Dual IP-Adapter ëª¨ë“œ: Standard (ë¨¸ë¦¬ì¹´ë½ CLIP) + FaceID (ì–¼êµ´)
            # diffusersëŠ” ip_adapter_imageë¡œ ë¦¬ìŠ¤íŠ¸ ì „ë‹¬ ì‹œ ê° ì–´ëŒ‘í„°ì— ë¶„ë°°
            print("   Dual IP-Adapter: ì–¼êµ´ + ë¨¸ë¦¬ì¹´ë½ ì¤€ë¹„ ì¤‘...")

            # 1. InsightFace ì–¼êµ´ ì„ë² ë”© ì¶”ì¶œ
            face_embedding = self.face_id_extractor.get_embedding_for_ip_adapter(
                source_face,
                dtype=self.dtype,
                device=self.device
            )

            # 2. ë¨¸ë¦¬ì¹´ë½ ì´ë¯¸ì§€ ì¤€ë¹„ (CLIPìš©)
            hair_image_for_clip = hair_region if hair_region is not None else source_face
            if hair_region is not None:
                print("   [Hair] BiSeNet ë¨¸ë¦¬ì¹´ë½ ì˜ì—­ ì‚¬ìš©")
            else:
                print("   [Hair] ì „ì²´ ì–¼êµ´ ì´ë¯¸ì§€ ì‚¬ìš©")

            if face_embedding is not None:
                # FaceID ì„ë² ë”© í¬ë§·
                if face_embedding.dim() == 2:
                    face_embedding = face_embedding.unsqueeze(1)

                # CFG í¬ë§·: (batch, seq, dim) -> (2*batch, seq, dim)
                negative_face = torch.zeros_like(face_embedding)
                face_embedding_cfg = torch.cat([negative_face, face_embedding], dim=0)

                # Dual adapter ì…ë ¥: ë‘ ì–´ëŒ‘í„°ì— ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸ ì „ë‹¬
                # [Standard, FaceID] ìˆœì„œ
                # StandardëŠ” CLIPìœ¼ë¡œ ìë™ ì¸ì½”ë”©, FaceIDëŠ” ì´ë¯¸ì§€ì—ì„œ ì–¼êµ´ ê°ì§€
                ip_adapter_kwargs["ip_adapter_image"] = [hair_image_for_clip, source_face]

                print(f"   [Face] ì–¼êµ´ ì´ë¯¸ì§€ for FaceID")
                print(f"   [Hair] ë¨¸ë¦¬ì¹´ë½ ì´ë¯¸ì§€ for CLIP: {hair_image_for_clip.size}")

                # ìŠ¤ì¼€ì¼ ì„¤ì •: [Standard(hair), FaceID(face)]
                hair_scale = face_strength * 0.4  # ë¨¸ë¦¬ì¹´ë½ ìŠ¤íƒ€ì¼
                face_scale = face_strength * 0.8  # ì–¼êµ´ ì •ì²´ì„±
                self.pipeline.set_ip_adapter_scale([hair_scale, face_scale])
                print(f"   ìŠ¤ì¼€ì¼: hair={hair_scale:.2f}, face={face_scale:.2f}")

            else:
                print("   Dual: ì–¼êµ´ ê²€ì¶œ ì‹¤íŒ¨, Standard ëª¨ë“œë¡œ í´ë°±")
                ip_adapter_kwargs["ip_adapter_image"] = source_face
                self.pipeline.set_ip_adapter_scale(face_strength)

        elif self.use_faceid_plus and self.face_id_extractor is not None:
            # FaceID Plus v2: InsightFace + CLIP ì´ë¯¸ì§€ ì„ë² ë”© (ë¨¸ë¦¬ìŠ¤íƒ€ì¼ í¬í•¨)
            self.pipeline.set_ip_adapter_scale(face_strength)
            print("   FaceID Plus v2: ì–¼êµ´+ë¨¸ë¦¬ìŠ¤íƒ€ì¼ ì„ë² ë”© ì¶”ì¶œ ì¤‘...")

            # 1. InsightFace ì–¼êµ´ ì„ë² ë”© ì¶”ì¶œ
            face_embedding = self.face_id_extractor.get_embedding_for_ip_adapter(
                source_face,
                dtype=self.dtype,
                device=self.device
            )

            if face_embedding is not None:
                # 2. CLIP ì´ë¯¸ì§€ ì„ë² ë”© ì¶”ì¶œ (ë¨¸ë¦¬ìŠ¤íƒ€ì¼ í¬í•¨)
                from transformers import CLIPImageProcessor
                clip_processor = CLIPImageProcessor.from_pretrained("laion/CLIP-ViT-H-14-laion2B-s32B-b79K")
                clip_input = clip_processor(images=source_face, return_tensors="pt").pixel_values.to(self.device, dtype=self.dtype)

                # CLIP hidden states ì¶”ì¶œ (last_hidden_state ì‚¬ìš©)
                clip_output = self.clip_image_encoder(clip_input, output_hidden_states=True)
                # (1, 257, 1280) - 257 = 1 CLS + 256 patches
                clip_embeds = clip_output.hidden_states[-2]  # ë§ˆì§€ë§‰ì—ì„œ ë‘ ë²ˆì§¸ ë ˆì´ì–´

                # Shape ë§ì¶”ê¸°
                if face_embedding.dim() == 2:
                    face_embedding = face_embedding.unsqueeze(1)  # (1, 1, 512)

                # CFGìš© negative ì„ë² ë”©
                neg_face = torch.zeros_like(face_embedding)
                neg_clip = torch.zeros_like(clip_embeds)

                face_embedding_cfg = torch.cat([neg_face, face_embedding], dim=0)  # (2, 1, 512)
                clip_embeds_cfg = torch.cat([neg_clip, clip_embeds], dim=0)  # (2, 257, 1280)

                # Plus v2: CLIP ì„ë² ë”©ì€ 4D í•„ìš”: (batch, num_images, seq, hidden)
                clip_embeds_cfg = clip_embeds_cfg.unsqueeze(1)  # (2, 1, 257, 1280)

                # Plus v2: CLIP ì„ë² ë”©ì„ projection layerì— ì§ì ‘ ì„¤ì •
                self.pipeline.unet.encoder_hid_proj.image_projection_layers[0].clip_embeds = clip_embeds_cfg
                self.pipeline.unet.encoder_hid_proj.image_projection_layers[0].shortcut_scale = 1.0

                # ì–¼êµ´ ì„ë² ë”©ë§Œ ì „ë‹¬
                ip_adapter_kwargs["ip_adapter_image_embeds"] = [face_embedding_cfg]
                print(f"   FaceID Plus v2: ì„ë² ë”© ì¶”ì¶œ ì™„ë£Œ")
                print(f"      ì–¼êµ´ ì„ë² ë”©: {face_embedding_cfg.shape}")
                print(f"      CLIP ì„ë² ë”©: {clip_embeds_cfg.shape} (ë¨¸ë¦¬ìŠ¤íƒ€ì¼ í¬í•¨)")

            else:
                print("   FaceID Plus v2: ì–¼êµ´ ê²€ì¶œ ì‹¤íŒ¨, ì´ë¯¸ì§€ ì§ì ‘ ì‚¬ìš©")
                ip_adapter_kwargs["ip_adapter_image"] = source_face

        elif self.use_faceid and self.face_id_extractor is not None:
            # FaceID (non-Plus): InsightFace 512-dim ì„ë² ë”© ì‚¬ìš©
            self.pipeline.set_ip_adapter_scale(face_strength)
            print("   FaceID: InsightFace ì„ë² ë”© ì¶”ì¶œ ì¤‘...")
            face_embedding = self.face_id_extractor.get_embedding_for_ip_adapter(
                source_face,
                dtype=self.dtype,
                device=self.device
            )

            if face_embedding is not None:
                # Shape ë³€í™˜: (1, 512) -> (1, 1, 512) for IP-Adapter
                if face_embedding.dim() == 2:
                    face_embedding = face_embedding.unsqueeze(1)  # (batch, 1, 512)

                # Classifier-free guidance: negative + positive embeddings
                # Shape: (1, 1, 512) -> (2, 1, 512)
                negative_embedding = torch.zeros_like(face_embedding)
                face_embedding_cfg = torch.cat([negative_embedding, face_embedding], dim=0)

                ip_adapter_kwargs["ip_adapter_image_embeds"] = [face_embedding_cfg]
                print(f"   FaceID: InsightFace ì„ë² ë”© ì¶”ì¶œ ì™„ë£Œ (shape: {face_embedding_cfg.shape})")

            else:
                print("   FaceID: ì–¼êµ´ ê²€ì¶œ ì‹¤íŒ¨, Standard ëª¨ë“œë¡œ í´ë°±")
                ip_adapter_kwargs["ip_adapter_image"] = source_face

        else:
            # Standard ëª¨ë“œ: ì´ë¯¸ì§€ ì§ì ‘ ì „ë‹¬ (CLIP ì¸ì½”ë”©)
            # ë¨¸ë¦¬ì¹´ë½ ì˜ì—­ì´ ìˆìœ¼ë©´ ë¸”ë Œë”©
            if hair_region is not None:
                face_array = np.array(source_face).astype(np.float32)
                hair_array = np.array(hair_region).astype(np.float32)

                # ë¨¸ë¦¬ì¹´ë½ ì˜ì—­ ë§ˆìŠ¤í¬ (íšŒìƒ‰ì´ ì•„ë‹Œ ë¶€ë¶„)
                hair_mask_arr = np.any(np.abs(hair_array - 128) > 10, axis=2).astype(np.float32)
                hair_mask_arr = hair_mask_arr[:, :, np.newaxis]

                # ì›ë³¸ ì–¼êµ´ + ë¨¸ë¦¬ì¹´ë½ ê°•ì¡° ë¸”ë Œë”©
                blended = face_array * (1 - hair_mask_arr * 0.3) + hair_array * (hair_mask_arr * 0.3)
                blended = np.clip(blended, 0, 255).astype(np.uint8)
                ip_adapter_input = Image.fromarray(blended)

                print("   Standard: ì–¼êµ´+ë¨¸ë¦¬ì¹´ë½ ë¸”ë Œë”© ì´ë¯¸ì§€ ì‚¬ìš©")

            else:
                ip_adapter_input = source_face
                print("   Standard: ì›ë³¸ ì–¼êµ´ ì´ë¯¸ì§€ ì‚¬ìš©")

            ip_adapter_kwargs["ip_adapter_image"] = ip_adapter_input

        print("\ní•©ì„± ì‹œì‘...")
        print("   ë°°ê²½ ìœ ì§€ + ìƒˆ ì–¼êµ´ í•©ì„± ì¤‘...")

        # 9. Inpainting ìˆ˜í–‰ (ê³ í•´ìƒë„ ìƒì„± í›„ ì›ë³¸ í¬ê¸°ë¡œ ì¶•ì†Œ)
        orig_width, orig_height = background_img.size

        # SDXL ìµœì  í•´ìƒë„ë¡œ ìŠ¤ì¼€ì¼ì—… (ìµœì†Œ 1024px, ë¹„ìœ¨ ìœ ì§€)
        min_size = 1024
        scale = max(min_size / orig_width, min_size / orig_height, 1.0)
        gen_width = int(orig_width * scale)
        gen_height = int(orig_height * scale)

        # 8ì˜ ë°°ìˆ˜ë¡œ ì¡°ì •
        gen_width = (gen_width // 8) * 8
        gen_height = (gen_height // 8) * 8

        # ìƒì„±ìš© ì´ë¯¸ì§€/ë§ˆìŠ¤í¬ ë¦¬ì‚¬ì´ì¦ˆ
        if scale > 1.0:
            bg_for_gen = background_img.resize((gen_width, gen_height), Image.Resampling.LANCZOS)
            mask_for_gen = face_mask.resize((gen_width, gen_height), Image.Resampling.LANCZOS)
            print(f"   ê³ í•´ìƒë„ ìƒì„±: {orig_width}x{orig_height} -> {gen_width}x{gen_height}")
        else:
            bg_for_gen = background_img
            mask_for_gen = face_mask

        print(f"ğŸ¨ ìƒì„± ì‹œì‘... (ì´ {num_inference_steps} ìŠ¤í…, Stop-at: {stop_at*100:.0f}%)")

        # íƒ€ì´ë° ì œì–´ìš© ì½œë°± í•¨ìˆ˜ ì •ì˜
        def step_callback(pipe, step_index, _timestep, callback_kwargs):
            # 1. í˜„ì¬ ìŠ¤í… ìˆ˜ ê³„ì‚° (í˜¸í™˜ì„± ì²˜ë¦¬)
            try:
                cur_step = step_index.item() if hasattr(step_index, "item") else step_index
            except:
                cur_step = step_index

            # 2. ì§„í–‰ë¥  ê³„ì‚°
            progress = cur_step / num_inference_steps

            # 3. Stop-At ë¡œì§ ì ìš© & ë¡œê·¸ ì¶œë ¥
            if progress > stop_at:
                # ì§€ì •ëœ êµ¬ê°„ì„ ë„˜ì—ˆì„ ë•Œ -> ì–¼êµ´ ë°˜ì˜ ë„ê¸°
                pipe.set_ip_adapter_scale(0.0)
                status_msg = f"ğŸ›‘ OFF (Scale: 0.0)"
            else:
                # êµ¬ê°„ ì•ˆì¼ ë•Œ -> ì–¼êµ´ ë°˜ì˜ ì¼œê¸°
                pipe.set_ip_adapter_scale(face_strength)
                status_msg = f"âœ… ON  (Scale: {face_strength})"

            # ë§¤ ìŠ¤í…ë§ˆë‹¤ ë¡œê·¸ ì¶œë ¥
            print(f"   [Step {cur_step:02d}/{num_inference_steps}] ì§„í–‰ë¥  {progress*100:.0f}% -> FaceID: {status_msg}", flush=True)

            # 4. Preview ì´ë¯¸ì§€ ìƒì„± (5 ìŠ¤í…ë§ˆë‹¤)
            if hasattr(self, 'save_preview') and self.save_preview and cur_step > 0 and cur_step % 5 == 0:
                try:
                    latents = callback_kwargs.get("latents")
                    if latents is not None:
                        # VAEë¡œ latents ë””ì½”ë”©
                        latents_scaled = 1 / 0.18215 * latents
                        with torch.no_grad():
                            image_tensor = pipe.vae.decode(latents_scaled).sample

                        # Tensorë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜
                        image_tensor = (image_tensor / 2 + 0.5).clamp(0, 1)
                        image_np = image_tensor.cpu().permute(0, 2, 3, 1).float().numpy()[0]
                        image_np = (image_np * 255).round().astype("uint8")
                        preview_img = Image.fromarray(image_np)

                        # Preview ì €ì¥
                        preview_path = self.preview_path.replace('.png', f'_step{cur_step:03d}.png')
                        preview_img.save(preview_path)

                        # stdoutì— preview ê²½ë¡œ ì¶œë ¥ (ë°±ì—”ë“œê°€ íŒŒì‹±í•¨)
                        print(f"PREVIEW:{preview_path}", flush=True)
                except Exception as e:
                    print(f"   Preview ìƒì„± ì‹¤íŒ¨ (Step {cur_step}): {e}")

            return callback_kwargs

        result = self.pipeline(
            prompt=full_prompt,
            negative_prompt=negative_prompt,
            image=bg_for_gen,
            mask_image=mask_for_gen,
            width=gen_width,
            height=gen_height,
            num_inference_steps=num_inference_steps,
            guidance_scale=guidance_scale,
            strength=denoising_strength,
            generator=generator,
            callback_on_step_end=step_callback,
            **ip_adapter_kwargs  # ip_adapter_image ë˜ëŠ” ip_adapter_image_embeds
        )

        output_image = result.images[0]

        # ì›ë³¸ í¬ê¸°ì™€ ë‹¤ë¥´ë©´ ë³µì›
        if output_image.size != (orig_width, orig_height):
            output_image = output_image.resize((orig_width, orig_height), Image.Resampling.LANCZOS)
            print(f"   ì¶œë ¥ í¬ê¸° ë³µì›: {gen_width}x{gen_height} -> {orig_width}x{orig_height}")

        # 10. ì €ì¥
        output_image.save(output_path)
        print(f"\nâœ… ì™„ë£Œ! ì €ì¥ë¨: {output_path}")
        print("=" * 70)

        return output_image


def main():
    parser = argparse.ArgumentParser(
        description='ìë™ ì–¼êµ´ í•©ì„± (ë§ˆìŠ¤í¬ ìë™ ìƒì„±)',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ğŸ“– ì‚¬ìš© ë°©ë²• (ê°„ë‹¨ ë²„ì „!):

í•„ìš”í•œ ê²ƒ:
  1. ë°°ê²½ ì´ë¯¸ì§€ (ì¢‹ì€ ë°°ê²½/ì˜·ì˜ ì¦ëª…ì‚¬ì§„)
  2. í•©ì„±í•  ì–¼êµ´ (ì›ë³¸ ì–¼êµ´ ì´ë¯¸ì§€)

  âš ï¸ ë§ˆìŠ¤í¬ëŠ” ìë™ìœ¼ë¡œ ìƒì„±ë©ë‹ˆë‹¤!

ğŸ¯ ì‚¬ìš© ì˜ˆì‹œ:

# ê¸°ë³¸ (ê°€ì¥ ê°„ë‹¨)
python id_photo_face_composite_auto.py background.jpg face.jpg

# í”„ë¡¬í”„íŠ¸ ì¶”ê°€
python id_photo_face_composite_auto.py background.jpg face.jpg \\
    --prompt "young asian woman, natural smile"

# ê³ í’ˆì§ˆ ì„¤ì •
python id_photo_face_composite_auto.py background.jpg face.jpg \\
    --prompt "professional headshot" \\
    --face-strength 0.9 \\
    --steps 75 \\
    --seed 42 \\
    --output result.png

# ë§ˆìŠ¤í¬ë„ ì €ì¥í•˜ê³  ì‹¶ì„ ë•Œ
python id_photo_face_composite_auto.py background.jpg face.jpg \\
    --save-mask \\
    --output result.png

# FaceID ëª¨ë“œ (ì •ì²´ì„± ë³´ì¡´ í–¥ìƒ)
python id_photo_face_composite_auto.py background.jpg face.jpg \\
    --use-faceid \\
    --output result.png

ğŸ’¡ íŒŒë¼ë¯¸í„° ê°€ì´ë“œ:

--face-strength (ì–¼êµ´ ë°˜ì˜ ê°•ë„):
  0.75 ~ 0.85: ìì—°ìŠ¤ëŸ½ê²Œ (ê¸°ë³¸: 0.85)
  0.85 ~ 0.95: ì›ë³¸ê³¼ ë§¤ìš° ìœ ì‚¬í•˜ê²Œ

--denoising (ìƒì„± ê°•ë„):
  0.88 ~ 0.92: ë°°ê²½ ë§ì´ ìœ ì§€ (ê¸°ë³¸: 0.92)
  0.92 ~ 0.96: ë” ë§ì´ ë³€í˜•

--mask-expand (ë§ˆìŠ¤í¬ í™•ì¥):
  0.2 ~ 0.3: ì–¼êµ´ë§Œ (ê¸°ë³¸: 0.3)
  0.3 ~ 0.5: ì–¼êµ´ + ì£¼ë³€ ì¡°ê¸ˆ

--mask-blur (ë§ˆìŠ¤í¬ ë¸”ëŸ¬):
  10 ~ 15: ìì—°ìŠ¤ëŸ¬ìš´ ê²½ê³„ (ê¸°ë³¸: 15)
  15 ~ 25: ë” ë¶€ë“œëŸ¬ìš´ ê²½ê³„

ğŸ”§ ë¬¸ì œ í•´ê²°:

ì–¼êµ´ì„ ëª» ì°¾ì„ ë•Œ:
  - ì •ë©´ ì–¼êµ´ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”
  - ì–¼êµ´ì´ í¬ê³  ëª…í™•í•œ ì´ë¯¸ì§€ ì‚¬ìš©
  - --detection opencv ì˜µì…˜ ì‹œë„

ë°°ê²½ì´ ë„ˆë¬´ ë³€í•  ë•Œ:
  --denoising 0.88 (ë‚®ì¶”ê¸°)

ì–¼êµ´ì´ ì˜ ì•ˆë‚˜ì˜¬ ë•Œ:
  --face-strength 0.95 (ë†’ì´ê¸°)
  --steps 100 (ìŠ¤í… ì¦ê°€)

ì •ì²´ì„±ì´ ì•ˆ ë§ì„ ë•Œ:
  --use-faceid (InsightFace ê¸°ë°˜ FaceID ëª¨ë“œ)
  â€» pip install insightface onnxruntime í•„ìš”

ë¨¸ë¦¬ì¹´ë½ ìŠ¤íƒ€ì¼ë„ ì „ì´í•˜ê³  ì‹¶ì„ ë•Œ:
  --use-clip-blend (ê¶Œì¥! CLIP ì„ë² ë”© ë¸”ë Œë”©)
  â€» ì–¼êµ´ + ë¨¸ë¦¬ì¹´ë½ CLIP ì„ë² ë”© ê°€ì¤‘ì¹˜ ë¸”ë Œë”©
  â€» BiSeNetìœ¼ë¡œ ë¨¸ë¦¬ì¹´ë½ ì˜ì—­ ìë™ ì¶”ì¶œ
        """
    )

    parser.add_argument('background',
                       help='ë°°ê²½ ì´ë¯¸ì§€ (ì¦ëª…ì‚¬ì§„)')
    parser.add_argument('face',
                       help='í•©ì„±í•  ì–¼êµ´ ì´ë¯¸ì§€')

    parser.add_argument('--prompt', '-p',
                       default='professional portrait, natural expression',
                       help='í”„ë¡¬í”„íŠ¸')
    parser.add_argument('--auto-prompt', action='store_true',
                       help='Gemini Visionìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ ìë™ ìƒì„± (GEMINI_API_KEY í•„ìš”)')
    parser.add_argument('--output', '-o', default='output.png',
                       help='ì¶œë ¥ íŒŒì¼')
    parser.add_argument('--face-strength', type=float, default=0.85,
                       help='ì–¼êµ´ ë°˜ì˜ ê°•ë„ (ê¸°ë³¸: 0.85)')
    parser.add_argument('--denoising', type=float, default=0.92,
                       help='ìƒì„± ê°•ë„ (ê¸°ë³¸: 0.92)')
    parser.add_argument('--steps', type=int, default=50,
                       help='ìƒì„± ìŠ¤í… (ê¸°ë³¸: 50)')
    parser.add_argument('--guidance', type=float, default=7.5,
                       help='ê°€ì´ë˜ìŠ¤ (ê¸°ë³¸: 7.5)')
    parser.add_argument('--mask-expand', type=float, default=0.3,
                       help='ë§ˆìŠ¤í¬ í™•ì¥ ë¹„ìœ¨ (ê¸°ë³¸: 0.3)')
    parser.add_argument('--mask-blur', type=int, default=15,
                       help='ë§ˆìŠ¤í¬ ë¸”ëŸ¬ (ê¸°ë³¸: 15)')
    parser.add_argument('--mask-padding', type=int, default=0,
                       help='ë§ˆìŠ¤í¬ íŒ¨ë”© í”½ì…€ (ê¸°ë³¸: 0, ì–‘ìˆ˜=í™•ì¥, ìŒìˆ˜=ì¶•ì†Œ)')
    parser.add_argument('--seed', type=int, help='ëœë¤ ì‹œë“œ')
    parser.add_argument('--save-mask', action='store_true',
                       help='ë§ˆìŠ¤í¬ íŒŒì¼ë„ ì €ì¥')
    parser.add_argument('--use-background-size', action='store_true',
                       help='ë°°ê²½ ì´ë¯¸ì§€ í¬ê¸° ì‚¬ìš© (ê¸°ë³¸: ì›ë³¸ ì–¼êµ´ í¬ê¸°)')
    parser.add_argument('--detection', choices=['mediapipe', 'opencv'],
                       default='opencv',
                       help='ì–¼êµ´ ê°ì§€ ë°©ë²• (ê¸°ë³¸: opencv)')
    parser.add_argument('--no-hair', action='store_true',
                       help='ë¨¸ë¦¬ì¹´ë½ ì œì™¸ (ì–¼êµ´ë§Œ ë§ˆìŠ¤í‚¹)')
    parser.add_argument('--include-neck', action='store_true',
                       help='ëª© í¬í•¨ ë§ˆìŠ¤í‚¹ (ë ˆí¼ëŸ°ìŠ¤ ëª©ì´ ì´ìƒí•  ë•Œ ì‚¬ìš©)')
    parser.add_argument('--no-bisenet', action='store_true',
                       help='BiSeNet ë¹„í™œì„±í™” (íƒ€ì› ë§ˆìŠ¤í¬ë§Œ ì‚¬ìš©)')
    parser.add_argument('--no-gender-detect', action='store_true',
                       help='ì„±ë³„ ìë™ ê°ì§€ ë¹„í™œì„±í™”')
    parser.add_argument('--use-faceid', action='store_true',
                       help='IP-Adapter FaceID ì‚¬ìš© (InsightFace ê¸°ë°˜, ì •ì²´ì„± ë³´ì¡´ í–¥ìƒ)')
    parser.add_argument('--use-faceid-plus', action='store_true',
                       help='IP-Adapter FaceID Plus v2 ì‚¬ìš© (ì–¼êµ´ ì •ì²´ì„± + ë¨¸ë¦¬ìŠ¤íƒ€ì¼ ë™ì‹œ ë°˜ì˜)')
    parser.add_argument('--use-dual-adapter', action='store_true',
                       help='Dual IP-Adapter ì‚¬ìš© (FaceID + CLIP, ì–¼êµ´ ì •ì²´ì„± + ë¨¸ë¦¬ì¹´ë½ ìŠ¤íƒ€ì¼ ì „ì´)')
    parser.add_argument('--use-clip-blend', action='store_true',
                       help='CLIP Blending ëª¨ë“œ (ì–¼êµ´+ë¨¸ë¦¬ì¹´ë½ CLIP ì„ë² ë”© ë¸”ë Œë”©, ê¶Œì¥)')
    parser.add_argument('--face-blend-weight', type=float, default=0.6,
                       help='CLIP Blending: ì–¼êµ´ ê°€ì¤‘ì¹˜ (ê¸°ë³¸: 0.6)')
    parser.add_argument('--hair-blend-weight', type=float, default=0.4,
                       help='CLIP Blending: ë¨¸ë¦¬ì¹´ë½ ê°€ì¤‘ì¹˜ (ê¸°ë³¸: 0.4)')
    parser.add_argument('--stop-at', type=float, default=1.0,
                       help='FaceID ì ìš© ì¤‘ë‹¨ ì‹œì  (0.0~1.0, ê¸°ë³¸: 1.0=ëê¹Œì§€)')
    parser.add_argument('--save-preview', action='store_true',
                       help='ì¤‘ê°„ ìƒì„± ê³¼ì • preview ì´ë¯¸ì§€ ì €ì¥ (5 ìŠ¤í…ë§ˆë‹¤)')
    parser.add_argument('--show', action='store_true',
                       help='ê²°ê³¼ í‘œì‹œ')

    args = parser.parse_args()

    # ì…ë ¥ ê²½ë¡œ ì²˜ë¦¬ (inputs/ í´ë” ìë™ í™•ì¸)
    background_path = get_input_path(args.background)
    face_path = get_input_path(args.face)

    # íŒŒì¼ í™•ì¸
    for path, name in [(background_path, 'ë°°ê²½'), (face_path, 'ì–¼êµ´')]:
        if not os.path.exists(path):
            print(f"âŒ {name} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {path}")
            print(f"   (inputs/ í´ë”ë„ í™•ì¸í–ˆìŠµë‹ˆë‹¤)")
            return

    # í•©ì„± ìˆ˜í–‰
    compositor = AutoIDPhotoCompositor(
        detection_method=args.detection,
        use_bisenet=not args.no_bisenet,
        use_faceid=args.use_faceid,
        use_faceid_plus=args.use_faceid_plus,
        use_dual_adapter=args.use_dual_adapter,
        use_clip_blend=args.use_clip_blend
    )

    if not compositor.has_ip_adapter:
        print("\nIP-Adapter ë¡œë”© ì‹¤íŒ¨")
        print("   pip install diffusers transformers accelerate")
        return

    # ì‹¤í–‰ í´ë” ìƒì„± (outputs/run_name_timestamp/)
    run_folder = setup_run_folder(args.output)
    print(f"\nì‹¤í–‰ í´ë”: {run_folder}")

    # ì…ë ¥ ì´ë¯¸ì§€ ë³µì‚¬
    bg_copy_path = os.path.join(run_folder, "1_reference.png")
    face_copy_path = os.path.join(run_folder, "2_face.png")
    shutil.copy2(background_path, bg_copy_path)
    shutil.copy2(face_path, face_copy_path)
    print(f"   ì…ë ¥ ì´ë¯¸ì§€ ë³µì‚¬ ì™„ë£Œ")

    # ì‹œë“œ ì²˜ë¦¬ (ë¯¸ì§€ì •ì‹œ ëœë¤ ìƒì„±)
    actual_seed = args.seed if args.seed is not None else random.randint(0, 2**32 - 1)

    # í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬ (auto-prompt ë˜ëŠ” ìˆ˜ë™)
    if args.auto_prompt and HAS_PROMPT_GENERATOR:
        print("\nğŸ¤– Gemini Visionìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ ìƒì„± ì¤‘...")
        generated_prompt = generate_prompt_from_face_image(face_path)
        final_prompt = generated_prompt
        print(f"   ìƒì„±ëœ í”„ë¡¬í”„íŠ¸: {final_prompt}")
        print(f"GENERATED_PROMPT:{final_prompt}", flush=True)
    elif args.auto_prompt and not HAS_PROMPT_GENERATOR:
        print("\n   prompt_generator.pyë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        final_prompt = args.prompt
    else:
        final_prompt = args.prompt

    # ì‹¤í–‰ ëª…ë ¹ì–´ ê¸°ë¡
    command = ' '.join(sys.argv)

    # ì¶œë ¥ ê²½ë¡œ (í´ë” ë‚´ result.png)
    internal_output_path = os.path.join(run_folder, "5_result.png")

    result = compositor.composite_face_auto(
        background_path=background_path,
        source_face_path=face_path,
        prompt=final_prompt,
        output_path=internal_output_path,
        face_strength=args.face_strength,
        denoising_strength=args.denoising,
        num_inference_steps=args.steps,
        guidance_scale=args.guidance,
        mask_expand=args.mask_expand,
        mask_blur=args.mask_blur,
        seed=actual_seed,
        save_mask=True,  # í•­ìƒ ë§ˆìŠ¤í¬ ì €ì¥
        use_source_size=not args.use_background_size,
        include_hair=not args.no_hair,
        include_neck=args.include_neck,
        auto_detect_gender=not args.no_gender_detect,
        face_blend_weight=args.face_blend_weight,
        hair_blend_weight=args.hair_blend_weight,
        mask_padding=args.mask_padding,
        run_folder=run_folder,
        stop_at=args.stop_at,
        save_preview=args.save_preview
    )

    # íŒŒë¼ë¯¸í„° ì €ì¥
    save_run_params(run_folder, args, command, actual_seed, background_path, face_path, final_prompt)

    if result and args.show:
        result.show()


if __name__ == "__main__":
    import sys

    if len(sys.argv) > 1:
        main()
    else:
        # ëŒ€í™”í˜• ëª¨ë“œ
        print("=" * 70)
        print("ìë™ ì–¼êµ´ í•©ì„± (ë§ˆìŠ¤í¬ ìë™ ìƒì„±)")
        print("=" * 70)
        print("\nğŸ’¡ í•„ìš”í•œ ê²ƒ: ë°°ê²½ ì´ë¯¸ì§€ + ì–¼êµ´ ì´ë¯¸ì§€ (2ê°œë§Œ!)\n")

        background = input("ë°°ê²½ ì´ë¯¸ì§€ ê²½ë¡œ: ").strip()
        face = input("í•©ì„±í•  ì–¼êµ´ ê²½ë¡œ: ").strip()

        if not os.path.exists(background):
            print(f"âŒ ë°°ê²½ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {background}")
            sys.exit(1)
        if not os.path.exists(face):
            print(f"âŒ ì–¼êµ´ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {face}")
            sys.exit(1)

        prompt = input("í”„ë¡¬í”„íŠ¸ (Enter=ê¸°ë³¸ê°’): ").strip()
        if not prompt:
            prompt = "professional portrait, natural expression"

        compositor = AutoIDPhotoCompositor()

        if compositor.has_ip_adapter:
            result = compositor.composite_face_auto(
                background_path=background,
                source_face_path=face,
                prompt=prompt,
                save_mask=True
            )
            if result:
                result.show()
