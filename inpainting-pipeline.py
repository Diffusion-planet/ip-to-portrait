"""
Inpainting Pipeline for Face Composition
ìë™ ì–¼êµ´ í•©ì„± - ë§ˆìŠ¤í¬ ìë™ ìƒì„± (ë¨¸ë¦¬ì¹´ë½ í¬í•¨)
í•„ìš”í•œ ê²ƒ: ë°°ê²½ ì´ë¯¸ì§€ + í•©ì„±í•  ì–¼êµ´ ì´ë¯¸ì§€ (2ê°œë§Œ!)

v2: BiSeNet ê¸°ë°˜ face+hair ë§ˆìŠ¤í‚¹ ì¶”ê°€
v3: IP-Adapter FaceID ì§€ì› (ì •ì²´ì„± ë³´ì¡´)
v4: Dual IP-Adapter ì‹œë„ (ì‹¤íŒ¨ - diffusers í•œê³„)
v5: CLIP Blending ëª¨ë“œ ì¶”ê°€ - ì–¼êµ´/ë¨¸ë¦¬ì¹´ë½ CLIP ì„ë² ë”© ë¸”ë Œë”©
"""

import torch
import gc
from diffusers import AutoPipelineForInpainting
from PIL import Image, ImageFilter, ImageOps
import numpy as np
import cv2
import argparse
import os
import shutil
import sys
import random
from datetime import datetime


def cleanup_gpu_memory():
    """GPU ë©”ëª¨ë¦¬ ì •ë¦¬ - ìƒì„± ì™„ë£Œ í›„ í˜¸ì¶œ"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
    gc.collect()


def get_input_path(input_path: str) -> str:
    """ì…ë ¥ ê²½ë¡œ ì²˜ë¦¬ - inputs/ í´ë” ìë™ í™•ì¸

    íŒŒì¼ì´ í˜„ì¬ ìœ„ì¹˜ì— ì—†ìœ¼ë©´ inputs/ í´ë”ì—ì„œ ì°¾ìŒ
    """
    # ì´ë¯¸ ì¡´ì¬í•˜ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    if os.path.exists(input_path):
        return input_path

    # ì ˆëŒ€ ê²½ë¡œë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
    if os.path.isabs(input_path):
        return input_path

    # ìŠ¤í¬ë¦½íŠ¸ ìœ„ì¹˜ ê¸°ì¤€ìœ¼ë¡œ inputs í´ë” í™•ì¸
    script_dir = os.path.dirname(os.path.abspath(__file__))
    inputs_dir = os.path.join(script_dir, "inputs")
    inputs_path = os.path.join(inputs_dir, input_path)

    if os.path.exists(inputs_path):
        return inputs_path

    # ëª» ì°¾ìœ¼ë©´ ì›ë³¸ ê²½ë¡œ ë°˜í™˜ (ì—ëŸ¬ëŠ” ë‚˜ì¤‘ì— ì²˜ë¦¬)
    return input_path


def setup_run_folder(run_name: str = None) -> str:
    """ì‹¤í–‰ í´ë” ìƒì„±

    ê° ì‹¤í–‰ë§ˆë‹¤ í•˜ë‚˜ì˜ í´ë”ê°€ ìƒì„±ë¨:
    - outputs/run_name_timestamp/ ë˜ëŠ”
    - outputs/timestamp/ (run_name ë¯¸ì§€ì •ì‹œ)

    Returns:
        í´ë” ê²½ë¡œ
    """
    script_dir = os.path.dirname(os.path.abspath(__file__))
    outputs_dir = os.path.join(script_dir, "outputs")

    # íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„± (YYYYMMDD_HHMMSS)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # í´ë”ëª… ê²°ì •
    if run_name:
        # í™•ì¥ì ì œê±° (ì‚¬ìš©ìê°€ .png ë“±ì„ ë¶™ì˜€ì„ ê²½ìš°)
        if '.' in run_name:
            run_name = run_name.rsplit('.', 1)[0]
        folder_name = f"{run_name}_{timestamp}"
    else:
        folder_name = f"run_{timestamp}"

    run_folder = os.path.join(outputs_dir, folder_name)
    os.makedirs(run_folder, exist_ok=True)

    return run_folder


def save_run_params(run_folder: str, args, command: str, actual_seed: int,
                    background_path: str, face_path: str, actual_prompt: str = None):
    """ì‹¤í–‰ íŒŒë¼ë¯¸í„°ë¥¼ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥

    Args:
        run_folder: ì‹¤í–‰ í´ë” ê²½ë¡œ
        args: argparse ê²°ê³¼
        command: ì‹¤ì œ ì‹¤í–‰í•œ ëª…ë ¹ì–´
        actual_seed: ì‹¤ì œ ì‚¬ìš©ëœ ì‹œë“œ (ëœë¤ ìƒì„±ëœ ê²½ìš° í¬í•¨)
        background_path: ë°°ê²½ ì´ë¯¸ì§€ ê²½ë¡œ
        face_path: ì–¼êµ´ ì´ë¯¸ì§€ ê²½ë¡œ
        actual_prompt: ì‹¤ì œ ì‚¬ìš©ëœ í”„ë¡¬í”„íŠ¸ (auto-prompt ì‹œ ìƒì„±ëœ í”„ë¡¬í”„íŠ¸)
    """
    params_path = os.path.join(run_folder, "params.txt")

    with open(params_path, 'w', encoding='utf-8') as f:
        f.write("=" * 70 + "\n")
        f.write("Inpainting Pipeline - Run Parameters\n")
        f.write("=" * 70 + "\n\n")

        # ì‹¤í–‰ ì‹œê°„
        f.write(f"ì‹¤í–‰ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")

        # ì‹¤ì œ ëª…ë ¹ì–´
        f.write("[ ì‹¤í–‰ ëª…ë ¹ì–´ ]\n")
        f.write(f"{command}\n\n")

        # ì…ë ¥ íŒŒì¼
        f.write("[ ì…ë ¥ íŒŒì¼ ]\n")
        f.write(f"ë°°ê²½ ì´ë¯¸ì§€: {background_path}\n")
        f.write(f"ì–¼êµ´ ì´ë¯¸ì§€: {face_path}\n\n")

        # ëª¨ë“  íŒŒë¼ë¯¸í„°
        f.write("[ íŒŒë¼ë¯¸í„° ]\n")
        f.write(f"seed: {actual_seed}  # ì¬í˜„ì— í•„ìˆ˜!\n")
        f.write(f"face_strength: {args.face_strength}\n")
        f.write(f"denoising: {args.denoising}\n")
        f.write(f"guidance: {args.guidance}\n")
        f.write(f"steps: {args.steps}\n")
        f.write(f"mask_expand: {args.mask_expand}\n")
        f.write(f"mask_blur: {args.mask_blur}\n")
        f.write(f"mask_padding: {args.mask_padding}\n")
        # ì‹¤ì œ ì‚¬ìš©ëœ í”„ë¡¬í”„íŠ¸ ì €ì¥ (auto-promptë©´ ìƒì„±ëœ ê²ƒ, ì•„ë‹ˆë©´ ì›ë³¸)
        used_prompt = actual_prompt if actual_prompt else args.prompt
        f.write(f"prompt: {used_prompt}\n\n")

        # ëª¨ë“œ ì„¤ì •
        f.write("[ ëª¨ë“œ ì„¤ì • ]\n")
        f.write(f"use_faceid: {args.use_faceid}\n")
        f.write(f"use_faceid_plus: {args.use_faceid_plus}\n")
        f.write(f"use_dual_adapter: {args.use_dual_adapter}\n")
        f.write(f"use_clip_blend: {args.use_clip_blend}\n")
        f.write(f"detection: {args.detection}\n")
        f.write(f"no_bisenet: {args.no_bisenet}\n")
        f.write(f"no_hair: {args.no_hair}\n")
        f.write(f"include_neck: {args.include_neck}\n")
        f.write(f"no_gender_detect: {args.no_gender_detect}\n")
        f.write(f"use_background_size: {args.use_background_size}\n")
        f.write(f"stop_at: {args.stop_at}\n")
        f.write(f"auto_prompt: {args.auto_prompt}\n\n")

        # CLIP Blending íŒŒë¼ë¯¸í„°
        f.write("[ CLIP Blending ]\n")
        f.write(f"face_blend_weight: {args.face_blend_weight}\n")
        f.write(f"hair_blend_weight: {args.hair_blend_weight}\n")
        f.write(f"shortcut_scale: {args.shortcut_scale}\n\n")

        # ì¬í˜„ ëª…ë ¹ì–´
        f.write("=" * 70 + "\n")
        f.write("[ ì¬í˜„ ëª…ë ¹ì–´ ]\n")
        f.write("=" * 70 + "\n")
        reproduce_cmd = (
            f"python inpainting-pipeline.py {os.path.basename(background_path)} {os.path.basename(face_path)} "
            f"--face-strength {args.face_strength} "
            f"--denoising {args.denoising} "
            f"--guidance {args.guidance} "
            f"--steps {args.steps} "
            f"--mask-padding {args.mask_padding} "
            f"--seed {actual_seed} "
            f"--prompt \"{args.prompt}\""
        )
        if args.use_faceid_plus:
            reproduce_cmd += " --use-faceid-plus"
        elif args.use_faceid:
            reproduce_cmd += " --use-faceid"
        if args.no_gender_detect:
            reproduce_cmd += " --no-gender-detect"
        if args.no_hair:
            reproduce_cmd += " --no-hair"
        if args.include_neck:
            reproduce_cmd += " --include-neck"
        if args.stop_at < 1.0:
            reproduce_cmd += f" --stop-at {args.stop_at}"
        if args.auto_prompt:
            reproduce_cmd += " --auto-prompt"
        if hasattr(args, 'use_pre_paste') and args.use_pre_paste:
            reproduce_cmd += " --use-pre-paste"
            reproduce_cmd += f" --pre-paste-denoising {args.pre_paste_denoising}"
        if hasattr(args, 'use_face_swap') and args.use_face_swap:
            reproduce_cmd += " --use-face-swap"
            if hasattr(args, 'face_swap_model'):
                reproduce_cmd += f" --face-swap-model {args.face_swap_model}"
        if hasattr(args, 'use_face_enhance') and args.use_face_enhance:
            reproduce_cmd += " --use-face-enhance"
            reproduce_cmd += f" --face-enhance-strength {args.face_enhance_strength}"
        if hasattr(args, 'use_swap_refinement') and args.use_swap_refinement:
            reproduce_cmd += " --use-swap-refinement"
            reproduce_cmd += f" --swap-refinement-strength {args.swap_refinement_strength}"

        f.write(f"{reproduce_cmd}\n")

    print(f"   íŒŒë¼ë¯¸í„° ì €ì¥: {params_path}")

# BiSeNet face parser (optional, for hair-inclusive masks)
try:
    from face_parsing import FaceParser
    HAS_FACE_PARSER = True
except ImportError:
    HAS_FACE_PARSER = False
    print("face_parsing.py not found. BiSeNet hair masking unavailable.")

# FaceID module (optional, for better identity preservation)
try:
    print("[DEBUG inpainting-pipeline.py] Attempting to import face_id module...")
    print(f"[DEBUG inpainting-pipeline.py] sys.path: {__import__('sys').path[:3]}")
    print(f"[DEBUG inpainting-pipeline.py] __file__: {__file__}")
    from face_id import FaceIDExtractor, FaceIDIPAdapter, FaceSwapper, FaceEnhancer, check_insightface_available, HAS_GFPGAN, get_face_swapper
    print("[DEBUG inpainting-pipeline.py] âœ… face_id module imported successfully!")
    HAS_FACEID = check_insightface_available()
    HAS_FACESWAP = HAS_FACEID  # FaceSwap requires InsightFace
    HAS_FACE_ENHANCE = HAS_GFPGAN  # Face Enhance requires GFPGAN
    print(f"[DEBUG inpainting-pipeline.py] check_insightface_available() = {HAS_FACEID}")
    print(f"[DEBUG inpainting-pipeline.py] HAS_GFPGAN = {HAS_GFPGAN}")
    if not HAS_FACEID:
        print("InsightFace not installed. FaceID mode unavailable.")
        print("Install: pip install insightface onnxruntime")
    if not HAS_GFPGAN:
        print("GFPGAN not installed. Face Enhance mode unavailable.")
        print("Install: pip install gfpgan")
except ImportError as e:
    HAS_FACEID = False
    HAS_FACESWAP = False
    HAS_FACE_ENHANCE = False
    print(f"[DEBUG inpainting-pipeline.py] âŒ face_id import failed: {e}")
    print("face_id.py not found. FaceID mode unavailable.")

# Gemini Vision í”„ë¡¬í”„íŠ¸ ìƒì„±ê¸° (optional)
try:
    from prompt_generator import generate_prompt_from_face_image
    HAS_PROMPT_GENERATOR = True
except ImportError:
    HAS_PROMPT_GENERATOR = False


def get_device():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ìµœì ì˜ ë””ë°”ì´ìŠ¤ ë°˜í™˜"""
    if torch.cuda.is_available():
        return "cuda"
    elif torch.backends.mps.is_available():
        return "mps"
    else:
        return "cpu"


class AutoIDPhotoCompositor:
    """ìë™ ì–¼êµ´ ê°ì§€ + í•©ì„± (ë¨¸ë¦¬ì¹´ë½ í¬í•¨, FaceID ì§€ì›, CLIP Blending, Pre-paste, FaceSwap)"""

    def __init__(self, detection_method='opencv', use_bisenet=True, use_faceid=False,
                 use_dual_adapter=False, use_clip_blend=False, use_faceid_plus=False,
                 use_pre_paste=False, use_face_swap=False, use_face_enhance=False,
                 use_swap_refinement=False, no_ip_adapter=False, face_swap_model='insightface'):
        """
        íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”

        Args:
            detection_method: 'opencv' or 'mediapipe'
            use_bisenet: BiSeNet ì‚¬ìš© ì—¬ë¶€ (ë¨¸ë¦¬ì¹´ë½ ë§ˆìŠ¤í‚¹)
            use_faceid: FaceID ëª¨ë“œ ì‚¬ìš© ì—¬ë¶€ (ì •ì²´ì„± ë³´ì¡´ í–¥ìƒ)
            use_dual_adapter: Dual IP-Adapter ëª¨ë“œ (FaceID + CLIP for hair transfer)
            use_clip_blend: CLIP Blending ëª¨ë“œ (ì–¼êµ´+ë¨¸ë¦¬ì¹´ë½ CLIP ì„ë² ë”© ë¸”ë Œë”©)
            use_pre_paste: Pre-paste ëª¨ë“œ (ì†ŒìŠ¤ ì–¼êµ´ì„ ë¯¸ë¦¬ ë¶™ì—¬ë„£ê¸°, denoising ë‚®ì¶¤)
            use_face_swap: Face Swap ëª¨ë“œ (ìƒì„± í›„ ì–¼êµ´ êµì²´)
            use_face_enhance: Face Enhance ëª¨ë“œ (GFPGANìœ¼ë¡œ ì–¼êµ´ í™”ì§ˆ ê°œì„ )
            use_swap_refinement: Face Swap Refinement ëª¨ë“œ (Face Swap í›„ ê²½ë¯¸í•œ ì¸í˜ì¸íŒ…ìœ¼ë¡œ ë¸”ë Œë”©)
            no_ip_adapter: IP-Adapter ì—†ì´ ìˆœìˆ˜ ì¸í˜ì¸íŒ…ë§Œ ìˆ˜í–‰ (Pre-pasteì™€ í•¨ê»˜ ì‚¬ìš© ê¶Œì¥)
            face_swap_model: Face Swap ëª¨ë¸ ì„ íƒ ('insightface' ë¹ ë¦„, 'ghost' ê³ í™”ì§ˆ)
        """
        print("=" * 70)
        print("Inpainting Pipeline v5")
        print("=" * 70)

        # ë””ë°”ì´ìŠ¤ ê°ì§€
        self.device = get_device()
        print(f"ë””ë°”ì´ìŠ¤: {self.device}")

        # ëª¨ë“œ ì„¤ì •
        self.no_ip_adapter = no_ip_adapter  # ìˆœìˆ˜ ì¸í˜ì¸íŒ… ëª¨ë“œ (IP-Adapter ì—†ìŒ)
        print(f"[DEBUG __init__] no_ip_adapter = {no_ip_adapter}")
        print(f"[DEBUG __init__] HAS_FACEID = {HAS_FACEID}")
        print(f"[DEBUG __init__] use_faceid_plus argument = {use_faceid_plus}")

        # no_ip_adapter ëª¨ë“œë©´ ëª¨ë“  IP-Adapter ê´€ë ¨ ê¸°ëŠ¥ ë¹„í™œì„±í™”
        if no_ip_adapter:
            self.use_dual_adapter = False
            self.use_faceid = False
            self.use_faceid_plus = False
            self.use_clip_blend = False
            self.ip_adapter_mode = "none"
            print("ğŸ“‹ Simple Inpainting ëª¨ë“œ (IP-Adapter ì—†ìŒ)")
        else:
            # Dual adapter requires both FaceID and CLIP
            self.use_dual_adapter = use_dual_adapter and HAS_FACEID
            self.use_faceid = (use_faceid or use_dual_adapter or use_faceid_plus) and HAS_FACEID
            self.use_faceid_plus = use_faceid_plus and HAS_FACEID  # FaceID Plus v2 (ì–¼êµ´+ë¨¸ë¦¬ìŠ¤íƒ€ì¼)
            self.use_clip_blend = use_clip_blend  # CLIP Blending mode

            if self.use_clip_blend:
                self.ip_adapter_mode = "clip_blend"  # CLIP embedding blending
            elif self.use_faceid_plus:
                self.ip_adapter_mode = "faceid_plus"  # FaceID Plus v2 (InsightFace + CLIP)
            elif self.use_dual_adapter:
                self.ip_adapter_mode = "dual"  # FaceID + CLIP
            elif self.use_faceid:
                self.ip_adapter_mode = "faceid"
            else:
                self.ip_adapter_mode = "standard"

        self.use_pre_paste = use_pre_paste  # Pre-paste mode (ì†ŒìŠ¤ ì–¼êµ´ ë¯¸ë¦¬ ë¶™ì—¬ë„£ê¸°)
        self.use_face_swap = use_face_swap and HAS_FACESWAP  # Face Swap mode (ìƒì„± í›„ ì–¼êµ´ êµì²´)
        self.use_swap_refinement = use_swap_refinement  # Face Swap Refinement mode (Face Swap í›„ ê²½ë¯¸í•œ ì¸í˜ì¸íŒ…)
        print(f"[DEBUG __init__] self.use_faceid_plus = {self.use_faceid_plus}")
        print(f"[DEBUG __init__] self.use_pre_paste = {self.use_pre_paste}")
        print(f"[DEBUG __init__] self.use_face_swap = {self.use_face_swap}")
        print(f"[DEBUG __init__] self.use_swap_refinement = {self.use_swap_refinement}")

        if (use_faceid or use_dual_adapter) and not HAS_FACEID:
            print("FaceID ìš”ì²­ë˜ì—ˆìœ¼ë‚˜ InsightFace ë¯¸ì„¤ì¹˜. Standard ëª¨ë“œë¡œ ì „í™˜.")

        # BiSeNet face parser ì´ˆê¸°í™” (ë¨¸ë¦¬ì¹´ë½ ë§ˆìŠ¤í‚¹ìš©)
        self.face_parser = None
        self.use_bisenet = use_bisenet and HAS_FACE_PARSER
        if self.use_bisenet:
            try:
                self.face_parser = FaceParser(device=self.device)
                print("BiSeNet face parser ì¤€ë¹„ ì™„ë£Œ")
            except Exception as e:
                print(f"BiSeNet ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.use_bisenet = False

        # FaceID extractor ì´ˆê¸°í™” (ì •ì²´ì„± ë³´ì¡´ìš©)
        self.face_id_extractor = None
        if self.use_faceid:
            try:
                self.face_id_extractor = FaceIDExtractor(device=self.device)
                if self.face_id_extractor.load():
                    print("FaceID extractor ì¤€ë¹„ ì™„ë£Œ (InsightFace)")
                else:
                    print("FaceID ë¡œë”© ì‹¤íŒ¨, Standard ëª¨ë“œë¡œ ì „í™˜")
                    self.use_faceid = False
                    self.ip_adapter_mode = "standard"
            except Exception as e:
                print(f"FaceID ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.use_faceid = False
                self.ip_adapter_mode = "standard"

        # FaceSwapper ì´ˆê¸°í™” (ìƒì„± í›„ ì–¼êµ´ êµì²´ìš©)
        # CPUì—ì„œ ì‹¤í–‰ - GPU ë©”ëª¨ë¦¬ ì¶©ëŒ ë°©ì§€ (diffusion ëª¨ë¸ì´ GPU ì ìœ )
        self.face_swapper = None
        self.face_swap_model = face_swap_model
        self.face_swap_model_name = None  # Actual model name for logging
        if self.use_face_swap:
            try:
                self.face_swapper = get_face_swapper(model=face_swap_model, device="cpu")  # í•­ìƒ CPU ì‚¬ìš©
                if self.face_swapper.load():
                    # Check actual swapper type (Ghost may fall back to InsightFace)
                    swapper_class = type(self.face_swapper).__name__
                    if swapper_class == "GhostFaceSwapper":
                        self.face_swap_model_name = "Ghost (ê³ í™”ì§ˆ)"
                    else:
                        # InsightFace - show actual model name
                        actual_model = getattr(self.face_swapper, '_model_name', 'inswapper_128')
                        self.face_swap_model_name = f"InsightFace ({actual_model})"
                        if face_swap_model == "ghost":
                            print("âš ï¸ Ghost ì‚¬ìš© ë¶ˆê°€, InsightFaceë¡œ í´ë°±")
                    print(f"FaceSwapper ì¤€ë¹„ ì™„ë£Œ (CPU, {self.face_swap_model_name})")
                else:
                    print("FaceSwapper ë¡œë”© ì‹¤íŒ¨, Face Swap ë¹„í™œì„±í™”")
                    self.use_face_swap = False
            except Exception as e:
                print(f"FaceSwapper ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.use_face_swap = False

        # FaceEnhancer ì´ˆê¸°í™” (ì–¼êµ´ í™”ì§ˆ ê°œì„ ìš© - GFPGAN)
        # CPUì—ì„œ ì‹¤í–‰ - GPU ë©”ëª¨ë¦¬ ì¶©ëŒ ë°©ì§€
        self.use_face_enhance = use_face_enhance and HAS_FACE_ENHANCE
        self.face_enhancer = None
        if self.use_face_enhance:
            try:
                self.face_enhancer = FaceEnhancer(device="cpu", upscale=1)  # upscale=1: ì›ë³¸ í¬ê¸° ìœ ì§€
                if self.face_enhancer.load():
                    print("FaceEnhancer ì¤€ë¹„ ì™„ë£Œ (CPU, GFPGAN v1.4)")
                else:
                    print("FaceEnhancer ë¡œë”© ì‹¤íŒ¨, Face Enhance ë¹„í™œì„±í™”")
                    self.use_face_enhance = False
            except Exception as e:
                print(f"FaceEnhancer ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.use_face_enhance = False

        # dtype ì„¤ì • (CPUëŠ” float32 ì‚¬ìš©)
        self.dtype = torch.float32 if self.device == "cpu" else torch.float16

        # Inpainting íŒŒì´í”„ë¼ì¸
        print("\nRealVisXL V4.0 Inpainting ëª¨ë¸ ë¡œë”© ì¤‘...")
        self.pipeline = AutoPipelineForInpainting.from_pretrained(
            "OzzyGT/RealVisXL_V4.0_inpainting",
            torch_dtype=self.dtype,
            variant="fp16" if self.dtype == torch.float16 else None
        )

        # IP-Adapter ë¡œë“œ (ëª¨ë“œì— ë”°ë¼ ë‹¤ë¥¸ ì–´ëŒ‘í„°)
        # no_ip_adapter ëª¨ë“œë©´ IP-Adapter ë¡œë”© ê±´ë„ˆë›°ê¸°
        if self.no_ip_adapter:
            self.has_ip_adapter = True  # íŒŒì´í”„ë¼ì¸ì€ ì‚¬ìš© ê°€ëŠ¥
            print("IP-Adapter ë¡œë”© ê±´ë„ˆëœ€ (Simple Inpainting ëª¨ë“œ)")
        else:
            self.has_ip_adapter = self._load_ip_adapter()
            if not self.has_ip_adapter:
                return

        self.pipeline.to(self.device)

        # xFormersëŠ” CUDAì—ì„œë§Œ ì‚¬ìš©
        if self.device == "cuda":
            try:
                self.pipeline.enable_xformers_memory_efficient_attention()
                print("xFormers ë©”ëª¨ë¦¬ ìµœì í™” í™œì„±í™”")
            except:
                pass

        # ì–¼êµ´ ê°ì§€ ì´ˆê¸°í™”
        self.detection_method = detection_method
        self._init_face_detection()

        # ëª¨ë“œ ì •ë³´ ì¶œë ¥
        print(f"\ní˜„ì¬ ëª¨ë“œ: {self.ip_adapter_mode.upper()}")
        if self.no_ip_adapter:
            print("  - Simple Inpainting (IP-Adapter ì—†ìŒ)")
            print("  - ìˆœìˆ˜ ì¸í˜ì¸íŒ…ë§Œ ìˆ˜í–‰")
            print("  - Pre-pasteì™€ í•¨ê»˜ ì‚¬ìš© ì‹œ ì–¼êµ´ ì¡°í™”ë¡œìš´ ë¸”ë Œë”©")
        elif self.use_clip_blend:
            print("  - CLIP Blending Mode")
            print("  - ì–¼êµ´/ë¨¸ë¦¬ì¹´ë½ ì˜ì—­ ë³„ë„ CLIP ì¸ì½”ë”©")
            print("  - ê°€ì¤‘ì¹˜ ë¸”ë Œë”©ìœ¼ë¡œ ë‘ íŠ¹ì„± ë™ì‹œ ë°˜ì˜")
            print("  - ì •ì²´ì„± + ë¨¸ë¦¬ì¹´ë½ ìŠ¤íƒ€ì¼ ì „ì´")
        elif self.use_faceid_plus:
            print("  - IP-Adapter FaceID Plus v2")
            print("  - InsightFace 512-dim ì–¼êµ´ ì„ë² ë”© (ì •ì²´ì„±)")
            print("  - CLIP 1024-dim ì´ë¯¸ì§€ ì„ë² ë”© (ë¨¸ë¦¬ìŠ¤íƒ€ì¼)")
            print("  - ì–¼êµ´ + ë¨¸ë¦¬ìŠ¤íƒ€ì¼ ë™ì‹œ ë°˜ì˜!")
        elif self.use_dual_adapter:
            print("  - Dual IP-Adapter (FaceID + CLIP)")
            print("  - InsightFace 512-dim ì–¼êµ´ ì„ë² ë”© (ì •ì²´ì„±)")
            print("  - CLIP ë¨¸ë¦¬ì¹´ë½ ì´ë¯¸ì§€ ì„ë² ë”© (ìŠ¤íƒ€ì¼)")
            print("  - ì–¼êµ´ ì •ì²´ì„± + ë¨¸ë¦¬ì¹´ë½ ìŠ¤íƒ€ì¼ ë™ì‹œ ì „ì´")
        elif self.use_faceid:
            print("  - IP-Adapter FaceID (InsightFace)")
            print("  - InsightFace 512-dim ì–¼êµ´ ì„ë² ë”© ì‚¬ìš©")
            print("  - ì •ì²´ì„± ë³´ì¡´ í–¥ìƒ (ë¨¸ë¦¬ìŠ¤íƒ€ì¼ ë¯¸ë°˜ì˜)")
        else:
            print("  - Standard IP-Adapter")
            print("  - CLIP ì„ë² ë”©ë§Œ ì‚¬ìš©")
            print("  - ì •ì²´ì„± ë³´ì¡´ ì œí•œì ")

        # ì¶”ê°€ ëª¨ë“œ ì •ë³´
        if self.use_pre_paste:
            print("\nğŸ“‹ Pre-paste ëª¨ë“œ í™œì„±í™”")
            print("  - ì†ŒìŠ¤ ì–¼êµ´ì„ ë°°ê²½ì— ë¯¸ë¦¬ ë¶™ì—¬ë„£ê¸°")
            print("  - Denoising strength ìë™ ì¡°ì • (~0.65)")
            print("  - ì–¼êµ´ ìœ„ì¹˜/í¬ê¸° ë” ì •í™•í•˜ê²Œ ìœ ì§€")
        if self.use_face_swap:
            print("\nğŸ”„ Face Swap ëª¨ë“œ í™œì„±í™”")
            model_display = self.face_swap_model_name or self.face_swap_model
            print(f"  - ìƒì„± í›„ {model_display} ì ìš©")
            print("  - ì–¼êµ´ ìœ ì‚¬ë„ í–¥ìƒ")
        print("=" * 70)

    def _load_ip_adapter(self) -> bool:
        """IP-Adapter ë¡œë“œ (ëª¨ë“œì— ë”°ë¼ Standard, FaceID, Dual, ë˜ëŠ” CLIP Blend)"""
        try:
            if self.use_clip_blend:
                # CLIP Blending: Standard IP-Adapter ë¡œë“œ + CLIP ì¸ì½”ë” ì €ì¥
                print("CLIP Blending ëª¨ë“œ: Standard IP-Adapter ë¡œë”© ì¤‘...")
                self.pipeline.load_ip_adapter(
                    "h94/IP-Adapter",
                    subfolder="sdxl_models",
                    weight_name="ip-adapter_sdxl.bin"
                )
                # CLIP ì¸ì½”ë” ì €ì¥ (ìˆ˜ë™ ì„ë² ë”© ì¶”ì¶œìš©)
                self.clip_image_encoder = self.pipeline.image_encoder
                self.clip_image_processor = self.pipeline.feature_extractor
                print("CLIP Blending: Standard IP-Adapter + CLIP ì¸ì½”ë” ì¤€ë¹„ ì™„ë£Œ!")

            elif self.use_dual_adapter:
                # Dual IP-Adapter: Standard (ë¨¸ë¦¬ì¹´ë½ CLIP) + FaceID (ì–¼êµ´)
                # diffusersëŠ” ë¦¬ìŠ¤íŠ¸ë¡œ multiple IP-Adapter ë¡œë”© ì§€ì›
                print("Dual IP-Adapter ë¡œë”© ì¤‘ (Standard + FaceID)...")

                # ë‘ ì–´ëŒ‘í„°ë¥¼ í•œ ë²ˆì— ë¡œë“œ (ë¦¬ìŠ¤íŠ¸ í˜•ì‹)
                self.pipeline.load_ip_adapter(
                    ["h94/IP-Adapter", "h94/IP-Adapter-FaceID"],
                    subfolder=["sdxl_models", ""],
                    weight_name=["ip-adapter_sdxl.bin", "ip-adapter-faceid_sdxl.bin"],
                )
                print("  [1] Standard IP-Adapter (CLIP) ë¡œë”©")
                print("  [2] IP-Adapter FaceID ë¡œë”©")

                # CLIP image encoder ì €ì¥
                self.clip_image_encoder = self.pipeline.image_encoder
                self.clip_image_processor = self.pipeline.feature_extractor

                # ë‘ ì–´ëŒ‘í„°ì˜ ìŠ¤ì¼€ì¼ ì„¤ì • [Standard(hair), FaceID(face)]
                self.pipeline.set_ip_adapter_scale([0.3, 0.6])
                print("Dual IP-Adapter ë¡œë”© ì™„ë£Œ! (scales: hair=0.3, face=0.6)")

            elif self.use_faceid_plus:
                # FaceID Plus v2: InsightFace + CLIP ì´ë¯¸ì§€ ì„ë² ë”© (ë¨¸ë¦¬ìŠ¤íƒ€ì¼ í¬í•¨)
                print("IP-Adapter FaceID Plus v2 ë¡œë”© ì¤‘...")

                # CLIP ì´ë¯¸ì§€ ì¸ì½”ë” ë¡œë“œ (Plus v2 í•„ìˆ˜)
                from transformers import CLIPVisionModelWithProjection
                self.clip_image_encoder = CLIPVisionModelWithProjection.from_pretrained(
                    "laion/CLIP-ViT-H-14-laion2B-s32B-b79K",
                    torch_dtype=self.dtype,
                ).to(self.device)
                print("  CLIP ì´ë¯¸ì§€ ì¸ì½”ë” ë¡œë“œ ì™„ë£Œ")

                # IP-Adapter FaceID Plus v2 ë¡œë“œ
                self.pipeline.load_ip_adapter(
                    "h94/IP-Adapter-FaceID",
                    subfolder="",
                    weight_name="ip-adapter-faceid-plusv2_sdxl.bin",
                    image_encoder_folder=None,  # ì´ë¯¸ ë³„ë„ë¡œ ë¡œë“œí•¨
                )

                # shortcut ì„¤ì • (Plus v2 í•„ìˆ˜)
                self.pipeline.unet.encoder_hid_proj.image_projection_layers[0].shortcut = True
                print("IP-Adapter FaceID Plus v2 ë¡œë”© ì™„ë£Œ! (ì–¼êµ´+ë¨¸ë¦¬ìŠ¤íƒ€ì¼)")
            elif self.use_faceid:
                # FaceID (non-Plus): InsightFace ì„ë² ë”©ë§Œ ì‚¬ìš©
                print("IP-Adapter FaceID ë¡œë”© ì¤‘...")
                self.pipeline.load_ip_adapter(
                    "h94/IP-Adapter-FaceID",
                    subfolder="",
                    weight_name="ip-adapter-faceid_sdxl.bin",
                    image_encoder_folder=None,
                )
                print("IP-Adapter FaceID ë¡œë”© ì™„ë£Œ!")
            else:
                # Standard IP-Adapter (CLIP only)
                print("Standard IP-Adapter ë¡œë”© ì¤‘...")
                self.pipeline.load_ip_adapter(
                    "h94/IP-Adapter",
                    subfolder="sdxl_models",
                    weight_name="ip-adapter_sdxl.bin"
                )
                # CLIP ì¸ì½”ë” ì €ì¥ (Standard ëª¨ë“œì—ì„œë„ í•„ìš”)
                self.clip_image_encoder = self.pipeline.image_encoder
                self.clip_image_processor = self.pipeline.feature_extractor
                print("Standard IP-Adapter ë¡œë”© ì™„ë£Œ!")
            return True
        except Exception as e:
            print(f"IP-Adapter ë¡œë”© ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return False

    def switch_to_faceid(self, scale: float = 0.85) -> bool:
        """
        FaceID ëª¨ë“œë¡œ ì „í™˜ (ëŸ°íƒ€ì„ ì „í™˜)

        Args:
            scale: IP-Adapter scale

        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        if not HAS_FACEID:
            print("InsightFaceê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return False

        if self.ip_adapter_mode == "faceid":
            print("ì´ë¯¸ FaceID ëª¨ë“œì…ë‹ˆë‹¤.")
            return True

        try:
            # FaceID extractor ì´ˆê¸°í™”
            if self.face_id_extractor is None:
                self.face_id_extractor = FaceIDExtractor(device=self.device)
                if not self.face_id_extractor.load():
                    return False

            # IP-Adapter êµì²´ (FaceID)
            print("IP-Adapter FaceIDë¡œ ì „í™˜ ì¤‘...")
            self.pipeline.load_ip_adapter(
                "h94/IP-Adapter-FaceID",
                subfolder="",
                weight_name="ip-adapter-faceid_sdxl.bin",
            )
            self.pipeline.set_ip_adapter_scale(scale)

            self.use_faceid = True
            self.ip_adapter_mode = "faceid"
            print("FaceID Plus v2 ëª¨ë“œë¡œ ì „í™˜ ì™„ë£Œ!")
            return True

        except Exception as e:
            print(f"FaceID ì „í™˜ ì‹¤íŒ¨: {e}")
            return False

    def switch_to_standard(self, scale: float = 0.85) -> bool:
        """
        Standard ëª¨ë“œë¡œ ì „í™˜ (ëŸ°íƒ€ì„ ì „í™˜)

        Args:
            scale: IP-Adapter scale

        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        if self.ip_adapter_mode == "standard":
            print("ì´ë¯¸ Standard ëª¨ë“œì…ë‹ˆë‹¤.")
            return True

        try:
            print("Standard IP-Adapterë¡œ ì „í™˜ ì¤‘...")
            self.pipeline.load_ip_adapter(
                "h94/IP-Adapter",
                subfolder="sdxl_models",
                weight_name="ip-adapter_sdxl.bin"
            )
            self.pipeline.set_ip_adapter_scale(scale)

            self.use_faceid = False
            self.ip_adapter_mode = "standard"
            print("Standard ëª¨ë“œë¡œ ì „í™˜ ì™„ë£Œ!")
            return True

        except Exception as e:
            print(f"Standard ì „í™˜ ì‹¤íŒ¨: {e}")
            return False

    def get_current_mode(self) -> str:
        """í˜„ì¬ IP-Adapter ëª¨ë“œ ë°˜í™˜"""
        return self.ip_adapter_mode

    def _pre_paste_face(
        self,
        background_img: Image.Image,
        source_face_img: Image.Image,
        target_bbox: tuple = None,
        blend_mode: str = "seamless",
        run_folder: str = None
    ) -> Image.Image:
        """
        ì†ŒìŠ¤ ì–¼êµ´ì„ ë°°ê²½ ì´ë¯¸ì§€ì— ë¯¸ë¦¬ ë¶™ì—¬ë„£ê¸° (Pre-paste)

        Inpainting ì „ì— ì†ŒìŠ¤ ì–¼êµ´ì„ ë°°ê²½ì˜ ì–¼êµ´ ìœ„ì¹˜ì— ë¯¸ë¦¬ ë¶™ì—¬ë„£ì–´ì„œ
        ì–¼êµ´ ìœ„ì¹˜ì™€ í¬ê¸°ë¥¼ ë” ì •í™•í•˜ê²Œ ìœ ì§€í•©ë‹ˆë‹¤.

        Args:
            background_img: ë°°ê²½ ì´ë¯¸ì§€ (PIL Image)
            source_face_img: ì†ŒìŠ¤ ì–¼êµ´ ì´ë¯¸ì§€ (PIL Image)
            target_bbox: íƒ€ê²Ÿ ì–¼êµ´ ì˜ì—­ (x1, y1, x2, y2), Noneì´ë©´ ìë™ ê°ì§€
            blend_mode: ë¸”ë Œë”© ëª¨ë“œ ("seamless", "alpha", "direct")
            run_folder: ì¤‘ê°„ ê²°ê³¼ ì €ì¥ í´ë” (ë””ë²„ê¹…ìš©)

        Returns:
            ì†ŒìŠ¤ ì–¼êµ´ì´ ë¶™ì—¬ë„£ì–´ì§„ ì´ë¯¸ì§€ (PIL Image)
        """
        print("\nğŸ“‹ Pre-paste: ì†ŒìŠ¤ ì–¼êµ´ ë¯¸ë¦¬ ë¶™ì—¬ë„£ê¸°...")

        # ë””ë²„ê¹…: ì†ŒìŠ¤ ì–¼êµ´ ì €ì¥
        if run_folder:
            src_path = os.path.join(run_folder, "2.1_prepaste_source_face.png")
            source_face_img.save(src_path)
            print(f"   Pre-paste ì†ŒìŠ¤ ì–¼êµ´ ì €ì¥: {os.path.basename(src_path)}")

        bg_array = np.array(background_img)
        src_array = np.array(source_face_img)

        # ë°°ê²½ì—ì„œ íƒ€ê²Ÿ ì–¼êµ´ ìœ„ì¹˜ ê°ì§€
        if target_bbox is None:
            bg_bgr = bg_array[:, :, ::-1]
            if self.face_cascade is not None:
                gray = cv2.cvtColor(bg_bgr, cv2.COLOR_BGR2GRAY)
                faces = self.face_cascade.detectMultiScale(gray, 1.1, 4)
                if len(faces) > 0:
                    x, y, w, h = max(faces, key=lambda f: f[2] * f[3])
                    # ì–¼êµ´ ì˜ì—­ í™•ì¥ (ë¨¸ë¦¬ì¹´ë½ í¬í•¨)
                    expand = 0.5
                    x1 = max(0, int(x - w * expand))
                    y1 = max(0, int(y - h * expand * 1.2))  # ìœ„ìª½ ë” í™•ì¥ (ì´ë§ˆ/ë¨¸ë¦¬)
                    x2 = min(bg_array.shape[1], int(x + w + w * expand))
                    y2 = min(bg_array.shape[0], int(y + h + h * expand * 0.5))
                    target_bbox = (x1, y1, x2, y2)
                    print(f"   íƒ€ê²Ÿ ì–¼êµ´ ì˜ì—­: {target_bbox}")

        if target_bbox is None:
            print("   âš ï¸ ë°°ê²½ì—ì„œ ì–¼êµ´ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. Pre-paste ê±´ë„ˆëœ€.")
            return background_img

        x1, y1, x2, y2 = target_bbox
        target_w = x2 - x1
        target_h = y2 - y1

        # ë””ë²„ê¹…: íƒ€ê²Ÿ ì˜ì—­ ì‹œê°í™” (ë°°ê²½ì— ë°•ìŠ¤ í‘œì‹œ)
        if run_folder:
            target_vis = bg_array.copy()
            cv2.rectangle(target_vis, (x1, y1), (x2, y2), (0, 255, 0), 3)
            cv2.putText(target_vis, "Target Face Area", (x1, y1-10),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
            target_vis_path = os.path.join(run_folder, "2.2_prepaste_target_area.png")
            Image.fromarray(target_vis).save(target_vis_path)
            print(f"   Pre-paste íƒ€ê²Ÿ ì˜ì—­ ì €ì¥: {os.path.basename(target_vis_path)}")

        # ì†ŒìŠ¤ ì–¼êµ´ì—ì„œ ì–¼êµ´ ì˜ì—­ ê°ì§€
        src_bgr = src_array[:, :, ::-1]
        src_bbox = None
        if self.face_cascade is not None:
            gray = cv2.cvtColor(src_bgr, cv2.COLOR_BGR2GRAY)
            faces = self.face_cascade.detectMultiScale(gray, 1.1, 4)
            if len(faces) > 0:
                sx, sy, sw, sh = max(faces, key=lambda f: f[2] * f[3])
                # ì–¼êµ´ ì˜ì—­ í™•ì¥
                expand = 0.4
                sx1 = max(0, int(sx - sw * expand))
                sy1 = max(0, int(sy - sh * expand * 1.0))
                sx2 = min(src_array.shape[1], int(sx + sw + sw * expand))
                sy2 = min(src_array.shape[0], int(sy + sh + sh * expand * 0.3))
                src_bbox = (sx1, sy1, sx2, sy2)

        # ì†ŒìŠ¤ ì–¼êµ´ í¬ë¡­ ë° ë¦¬ì‚¬ì´ì¦ˆ
        if src_bbox:
            sx1, sy1, sx2, sy2 = src_bbox
            src_cropped = src_array[sy1:sy2, sx1:sx2]
        else:
            src_cropped = src_array

        # íƒ€ê²Ÿ í¬ê¸°ì— ë§ê²Œ ë¦¬ì‚¬ì´ì¦ˆ
        src_resized = cv2.resize(src_cropped, (target_w, target_h), interpolation=cv2.INTER_LANCZOS4)

        # ë””ë²„ê¹…: í¬ë¡­/ë¦¬ì‚¬ì´ì¦ˆëœ ì†ŒìŠ¤ ì–¼êµ´ ì €ì¥
        if run_folder:
            cropped_path = os.path.join(run_folder, "2.3_prepaste_source_cropped.png")
            Image.fromarray(src_cropped).save(cropped_path)
            print(f"   Pre-paste í¬ë¡­ëœ ì†ŒìŠ¤ ì €ì¥: {os.path.basename(cropped_path)}")

            resized_path = os.path.join(run_folder, "2.4_prepaste_source_resized.png")
            Image.fromarray(src_resized).save(resized_path)
            print(f"   Pre-paste ë¦¬ì‚¬ì´ì¦ˆëœ ì†ŒìŠ¤ ì €ì¥: {os.path.basename(resized_path)}")

        # ë¸”ë Œë”©
        result = bg_array.copy()

        if blend_mode == "seamless":
            # OpenCV seamlessClone ì‚¬ìš©
            try:
                # BiSeNetìœ¼ë¡œ ì •êµí•œ ë§ˆìŠ¤í¬ ìƒì„± ì‹œë„
                mask = None
                if self.use_bisenet and self.face_parser is not None:
                    try:
                        # ì†ŒìŠ¤ ì–¼êµ´ í¬ë¡­ ì´ë¯¸ì§€ì—ì„œ BiSeNet ë§ˆìŠ¤í¬ ìƒì„±
                        src_cropped_pil = Image.fromarray(src_cropped)
                        bisenet_mask = self.face_parser.get_face_hair_mask(
                            src_cropped_pil,
                            target_size=src_cropped_pil.size,  # í¬ë¡­ ì´ë¯¸ì§€ í¬ê¸°
                            include_hair=True,
                            include_neck=False,
                            blur_radius=0,  # ë¸”ëŸ¬ ì—†ì´ (ë‚˜ì¤‘ì— ë³„ë„ë¡œ ì ìš©)
                            expand_ratio=1.0  # í™•ì¥ ì—†ì´ ì •í™•í•œ ì˜ì—­ë§Œ
                        )
                        if bisenet_mask is not None:
                            # ë§ˆìŠ¤í¬ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜
                            bisenet_mask_array = np.array(bisenet_mask.convert('L'))

                            # ë””ë²„ê¹…: BiSeNet ì›ë³¸ ë§ˆìŠ¤í¬ ì €ì¥ (ë¦¬ì‚¬ì´ì¦ˆ ì „)
                            if run_folder:
                                raw_mask_path = os.path.join(run_folder, "2.5a_prepaste_bisenet_raw_mask.png")
                                Image.fromarray(bisenet_mask_array).save(raw_mask_path)
                                print(f"   BiSeNet ì›ë³¸ ë§ˆìŠ¤í¬ ì €ì¥: {os.path.basename(raw_mask_path)}")

                            # íƒ€ê²Ÿ í¬ê¸°ì— ë§ê²Œ ë¦¬ì‚¬ì´ì¦ˆ
                            mask = cv2.resize(bisenet_mask_array, (target_w, target_h), interpolation=cv2.INTER_LINEAR)

                            # ë””ë²„ê¹…: ë¦¬ì‚¬ì´ì¦ˆëœ ë§ˆìŠ¤í¬ ì €ì¥ (ë¸”ëŸ¬ ì „)
                            if run_folder:
                                resized_mask_path = os.path.join(run_folder, "2.5b_prepaste_mask_resized.png")
                                Image.fromarray(mask).save(resized_mask_path)
                                print(f"   ë¦¬ì‚¬ì´ì¦ˆ ë§ˆìŠ¤í¬ ì €ì¥: {os.path.basename(resized_mask_path)}")

                            # seamlessCloneì„ ìœ„í•´ ì™„ì „ ì´ì§„ ë§ˆìŠ¤í¬ë¡œ ë³€í™˜
                            # GaussianBlurëŠ” seamlessCloneì—ì„œ ìì²´ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ë¯€ë¡œ í•„ìš”ì—†ìŒ
                            _, mask = cv2.threshold(mask, 100, 255, cv2.THRESH_BINARY)

                            # ëª¨í´ë¡œì§€ ì—°ì‚°ìœ¼ë¡œ ê°€ì¥ìë¦¬ ì •ë¦¬ (ë¸”ëŸ¬ ëŒ€ì‹ )
                            kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))
                            mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)
                            mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)

                            # ì™„ì „ ì´ì§„í™” ë³´ì¥ (ì¤‘ê°„ê°’ ì œê±°)
                            mask = np.where(mask >= 128, 255, 0).astype(np.uint8)
                            print(f"   BiSeNet ì´ì§„ ë§ˆìŠ¤í¬ ìƒì„±: min={mask.min()}, max={mask.max()}, nonzero={np.count_nonzero(mask)}")
                    except Exception as e:
                        print(f"   BiSeNet ë§ˆìŠ¤í¬ ì‹¤íŒ¨: {e}, íƒ€ì› ë§ˆìŠ¤í¬ë¡œ ëŒ€ì²´")
                        mask = None

                # BiSeNet ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ íƒ€ì› ë§ˆìŠ¤í¬ ì‚¬ìš©
                if mask is None:
                    mask = np.zeros((target_h, target_w), dtype=np.uint8)
                    center = (target_w // 2, target_h // 2)
                    axes = (int(target_w * 0.45), int(target_h * 0.48))
                    cv2.ellipse(mask, center, axes, 0, 0, 360, 255, -1)
                    # ì´ë¯¸ ì™„ì „ ì´ì§„ ë§ˆìŠ¤í¬ (cv2.ellipseê°€ 255ë¡œ ì±„ì›€)
                    print(f"   íƒ€ì›í˜• ê¸°ë³¸ ë§ˆìŠ¤í¬ ì‚¬ìš©: min={mask.min()}, max={mask.max()}")

                # ë””ë²„ê¹…: ë¸”ë Œë”© ë§ˆìŠ¤í¬ ì €ì¥
                if run_folder:
                    mask_path = os.path.join(run_folder, "2.5_prepaste_blend_mask.png")
                    Image.fromarray(mask).save(mask_path)
                    print(f"   Pre-paste ë¸”ë Œë”© ë§ˆìŠ¤í¬ ì €ì¥: {os.path.basename(mask_path)}")

                # seamlessClone center ê³„ì‚°
                clone_center = (x1 + target_w // 2, y1 + target_h // 2)

                # BGR ë³€í™˜
                result_bgr = result[:, :, ::-1].copy()
                src_resized_bgr = src_resized[:, :, ::-1]

                # seamlessCloneìš© ì™„ì „ ì´ì§„ ë§ˆìŠ¤í¬ í™•ì¸ (ì´ë¯¸ ì´ì§„í™”ë¨)
                binary_mask = mask.copy()
                # í˜¹ì‹œ ëª¨ë¥´ë‹ˆ í•œë²ˆ ë” ì´ì§„í™” ë³´ì¥
                binary_mask = np.where(binary_mask >= 128, 255, 0).astype(np.uint8)

                # ë§ˆìŠ¤í¬ ìœ íš¨ì„± ê²€ì‚¬
                unique_vals = np.unique(binary_mask)
                nonzero_ratio = np.count_nonzero(binary_mask) / binary_mask.size
                print(f"   ìµœì¢… ë§ˆìŠ¤í¬: ê³ ìœ ê°’={unique_vals}, ë¹„ìœ¨={nonzero_ratio:.2%}")

                if nonzero_ratio < 0.01:
                    print("   âš ï¸ ë§ˆìŠ¤í¬ ì˜ì—­ì´ ë„ˆë¬´ ì‘ìŒ! íƒ€ì› ë§ˆìŠ¤í¬ë¡œ ëŒ€ì²´")
                    binary_mask = np.zeros((target_h, target_w), dtype=np.uint8)
                    center = (target_w // 2, target_h // 2)
                    axes = (int(target_w * 0.45), int(target_h * 0.48))
                    cv2.ellipse(binary_mask, center, axes, 0, 0, 360, 255, -1)

                # ë””ë²„ê¹…: ìµœì¢… ì´ì§„ ë§ˆìŠ¤í¬ ì €ì¥
                if run_folder:
                    binary_mask_path = os.path.join(run_folder, "2.5c_prepaste_binary_mask.png")
                    Image.fromarray(binary_mask).save(binary_mask_path)
                    print(f"   ìµœì¢… ì´ì§„ ë§ˆìŠ¤í¬ ì €ì¥: {os.path.basename(binary_mask_path)}")

                # Seamless clone
                print(f"   seamlessClone í˜¸ì¶œ: src={src_resized_bgr.shape}, dst={result_bgr.shape}, mask={binary_mask.shape}, center={clone_center}")
                result_bgr = cv2.seamlessClone(
                    src_resized_bgr, result_bgr, binary_mask,
                    clone_center, cv2.NORMAL_CLONE
                )
                result = result_bgr[:, :, ::-1]
                print("   âœ… Seamless clone ì ìš© ì™„ë£Œ (ë¶ˆíˆ¬ëª… í•©ì„±)")

            except Exception as e:
                import traceback
                print(f"   âŒ Seamless clone ì‹¤íŒ¨!")
                print(f"   ì—ëŸ¬: {e}")
                traceback.print_exc()
                print("   â†’ alpha ë¸”ë Œë”©ìœ¼ë¡œ ëŒ€ì²´ (ê·¸ë¼ë””ì–¸íŠ¸ ë§ˆìŠ¤í¬ ì‚¬ìš©)")
                blend_mode = "alpha"

        if blend_mode == "alpha":
            # Alpha ë¸”ë Œë”© (ê·¸ë¼ë””ì–¸íŠ¸ ë§ˆìŠ¤í¬) - ë°˜íˆ¬ëª… íš¨ê³¼ ë°œìƒ!
            print("   âš ï¸ Alpha ë¸”ë Œë”© ì‚¬ìš© - ì´ ëª¨ë“œëŠ” ë°˜íˆ¬ëª… íš¨ê³¼ë¥¼ ì¼ìœ¼í‚¬ ìˆ˜ ìˆìŒ!")

            # ê·¸ë¼ë””ì–¸íŠ¸ ëŒ€ì‹  ì´ì§„ ë§ˆìŠ¤í¬ë¡œ ì§ì ‘ í•©ì„± ì‹œë„
            # ì´ë ‡ê²Œ í•˜ë©´ ë°˜íˆ¬ëª… ë¬¸ì œ í•´ê²°ë¨
            mask = np.zeros((target_h, target_w), dtype=np.float32)
            center = (target_w // 2, target_h // 2)
            axes = (int(target_w * 0.45), int(target_h * 0.48))

            # íƒ€ì› ë§ˆìŠ¤í¬ë¥¼ floatë¡œ ìƒì„± (1.0 = ë¶ˆíˆ¬ëª…)
            mask_uint8 = np.zeros((target_h, target_w), dtype=np.uint8)
            cv2.ellipse(mask_uint8, center, axes, 0, 0, 360, 255, -1)
            # ê°€ì¥ìë¦¬ë§Œ ì•„ì£¼ ì‚´ì§ ë¸”ëŸ¬ (5í”½ì…€)
            mask_uint8 = cv2.GaussianBlur(mask_uint8, (11, 11), 0)
            mask = mask_uint8.astype(np.float32) / 255.0
            mask_3d = mask[:, :, np.newaxis]

            # ë¸”ë Œë”©
            region = result[y1:y2, x1:x2].astype(np.float32)
            src_float = src_resized.astype(np.float32)
            blended = region * (1 - mask_3d) + src_float * mask_3d
            result[y1:y2, x1:x2] = blended.astype(np.uint8)
            print("   Alpha ë¸”ë Œë”© ì ìš© ì™„ë£Œ (ê°œì„ ëœ íƒ€ì› ë§ˆìŠ¤í¬)")

        elif blend_mode == "direct":
            # ì§ì ‘ ë¶™ì—¬ë„£ê¸°
            result[y1:y2, x1:x2] = src_resized
            print("   ì§ì ‘ ë¶™ì—¬ë„£ê¸° ì™„ë£Œ")

        return Image.fromarray(result)

    def _apply_face_swap(
        self,
        result_image: Image.Image,
        source_face_img: Image.Image,
        run_folder: str = None
    ) -> Image.Image:
        """
        ìƒì„±ëœ ê²°ê³¼ì— InsightFace Face Swap ì ìš©

        Args:
            result_image: ìƒì„±ëœ ê²°ê³¼ ì´ë¯¸ì§€ (PIL Image)
            source_face_img: ì†ŒìŠ¤ ì–¼êµ´ ì´ë¯¸ì§€ (PIL Image)
            run_folder: ì¤‘ê°„ ê²°ê³¼ ì €ì¥ í´ë” (ë””ë²„ê¹…ìš©)

        Returns:
            Face swapì´ ì ìš©ëœ ì´ë¯¸ì§€ (PIL Image)
        """
        if self.face_swapper is None:
            print("   âš ï¸ FaceSwapperê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return result_image

        # Use stored model name
        model_name = self.face_swap_model_name or self.face_swap_model
        print(f"\nğŸ”„ Face Swap: {model_name} ì ìš© ì¤‘...")

        # ë””ë²„ê¹…: ì†ŒìŠ¤ ì–¼êµ´ ì €ì¥
        if run_folder:
            src_path = os.path.join(run_folder, "6.0_faceswap_source.png")
            source_face_img.save(src_path)
            print(f"   Face Swap ì†ŒìŠ¤ ì–¼êµ´ ì €ì¥: {os.path.basename(src_path)}")

        try:
            swapped = self.face_swapper.swap_face(result_image, source_face_img)
            if swapped is not None:
                print("   Face Swap ì™„ë£Œ!")
                # ë””ë²„ê¹…: Face Swap ê²°ê³¼ ì €ì¥
                if run_folder:
                    swap_result_path = os.path.join(run_folder, "6.1_faceswap_result.png")
                    swapped.save(swap_result_path)
                    print(f"   Face Swap ê²°ê³¼ ì €ì¥: {os.path.basename(swap_result_path)}")
                return swapped
            else:
                print("   âš ï¸ Face Swap ì‹¤íŒ¨, ì›ë³¸ ê²°ê³¼ ë°˜í™˜")
                return result_image
        except Exception as e:
            print(f"   âš ï¸ Face Swap ì˜¤ë¥˜: {e}")
            return result_image

    def _apply_face_enhance(
        self,
        result_image: Image.Image,
        strength: float = 0.8,
        run_folder: str = None
    ) -> Image.Image:
        """
        GFPGANìœ¼ë¡œ ì–¼êµ´ í™”ì§ˆ ê°œì„ 

        Args:
            result_image: ì…ë ¥ ì´ë¯¸ì§€ (PIL Image)
            strength: ê°œì„  ê°•ë„ (0.0=ì›ë³¸, 1.0=ì™„ì „ ê°œì„ )
            run_folder: ì¤‘ê°„ ê²°ê³¼ ì €ì¥ í´ë” (ë””ë²„ê¹…ìš©)

        Returns:
            í™”ì§ˆ ê°œì„ ëœ ì´ë¯¸ì§€ (PIL Image)
        """
        print("\nğŸ”§ Face Enhance (GFPGAN) ì ìš© ì¤‘...")

        if self.face_enhancer is None:
            print("   âš ï¸ FaceEnhancerê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ, ì›ë³¸ ë°˜í™˜")
            return result_image

        try:
            if strength >= 1.0:
                # ì™„ì „ ê°œì„ 
                enhanced = self.face_enhancer.enhance(result_image, only_center_face=True, paste_back=True)
            else:
                # ë¶€ë¶„ ë¸”ë Œë”©
                enhanced = self.face_enhancer.enhance_face_region(result_image, blend_ratio=strength)

            if enhanced is not None:
                print(f"   Face Enhance ì™„ë£Œ! (ê°•ë„: {strength:.0%})")
                # ë””ë²„ê¹…: Face Enhance ê²°ê³¼ ì €ì¥
                if run_folder:
                    enhance_result_path = os.path.join(run_folder, "6.2_face_enhance_result.png")
                    enhanced.save(enhance_result_path)
                    print(f"   Face Enhance ê²°ê³¼ ì €ì¥: {os.path.basename(enhance_result_path)}")
                return enhanced
            else:
                print("   âš ï¸ Face Enhance ì‹¤íŒ¨, ì›ë³¸ ê²°ê³¼ ë°˜í™˜")
                return result_image
        except Exception as e:
            print(f"   âš ï¸ Face Enhance ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            return result_image

    def _apply_swap_refinement(
        self,
        swapped_image: Image.Image,
        prompt: str,
        denoising_strength: float = 0.3,
        guidance_scale: float = 7.5,
        num_steps: int = 20,
        seed: int = None,
        run_folder: str = None
    ) -> Image.Image:
        """
        Face Swap í›„ ì–¼êµ´ ì˜ì—­ì— ê²½ë¯¸í•œ ì¸í˜ì¸íŒ…ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë¸”ë Œë”©

        Face Swapì€ ì–¼êµ´ì„ êµì²´í•˜ì§€ë§Œ ê²½ê³„ê°€ ë¶€ìì—°ìŠ¤ëŸ¬ìš¸ ìˆ˜ ìˆìŒ.
        ì´ ë©”ì„œë“œëŠ” ì–¼êµ´ ì˜ì—­ì—ë§Œ ë‚®ì€ denoisingìœ¼ë¡œ ê°€ë³ê²Œ ì¸í˜ì¸íŒ…í•˜ì—¬
        ìì—°ìŠ¤ëŸ¬ìš´ ë¸”ë Œë”©ì„ ë‹¬ì„±í•¨.

        Args:
            swapped_image: Face Swapì´ ì ìš©ëœ ì´ë¯¸ì§€ (PIL Image)
            prompt: ì¸í˜ì¸íŒ… í”„ë¡¬í”„íŠ¸
            denoising_strength: Denoising ê°•ë„ (0.1~0.5 ê¶Œì¥, ë‚®ì„ìˆ˜ë¡ ì›ë³¸ ìœ ì§€)
            guidance_scale: ê°€ì´ë˜ìŠ¤ ìŠ¤ì¼€ì¼
            num_steps: ì¶”ë¡  ìŠ¤í… ìˆ˜ (ë¹ ë¥¸ ì •ì œë¥¼ ìœ„í•´ ì ì€ ìŠ¤í… ì‚¬ìš©)
            seed: ëœë¤ ì‹œë“œ
            run_folder: ì¤‘ê°„ ê²°ê³¼ ì €ì¥ í´ë” (ë””ë²„ê¹…ìš©)

        Returns:
            ì •ì œëœ ì´ë¯¸ì§€ (PIL Image)
        """
        print(f"\nğŸ”§ Face Swap Refinement ì ìš© ì¤‘... (denoising: {denoising_strength:.2f})")

        # íŒŒì´í”„ë¼ì¸ì´ ì—†ìœ¼ë©´ ì›ë³¸ ë°˜í™˜
        if not self.has_ip_adapter and not self.no_ip_adapter:
            print("   âš ï¸ ì¸í˜ì¸íŒ… íŒŒì´í”„ë¼ì¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            return swapped_image

        # BiSeNetìœ¼ë¡œ ì–¼êµ´ ë§ˆìŠ¤í¬ ìƒì„±
        if self.face_parser is None:
            print("   âš ï¸ BiSeNetì´ ì—†ì–´ ì „ì²´ ì´ë¯¸ì§€ ë¦¬íŒŒì¸ë¨¼íŠ¸ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
            # BiSeNetì´ ì—†ìœ¼ë©´ ê°„ë‹¨í•œ ì¤‘ì•™ ì˜ì—­ ë§ˆìŠ¤í¬ ì‚¬ìš©
            w, h = swapped_image.size
            mask = Image.new("L", (w, h), 0)
            # ì¤‘ì•™ 60% ì˜ì—­ì— ë§ˆìŠ¤í¬
            margin_x = int(w * 0.2)
            margin_y = int(h * 0.15)
            for y in range(margin_y, h - margin_y):
                for x in range(margin_x, w - margin_x):
                    mask.putpixel((x, y), 255)
            mask = mask.filter(ImageFilter.GaussianBlur(radius=30))
        else:
            try:
                # BiSeNetìœ¼ë¡œ ì •í™•í•œ ì–¼êµ´ ë§ˆìŠ¤í¬ ìƒì„± (ì–¼êµ´ë§Œ, ë¨¸ë¦¬ì¹´ë½ ì œì™¸)
                face_mask = self.face_parser.get_face_hair_mask(
                    swapped_image,
                    include_hair=False,  # ë¨¸ë¦¬ì¹´ë½ ì œì™¸ (ì–¼êµ´ë§Œ)
                    include_neck=False,
                    blur_radius=10,
                    expand_ratio=1.15  # ì•½ê°„ í™•ì¥
                )
                if face_mask is not None:
                    mask = face_mask
                else:
                    raise ValueError("BiSeNet failed to generate mask")
            except Exception as e:
                print(f"   âš ï¸ ë§ˆìŠ¤í¬ ìƒì„± ì‹¤íŒ¨: {e}, ì¤‘ì•™ ì˜ì—­ ë§ˆìŠ¤í¬ ì‚¬ìš©")
                w, h = swapped_image.size
                mask = Image.new("L", (w, h), 0)
                margin_x = int(w * 0.2)
                margin_y = int(h * 0.15)
                for y in range(margin_y, h - margin_y):
                    for x in range(margin_x, w - margin_x):
                        mask.putpixel((x, y), 255)
                mask = mask.filter(ImageFilter.GaussianBlur(radius=30))

        # ë§ˆìŠ¤í¬ ì €ì¥ (ë””ë²„ê¹…ìš©)
        if run_folder:
            refinement_mask_path = os.path.join(run_folder, "6.3_swap_refinement_mask.png")
            mask.save(refinement_mask_path)
            print(f"   Refinement ë§ˆìŠ¤í¬ ì €ì¥: {os.path.basename(refinement_mask_path)}")

        # Generator ì„¤ì •
        if seed is not None:
            generator = torch.Generator(device=self.device).manual_seed(seed)
        else:
            generator = None

        try:
            # IP-Adapterê°€ ë¡œë“œëœ ê²½ìš°, ì œë¡œ ì„ë² ë”© ì „ë‹¬ (ì •ì œ ì‹œì—ëŠ” IP-Adapter ì˜í–¥ ì—†ì´)
            pipeline_kwargs = {
                "prompt": prompt,
                "image": swapped_image,
                "mask_image": mask,
                "num_inference_steps": num_steps,
                "guidance_scale": guidance_scale,
                "strength": denoising_strength,
                "generator": generator,
            }

            # IP-Adapterê°€ ë¡œë“œëœ ìƒíƒœë©´ ì„ë² ë”© í•„ìš”
            if self.has_ip_adapter:
                # IP-Adapter scaleì„ 0ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ì˜í–¥ ì œê±°
                original_scale = self.pipeline.get_ip_adapter_scale() if hasattr(self.pipeline, 'get_ip_adapter_scale') else None
                self.pipeline.set_ip_adapter_scale(0.0)

                # ì œë¡œ ì„ë² ë”© ì „ë‹¬ (FaceID Plus v2: shape (2, 1, 512))
                zero_embedding = torch.zeros(2, 1, 512, dtype=self.dtype, device=self.device)
                pipeline_kwargs["ip_adapter_image_embeds"] = [zero_embedding]

            # ì¸í˜ì¸íŒ… ìˆ˜í–‰ (ë‚®ì€ denoisingìœ¼ë¡œ ê°€ë²¼ìš´ ì •ì œ)
            result = self.pipeline(**pipeline_kwargs)

            # IP-Adapter scale ë³µì›
            if self.has_ip_adapter and original_scale is not None:
                self.pipeline.set_ip_adapter_scale(original_scale)

            refined_image = result.images[0]

            # í¬ê¸°ê°€ ë‹¤ë¥´ë©´ ë³µì›
            if refined_image.size != swapped_image.size:
                refined_image = refined_image.resize(swapped_image.size, Image.Resampling.LANCZOS)

            print(f"   Swap Refinement ì™„ë£Œ!")

            # ê²°ê³¼ ì €ì¥ (ë””ë²„ê¹…ìš©)
            if run_folder:
                refinement_result_path = os.path.join(run_folder, "6.4_swap_refinement_result.png")
                refined_image.save(refinement_result_path)
                print(f"   Refinement ê²°ê³¼ ì €ì¥: {os.path.basename(refinement_result_path)}")

            return refined_image

        except Exception as e:
            print(f"   âš ï¸ Swap Refinement ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            return swapped_image

    def _create_face_hair_composite(
        self,
        source_face: Image.Image,
        hair_region: Image.Image,
        face_weight: float = 0.6,
        hair_weight: float = 0.4
    ) -> Image.Image:
        """
        ì–¼êµ´ê³¼ ë¨¸ë¦¬ì¹´ë½ ì˜ì—­ì„ ê°€ì¤‘ì¹˜ ë¸”ë Œë”©í•˜ì—¬ í•©ì„± ì´ë¯¸ì§€ ìƒì„±

        BiSeNetìœ¼ë¡œ ì¶”ì¶œí•œ ì–¼êµ´/ë¨¸ë¦¬ì¹´ë½ ì˜ì—­ì„ ê¸°ë°˜ìœ¼ë¡œ
        ì–¼êµ´ íŠ¹ì§•ê³¼ ë¨¸ë¦¬ì¹´ë½ ìŠ¤íƒ€ì¼ì„ ëª¨ë‘ ê°•ì¡°í•œ í•©ì„± ì´ë¯¸ì§€ ìƒì„±

        Args:
            source_face: ì›ë³¸ ì–¼êµ´ ì´ë¯¸ì§€
            hair_region: BiSeNetìœ¼ë¡œ ì¶”ì¶œí•œ ë¨¸ë¦¬ì¹´ë½ ì˜ì—­ ì´ë¯¸ì§€
            face_weight: ì–¼êµ´ ì˜ì—­ ê°€ì¤‘ì¹˜ (ê¸°ë³¸: 0.6)
            hair_weight: ë¨¸ë¦¬ì¹´ë½ ì˜ì—­ ê°€ì¤‘ì¹˜ (ê¸°ë³¸: 0.4)

        Returns:
            í•©ì„±ëœ ì´ë¯¸ì§€ (PIL Image)
        """
        # numpy ë°°ì—´ë¡œ ë³€í™˜
        face_array = np.array(source_face).astype(np.float32)
        hair_array = np.array(hair_region).astype(np.float32)

        # ë¨¸ë¦¬ì¹´ë½ ì˜ì—­ ë§ˆìŠ¤í¬ ìƒì„± (íšŒìƒ‰(128)ì´ ì•„ë‹Œ ì˜ì—­ì´ ë¨¸ë¦¬ì¹´ë½)
        hair_mask = np.any(np.abs(hair_array - 128) > 20, axis=2).astype(np.float32)

        # ê°€ì¤‘ì¹˜ ì •ê·œí™” (hair_weightë§Œ ì‚¬ìš©)
        total = face_weight + hair_weight
        hair_w = hair_weight / total

        # ë§ˆìŠ¤í¬ í™•ì¥ (3ì±„ë„)
        hair_mask_3d = hair_mask[:, :, np.newaxis]

        # ê°€ì¤‘ì¹˜ ë¸”ë Œë”©:
        # - ë¨¸ë¦¬ì¹´ë½ ì˜ì—­ì—ì„œ ë¨¸ë¦¬ì¹´ë½ ì´ë¯¸ì§€ì˜ ìƒ‰ìƒ/ì§ˆê°ì„ ë” ë°˜ì˜
        # - ì–¼êµ´ ì˜ì—­ì€ ì›ë³¸ ìœ ì§€
        blended = face_array.copy()
        blended = blended * (1 - hair_mask_3d * hair_w) + hair_array * (hair_mask_3d * hair_w)

        # í´ë¦¬í•‘ ë° ë³€í™˜
        blended = np.clip(blended, 0, 255).astype(np.uint8)

        return Image.fromarray(blended)

    def _blend_clip_embeddings(
        self,
        face_embeds: torch.Tensor,
        hair_embeds: torch.Tensor,
        face_weight: float = 0.6,
        hair_weight: float = 0.4
    ) -> torch.Tensor:
        """
        ì–¼êµ´ê³¼ ë¨¸ë¦¬ì¹´ë½ CLIP ì„ë² ë”© ë¸”ë Œë”©

        Args:
            face_embeds: ì–¼êµ´ CLIP ì„ë² ë”©
            hair_embeds: ë¨¸ë¦¬ì¹´ë½ CLIP ì„ë² ë”©
            face_weight: ì–¼êµ´ ê°€ì¤‘ì¹˜
            hair_weight: ë¨¸ë¦¬ì¹´ë½ ê°€ì¤‘ì¹˜

        Returns:
            ë¸”ë Œë”©ëœ ì„ë² ë”©
        """
        # ê°€ì¤‘ì¹˜ ì •ê·œí™”
        total = face_weight + hair_weight
        face_w = face_weight / total
        hair_w = hair_weight / total

        # ë¸”ë Œë”©
        blended = face_embeds * face_w + hair_embeds * hair_w

        return blended

    def _init_face_detection(self):
        """ì–¼êµ´ ê°ì§€ ì´ˆê¸°í™”"""
        if self.detection_method == 'mediapipe':
            try:
                import mediapipe as mp
                self.mp_face_detection = mp.solutions.face_detection
                self.face_detection = self.mp_face_detection.FaceDetection(
                    model_selection=1,
                    min_detection_confidence=0.5
                )
                print("âœ… MediaPipe ì–¼êµ´ ê°ì§€ ì¤€ë¹„ ì™„ë£Œ")
            except ImportError:
                print("âš ï¸ MediaPipe ì—†ìŒ. OpenCVë¡œ ì „í™˜...")
                self.detection_method = 'opencv'

        if self.detection_method == 'opencv':
            cascade_path = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
            self.face_cascade = cv2.CascadeClassifier(cascade_path)
            print("âœ… OpenCV ì–¼êµ´ ê°ì§€ ì¤€ë¹„ ì™„ë£Œ")

    def detect_face(self, image_path):
        """
        ì–¼êµ´ ê°ì§€

        Returns:
            (x, y, w, h) ë˜ëŠ” None
        """
        image = cv2.imread(image_path)
        if image is None:
            return None

        if self.detection_method == 'mediapipe':
            rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            results = self.face_detection.process(rgb_image)

            if not results.detections:
                return None

            detection = results.detections[0]
            bbox = detection.location_data.relative_bounding_box

            h, w = image.shape[:2]
            x = int(bbox.xmin * w)
            y = int(bbox.ymin * h)
            box_w = int(bbox.width * w)
            box_h = int(bbox.height * h)

            return (x, y, box_w, box_h)

        else:  # opencv
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            faces = self.face_cascade.detectMultiScale(
                gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30)
            )

            if len(faces) == 0:
                return None

            # ê°€ì¥ í° ì–¼êµ´
            largest_face = max(faces, key=lambda f: f[2] * f[3])
            return tuple(largest_face)

    def create_face_mask(self, image_path, expand_ratio=0.3, feather=15, include_hair=True, include_neck=False):
        """
        ì´ë¯¸ì§€ì—ì„œ ì–¼êµ´ ìë™ ê°ì§€ í›„ ë§ˆìŠ¤í¬ ìƒì„±

        Args:
            image_path: ì´ë¯¸ì§€ ê²½ë¡œ
            expand_ratio: ì–¼êµ´ ì˜ì—­ í™•ì¥
            feather: ê²½ê³„ ë¸”ëŸ¬
            include_hair: ë¨¸ë¦¬ì¹´ë½ í¬í•¨ ì—¬ë¶€ (BiSeNet ì‚¬ìš© ì‹œ)
            include_neck: ëª© í¬í•¨ ì—¬ë¶€ (BiSeNet ì‚¬ìš© ì‹œ)

        Returns:
            ë§ˆìŠ¤í¬ (PIL Image) ë˜ëŠ” None
        """
        print(f"ì–¼êµ´ ê°ì§€ ì¤‘: {os.path.basename(image_path)}")

        # BiSeNetìœ¼ë¡œ ë¨¸ë¦¬ì¹´ë½ í¬í•¨ ë§ˆìŠ¤í¬ ìƒì„± ì‹œë„
        if self.use_bisenet and self.face_parser is not None:
            try:
                image_pil = Image.open(image_path).convert("RGB")
                bisenet_mask = self.face_parser.get_face_hair_mask(
                    image_pil,
                    target_size=image_pil.size,
                    include_hair=include_hair,
                    include_neck=include_neck,
                    blur_radius=feather,
                    expand_ratio=1.0 + expand_ratio  # 1.3 for 0.3 expand
                )
                if bisenet_mask is not None:
                    parts = []
                    if include_hair:
                        parts.append("ë¨¸ë¦¬ì¹´ë½")
                    if include_neck:
                        parts.append("ëª©")
                    parts_str = "+".join(parts) if parts else "ì–¼êµ´ë§Œ"
                    print(f"BiSeNet ë§ˆìŠ¤í¬ ìƒì„± ì™„ë£Œ ({parts_str})")
                    return bisenet_mask
                else:
                    print("BiSeNet ë§ˆìŠ¤í¬ ì‹¤íŒ¨, íƒ€ì› ë§ˆìŠ¤í¬ë¡œ ì „í™˜")
            except Exception as e:
                print(f"BiSeNet ì˜¤ë¥˜: {e}, íƒ€ì› ë§ˆìŠ¤í¬ë¡œ ì „í™˜")

        # Fallback: íƒ€ì›í˜• ë§ˆìŠ¤í¬
        face_bbox = self.detect_face(image_path)

        if face_bbox is None:
            print("ì–¼êµ´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
            return None

        x, y, box_w, box_h = face_bbox
        print(f"ì–¼êµ´ ë°œê²¬: x={x}, y={y}, w={box_w}, h={box_h}")

        # ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°
        image = cv2.imread(image_path)
        h, w = image.shape[:2]

        # ë§ˆìŠ¤í¬ ìƒì„±
        mask = np.zeros((h, w), dtype=np.uint8)

        # ì˜ì—­ í™•ì¥ (ë¨¸ë¦¬ì¹´ë½ í¬í•¨ ì‹œ ë” í¬ê²Œ)
        hair_expand_multiplier = 1.5 if include_hair else 1.0
        expand_w = int(box_w * expand_ratio * hair_expand_multiplier)
        expand_h = int(box_h * expand_ratio * hair_expand_multiplier)

        # ë¨¸ë¦¬ì¹´ë½ í¬í•¨ ì‹œ ìœ„ìª½ìœ¼ë¡œ ë” í™•ì¥
        expand_h_up = int(expand_h * 1.8) if include_hair else expand_h

        x1 = max(0, x - expand_w)
        y1 = max(0, y - expand_h_up)  # ìœ„ìª½ í™•ì¥ (ë¨¸ë¦¬ì¹´ë½)
        x2 = min(w, x + box_w + expand_w)
        y2 = min(h, y + box_h + expand_h)

        # íƒ€ì›í˜• ë§ˆìŠ¤í¬
        center_x = (x1 + x2) // 2
        center_y = (y1 + y2) // 2
        axes_x = (x2 - x1) // 2
        axes_y = (y2 - y1) // 2

        cv2.ellipse(mask, (center_x, center_y), (axes_x, axes_y),
                   0, 0, 360, 255, -1)

        # ê²½ê³„ ë¸”ëŸ¬
        if feather > 0:
            mask = cv2.GaussianBlur(mask, (feather*2+1, feather*2+1), 0)

        return Image.fromarray(mask)

    def composite_face_auto(
        self,
        background_path,
        source_face_path,
        prompt="professional portrait, natural expression",
        output_path="output.png",
        face_strength=0.85,
        denoising_strength=0.92,
        num_inference_steps=50,
        guidance_scale=7.5,
        mask_expand=0.3,
        mask_blur=15,
        seed=None,
        save_mask=False,
        use_source_size=True,
        include_hair=True,
        include_neck=False,
        auto_detect_gender=True,
        face_blend_weight=0.8,
        hair_blend_weight=0.2,
        mask_padding=0,
        run_folder=None,
        stop_at=1.0,
        shortcut_scale=1.0,
        save_preview=False,
        use_pre_paste=None,
        pre_paste_denoising=0.65,
        use_face_swap=None,
        use_face_enhance=None,
        face_enhance_strength=0.8,
        use_swap_refinement=None,
        swap_refinement_strength=0.3
    ):
        """
        ìë™ ì–¼êµ´ í•©ì„± (ë¨¸ë¦¬ì¹´ë½/ëª© í¬í•¨)

        Args:
            background_path: ë ˆí¼ëŸ°ìŠ¤ ë°°ê²½ (ì–¼êµ´ì´ ìˆëŠ” ì¦ëª…ì‚¬ì§„)
            source_face_path: í•©ì„±í•  ì–¼êµ´ ì´ë¯¸ì§€
            prompt: í”„ë¡¬í”„íŠ¸
            output_path: ì¶œë ¥ ê²½ë¡œ
            face_strength: ì–¼êµ´ ë°˜ì˜ ê°•ë„
            denoising_strength: ìƒì„± ê°•ë„
            num_inference_steps: ìƒì„± ìŠ¤í…
            guidance_scale: ê°€ì´ë˜ìŠ¤
            mask_expand: ë§ˆìŠ¤í¬ í™•ì¥ ë¹„ìœ¨
            mask_blur: ë§ˆìŠ¤í¬ ë¸”ëŸ¬
            seed: ëœë¤ ì‹œë“œ
            save_mask: ë§ˆìŠ¤í¬ ì €ì¥ ì—¬ë¶€
            use_source_size: ì›ë³¸ ì–¼êµ´ ì´ë¯¸ì§€ í¬ê¸° ì‚¬ìš© (True=ì›ë³¸ í¬ê¸° ìœ ì§€)
            include_hair: ë¨¸ë¦¬ì¹´ë½ í¬í•¨ ë§ˆìŠ¤í‚¹ (BiSeNet ì‚¬ìš©)
            include_neck: ëª© í¬í•¨ ë§ˆìŠ¤í‚¹ (BiSeNet ì‚¬ìš©)
            auto_detect_gender: ë¨¸ë¦¬ì¹´ë½ìœ¼ë¡œ ì„±ë³„ íŒíŠ¸ ìë™ ê°ì§€
            face_blend_weight: CLIP Blending ì‹œ ì–¼êµ´ ê°€ì¤‘ì¹˜ (ê¸°ë³¸: 0.6)
            hair_blend_weight: CLIP Blending ì‹œ ë¨¸ë¦¬ì¹´ë½ ê°€ì¤‘ì¹˜ (ê¸°ë³¸: 0.4)
            mask_padding: ë§ˆìŠ¤í¬ íŒ¨ë”© í”½ì…€ (ì–‘ìˆ˜=í™•ì¥, ìŒìˆ˜=ì¶•ì†Œ)
            stop_at: FaceID ì ìš© ì¤‘ë‹¨ ì‹œì  (0.0~1.0, ê¸°ë³¸: 1.0=ëê¹Œì§€)
            use_pre_paste: Pre-paste ì‚¬ìš© ì—¬ë¶€ (Noneì´ë©´ í´ë˜ìŠ¤ ì„¤ì • ì‚¬ìš©)
            pre_paste_denoising: Pre-paste ì‹œ denoising strength (ê¸°ë³¸: 0.65)
            use_face_swap: Face Swap ì‚¬ìš© ì—¬ë¶€ (Noneì´ë©´ í´ë˜ìŠ¤ ì„¤ì • ì‚¬ìš©)
            use_face_enhance: Face Enhance ì‚¬ìš© ì—¬ë¶€ (Noneì´ë©´ í´ë˜ìŠ¤ ì„¤ì • ì‚¬ìš©)
            face_enhance_strength: Face Enhance ê°•ë„ (0.0~1.0, ê¸°ë³¸: 0.8)
            use_swap_refinement: Face Swap Refinement ì‚¬ìš© ì—¬ë¶€ (Noneì´ë©´ í´ë˜ìŠ¤ ì„¤ì • ì‚¬ìš©)
            swap_refinement_strength: Swap Refinement ê°•ë„ (0.1~0.5, ê¸°ë³¸: 0.3)

        Returns:
            í•©ì„±ëœ ì´ë¯¸ì§€ (PIL Image)
        """
        if not self.has_ip_adapter:
            print("IP-Adapterê°€ í•„ìš”í•©ë‹ˆë‹¤!")
            return None

        # Pre-paste / Face Swap / Face Enhance / Swap Refinement í”Œë˜ê·¸ í•´ê²° (Noneì´ë©´ í´ë˜ìŠ¤ ì„¤ì • ì‚¬ìš©)
        apply_pre_paste = use_pre_paste if use_pre_paste is not None else self.use_pre_paste
        apply_face_swap = use_face_swap if use_face_swap is not None else self.use_face_swap
        apply_face_enhance = use_face_enhance if use_face_enhance is not None else self.use_face_enhance
        apply_swap_refinement = use_swap_refinement if use_swap_refinement is not None else self.use_swap_refinement

        # Pre-paste ì‹œ denoising strength ìë™ ì¡°ì •
        actual_denoising = denoising_strength
        if apply_pre_paste:
            actual_denoising = pre_paste_denoising
            print(f"\nğŸ“‹ Pre-paste ëª¨ë“œ: denoising {denoising_strength} -> {actual_denoising}")

        # Preview ì„¤ì •
        self.save_preview = save_preview
        if save_preview:
            # Preview íŒŒì¼ ê²½ë¡œ ì„¤ì •
            base_path = output_path.replace('.png', '')
            self.preview_path = f"{base_path}_preview.png"

        print("=" * 70)
        mode_str = "ìë™ ì–¼êµ´ í•©ì„± (ë¨¸ë¦¬ì¹´ë½ í¬í•¨)" if include_hair else "ìë™ ì–¼êµ´ í•©ì„±"
        if apply_pre_paste:
            mode_str += " + Pre-paste"
        if apply_face_swap:
            mode_str += " + Face Swap"
        if apply_swap_refinement:
            mode_str += " + Swap Refinement"
        if apply_face_enhance:
            mode_str += " + Face Enhance"
        print(mode_str)
        print("=" * 70)

        # 1. ì›ë³¸ ì–¼êµ´ ì´ë¯¸ì§€ ë¡œë“œ (í¬ê¸° ê²°ì •ìš©)
        print("\nì›ë³¸ ì–¼êµ´ ì´ë¯¸ì§€ ë¡œë”©...")
        source_face = Image.open(source_face_path).convert("RGB")
        src_w, src_h = source_face.size
        print(f"   ì›ë³¸ ì–¼êµ´ í¬ê¸°: {src_w}x{src_h}")

        # 1.5. ì„±ë³„ íŒíŠ¸ ìë™ ê°ì§€ (ë¨¸ë¦¬ì¹´ë½ ê¸°ë°˜)
        gender_hint = ""
        if auto_detect_gender and self.use_bisenet and self.face_parser is not None:
            try:
                gender_hint = self.face_parser.detect_gender_from_hair(source_face)
                print(f"   ë¨¸ë¦¬ì¹´ë½ ë¶„ì„: {gender_hint}")

                # í”„ë¡¬í”„íŠ¸ì— ì„±ë³„ íŒíŠ¸ ì¶”ê°€
                if "female" in gender_hint.lower():
                    gender_hint = "woman, "
                elif "male" in gender_hint.lower():
                    gender_hint = "man, "
                else:
                    gender_hint = ""
            except Exception as e:
                print(f"   ì„±ë³„ ê°ì§€ ì‹¤íŒ¨: {e}")
                gender_hint = ""

        # 2. ë°°ê²½ ì´ë¯¸ì§€ ë¡œë“œ ë° í¬ê¸° ì¡°ì •
        print("\nğŸ“‚ ë°°ê²½ ì´ë¯¸ì§€ ë¡œë”©...")
        background_img = Image.open(background_path).convert("RGB")
        bg_w, bg_h = background_img.size

        # ë°°ê²½ ì´ë¯¸ì§€ ë¹„ìœ¨ ê¸°ì¤€ + SDXL ìµœì í™” (1024/8ë°°ìˆ˜)
        max_side = 1024
        aspect_ratio = bg_w / bg_h

        if bg_w > bg_h:
            target_w = max_side
            target_h = int(max_side / aspect_ratio)
        else:
            target_h = max_side
            target_w = int(max_side * aspect_ratio)

        # 8ì˜ ë°°ìˆ˜ë¡œ ì¡°ì • (SDXL í•„ìˆ˜ ì¡°ê±´)
        target_w = (target_w // 8) * 8
        target_h = (target_h // 8) * 8
        target_size = (target_w, target_h)

        print(f"   âœ¨ ë°°ê²½ ë¹„ìœ¨ ìœ ì§€ ìŠ¤ì¼€ì¼ì—…: ({bg_w}, {bg_h}) -> {target_size}")

        # ë°°ê²½ ì´ë¯¸ì§€ë¥¼ ë¹„ìœ¨ ìœ ì§€í•˜ë©° ë¦¬ì‚¬ì´ì¦ˆ (crop ì—†ì´)
        background_img = background_img.resize(target_size, Image.Resampling.LANCZOS)

        # 2.5. Pre-paste ì ìš© (ì†ŒìŠ¤ ì–¼êµ´ì„ ë°°ê²½ì— ë¯¸ë¦¬ ë¶™ì—¬ë„£ê¸°)
        if apply_pre_paste:
            background_img = self._pre_paste_face(
                background_img,
                source_face,
                target_bbox=None,
                blend_mode="seamless",
                run_folder=run_folder if save_mask else None
            )
            # Pre-paste ìµœì¢… ê²°ê³¼ ì €ì¥ (ë””ë²„ê¹…ìš©)
            if save_mask and run_folder:
                pre_paste_path = os.path.join(run_folder, "2.6_prepaste_final_result.png")
                background_img.save(pre_paste_path)
                print(f"   Pre-paste ìµœì¢… ê²°ê³¼ ì €ì¥: {os.path.basename(pre_paste_path)}")

        # 3. ë°°ê²½ì—ì„œ ì–¼êµ´ ìë™ ê°ì§€ + ë§ˆìŠ¤í¬ ìƒì„±
        print("\në°°ê²½ì—ì„œ ì–¼êµ´ ë§ˆìŠ¤í¬ ìë™ ìƒì„±...")
        if include_hair:
            print("   (ë¨¸ë¦¬ì¹´ë½ ì˜ì—­ í¬í•¨)")

        # ì„ì‹œë¡œ ë°°ê²½ì„ ë¦¬ì‚¬ì´ì¦ˆëœ í¬ê¸°ë¡œ ì €ì¥ (ë§ˆìŠ¤í¬ ìƒì„±ìš©)
        import tempfile
        with tempfile.NamedTemporaryFile(suffix='.jpg', delete=False) as tmp:
            background_img.save(tmp.name)
            temp_bg_path = tmp.name

        face_mask = self.create_face_mask(
            temp_bg_path,
            expand_ratio=mask_expand,
            feather=mask_blur,
            include_hair=include_hair,
            include_neck=include_neck
        )

        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        os.unlink(temp_bg_path)

        if face_mask is None:
            print("ë°°ê²½ì—ì„œ ì–¼êµ´ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤!")
            print("TIP: ì •ë©´ ì–¼êµ´ì´ ëª…í™•í•œ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”")
            return None

        # ë§ˆìŠ¤í¬ íŒ¨ë”© ì ìš© (ì–‘ìˆ˜=í™•ì¥, ìŒìˆ˜=ì¶•ì†Œ)
        if mask_padding != 0:
            mask_array = np.array(face_mask.convert('L'))
            kernel_size = abs(mask_padding)
            kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (kernel_size * 2 + 1, kernel_size * 2 + 1))

            if mask_padding > 0:
                # í™•ì¥ (dilate)
                mask_array = cv2.dilate(mask_array, kernel, iterations=1)
                print(f"   ë§ˆìŠ¤í¬ í™•ì¥: +{mask_padding}px")
            else:
                # ì¶•ì†Œ (erode)
                mask_array = cv2.erode(mask_array, kernel, iterations=1)
                print(f"   ë§ˆìŠ¤í¬ ì¶•ì†Œ: {mask_padding}px")

            face_mask = Image.fromarray(mask_array)

        # ë§ˆìŠ¤í¬ ë° ì¤‘ê°„ ê²°ê³¼ ì €ì¥ (ì˜µì…˜)
        if save_mask:
            # run_folderê°€ ìˆìœ¼ë©´ í•´ë‹¹ í´ë”ì— ì €ì¥, ì—†ìœ¼ë©´ ê¸°ì¡´ ë°©ì‹
            if run_folder:
                save_dir = run_folder
            else:
                save_dir = os.path.dirname(output_path) or '.'

            mask_array = np.array(face_mask.convert('L'))
            bg_array = np.array(background_img)
            mask_bool = mask_array > 127

            # 3. ë§ˆìŠ¤í¬ ì˜¤ë²„ë ˆì´ (ë°°ê²½ì— ë§ˆìŠ¤í¬ ì˜ì—­ ë¹¨ê°„ìƒ‰ í‘œì‹œ) - ê°€ì¥ ì¤‘ìš”!
            overlay_path = os.path.join(save_dir, "3_mask_overlay.png")
            overlay = bg_array.copy()
            overlay[mask_bool, 0] = np.clip(overlay[mask_bool, 0] * 0.5 + 127, 0, 255)  # Red
            overlay[mask_bool, 1] = (overlay[mask_bool, 1] * 0.5).astype(np.uint8)
            overlay[mask_bool, 2] = (overlay[mask_bool, 2] * 0.5).astype(np.uint8)
            Image.fromarray(overlay).save(overlay_path)
            print(f"   ë§ˆìŠ¤í¬ ì˜¤ë²„ë ˆì´ ì €ì¥: {os.path.basename(overlay_path)}")

            # 4. Inpainting ì…ë ¥ ì‹œê°í™” (ë§ˆìŠ¤í¬ ì˜ì—­ ê²€ì •ìƒ‰ìœ¼ë¡œ í‘œì‹œ)
            inpaint_input_path = os.path.join(save_dir, "4_inpaint_input.png")
            inpaint_vis = bg_array.copy()
            inpaint_vis[mask_bool] = 0  # ë§ˆìŠ¤í¬ ì˜ì—­ ê²€ì •ìƒ‰
            Image.fromarray(inpaint_vis).save(inpaint_input_path)
            print(f"   Inpainting ì…ë ¥ ì €ì¥: {os.path.basename(inpaint_input_path)}")

        # 4. ë¨¸ë¦¬ì¹´ë½ ì˜ì—­ ì¶”ì¶œ (IP-Adapter ì…ë ¥ìš©)
        hair_region = None
        if include_hair and self.use_bisenet and self.face_parser is not None:
            try:
                hair_region = self.face_parser.extract_hair_region(source_face)
                if hair_region is not None:
                    print(f"   ë¨¸ë¦¬ì¹´ë½ ì˜ì—­ ì¶”ì¶œ ì™„ë£Œ (IP-Adapter ì…ë ¥ìš©)")
            except Exception as e:
                print(f"   ë¨¸ë¦¬ì¹´ë½ ì¶”ì¶œ ì‹¤íŒ¨: {e}")

        # 5. ìµœì¢… í¬ê¸° í™•ì¸
        print(f"\nìµœì¢… ì¶œë ¥ í¬ê¸°: {target_size}")
        # 6. í”„ë¡¬í”„íŠ¸ ì¤€ë¹„ (ì„±ë³„ íŒíŠ¸ í¬í•¨)
        full_prompt = (
            f"professional ID photo, passport style photograph, "
            f"{gender_hint}"
            f"neutral background, studio lighting, front-facing portrait, "
            f"sharp focus, high quality, even lighting, formal photograph, "
            f"{prompt}"
        )

        print(f"\ní”„ë¡¬í”„íŠ¸: {gender_hint}{prompt}")

        negative_prompt = (
            "bad quality, blurry, distorted, deformed, ugly, bad anatomy, "
            "wrong face, disfigured, mutation, low resolution, pixelated, "
            "artifacts, watermark, multiple faces, cropped face, "
            "side view, profile, looking away, tilted head"
        )

        # 6. Generator ì„¤ì •
        generator = None
        if seed is not None:
            generator = torch.Generator(self.device).manual_seed(seed)
            print(f"ì‹œë“œ: {seed}")

        print(f"\nâš™ï¸ ì„¤ì •:")
        print(f"   ì–¼êµ´ ë°˜ì˜ ê°•ë„: {face_strength}")
        print(f"   ìƒì„± ê°•ë„: {denoising_strength}")
        print(f"   ìƒì„± ìŠ¤í…: {num_inference_steps}")

        # 8. IP-Adapter ì„¤ì • ë° ì´ë¯¸ì§€/ì„ë² ë”© ì¤€ë¹„
        print(f"   IP-Adapter ëª¨ë“œ: {self.ip_adapter_mode.upper()}")

        ip_adapter_kwargs = {}

        if self.no_ip_adapter:
            # Simple Inpainting ëª¨ë“œ: IP-Adapter ì—†ì´ ìˆœìˆ˜ ì¸í˜ì¸íŒ…ë§Œ
            print("   Simple Inpainting: IP-Adapter ì‚¬ìš© ì•ˆí•¨ (ìˆœìˆ˜ ì¸í˜ì¸íŒ…)")
            # ip_adapter_kwargsëŠ” ë¹„ì›Œë‘ 

        elif self.use_clip_blend:
            # CLIP Blending ëª¨ë“œ: í”½ì…€ ë ˆë²¨ ì–¼êµ´/ë¨¸ë¦¬ì¹´ë½ ë¸”ë Œë”© í›„ CLIP ì¸ì½”ë”©
            print("   CLIP Blending: í”½ì…€ ë ˆë²¨ ë¸”ë Œë”©...")
            self.pipeline.set_ip_adapter_scale(face_strength)

            if hair_region is not None:
                # ì–¼êµ´ + ë¨¸ë¦¬ì¹´ë½ í•©ì„± ì´ë¯¸ì§€ ìƒì„±
                composite_image = self._create_face_hair_composite(
                    source_face, hair_region,
                    face_weight=face_blend_weight,
                    hair_weight=hair_blend_weight
                )
                print(f"   [Face+Hair] í•©ì„± ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ")
                print(f"   ë¸”ë Œë”© ê°€ì¤‘ì¹˜: face={face_blend_weight:.0%}, hair={hair_blend_weight:.0%}")

                # í•©ì„± ì´ë¯¸ì§€ë¥¼ IP-Adapter ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©
                ip_adapter_kwargs["ip_adapter_image"] = composite_image

            else:
                # ë¨¸ë¦¬ì¹´ë½ ì˜ì—­ì´ ì—†ìœ¼ë©´ ì›ë³¸ ì–¼êµ´ ì‚¬ìš©
                print("   [Warning] ë¨¸ë¦¬ì¹´ë½ ì˜ì—­ ì—†ìŒ, ì›ë³¸ ì–¼êµ´ ì‚¬ìš©")
                ip_adapter_kwargs["ip_adapter_image"] = source_face

        elif self.use_dual_adapter and self.face_id_extractor is not None:
            # Dual IP-Adapter ëª¨ë“œ: Standard (ë¨¸ë¦¬ì¹´ë½ CLIP) + FaceID (ì–¼êµ´)
            # diffusersëŠ” ip_adapter_imageë¡œ ë¦¬ìŠ¤íŠ¸ ì „ë‹¬ ì‹œ ê° ì–´ëŒ‘í„°ì— ë¶„ë°°
            print("   Dual IP-Adapter: ì–¼êµ´ + ë¨¸ë¦¬ì¹´ë½ ì¤€ë¹„ ì¤‘...")

            # 1. InsightFace ì–¼êµ´ ì„ë² ë”© ì¶”ì¶œ
            face_embedding = self.face_id_extractor.get_embedding_for_ip_adapter(
                source_face,
                dtype=self.dtype,
                device=self.device
            )

            # 2. ë¨¸ë¦¬ì¹´ë½ ì´ë¯¸ì§€ ì¤€ë¹„ (CLIPìš©)
            hair_image_for_clip = hair_region if hair_region is not None else source_face
            if hair_region is not None:
                print("   [Hair] BiSeNet ë¨¸ë¦¬ì¹´ë½ ì˜ì—­ ì‚¬ìš©")
            else:
                print("   [Hair] ì „ì²´ ì–¼êµ´ ì´ë¯¸ì§€ ì‚¬ìš©")

            if face_embedding is not None:
                # FaceID ì„ë² ë”© í¬ë§·
                if face_embedding.dim() == 2:
                    face_embedding = face_embedding.unsqueeze(1)

                # CFG í¬ë§·: (batch, seq, dim) -> (2*batch, seq, dim)
                negative_face = torch.zeros_like(face_embedding)
                face_embedding_cfg = torch.cat([negative_face, face_embedding], dim=0)

                # Dual adapter ì…ë ¥: ë‘ ì–´ëŒ‘í„°ì— ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸ ì „ë‹¬
                # [Standard, FaceID] ìˆœì„œ
                # StandardëŠ” CLIPìœ¼ë¡œ ìë™ ì¸ì½”ë”©, FaceIDëŠ” ì´ë¯¸ì§€ì—ì„œ ì–¼êµ´ ê°ì§€
                ip_adapter_kwargs["ip_adapter_image"] = [hair_image_for_clip, source_face]

                print(f"   [Face] ì–¼êµ´ ì´ë¯¸ì§€ for FaceID")
                print(f"   [Hair] ë¨¸ë¦¬ì¹´ë½ ì´ë¯¸ì§€ for CLIP: {hair_image_for_clip.size}")

                # ìŠ¤ì¼€ì¼ ì„¤ì •: [Standard(hair), FaceID(face)]
                hair_scale = face_strength * 0.4  # ë¨¸ë¦¬ì¹´ë½ ìŠ¤íƒ€ì¼
                face_scale = face_strength * 0.8  # ì–¼êµ´ ì •ì²´ì„±
                self.pipeline.set_ip_adapter_scale([hair_scale, face_scale])
                print(f"   ìŠ¤ì¼€ì¼: hair={hair_scale:.2f}, face={face_scale:.2f}")

            else:
                print("   Dual: ì–¼êµ´ ê²€ì¶œ ì‹¤íŒ¨, Standard ëª¨ë“œë¡œ í´ë°±")
                ip_adapter_kwargs["ip_adapter_image"] = source_face
                self.pipeline.set_ip_adapter_scale(face_strength)

        elif self.use_faceid_plus and self.face_id_extractor is not None:
            # FaceID Plus v2: InsightFace + CLIP ì´ë¯¸ì§€ ì„ë² ë”© (ë¨¸ë¦¬ìŠ¤íƒ€ì¼ í¬í•¨)
            self.pipeline.set_ip_adapter_scale(face_strength)
            print("   FaceID Plus v2: ì–¼êµ´+ë¨¸ë¦¬ìŠ¤íƒ€ì¼ ì„ë² ë”© ì¶”ì¶œ ì¤‘...")

            # 1. InsightFace ì–¼êµ´ ì„ë² ë”© ì¶”ì¶œ
            face_embedding = self.face_id_extractor.get_embedding_for_ip_adapter(
                source_face,
                dtype=self.dtype,
                device=self.device
            )

            if face_embedding is not None:
                # 2. CLIP ì´ë¯¸ì§€ ì„ë² ë”© ì¶”ì¶œ (ë¨¸ë¦¬ìŠ¤íƒ€ì¼ í¬í•¨)
                from transformers import CLIPImageProcessor
                clip_processor = CLIPImageProcessor.from_pretrained("laion/CLIP-ViT-H-14-laion2B-s32B-b79K")
                clip_input = clip_processor(images=source_face, return_tensors="pt").pixel_values.to(self.device, dtype=self.dtype)

                # CLIP hidden states ì¶”ì¶œ (last_hidden_state ì‚¬ìš©)
                clip_output = self.clip_image_encoder(clip_input, output_hidden_states=True)
                # (1, 257, 1280) - 257 = 1 CLS + 256 patches
                clip_embeds = clip_output.hidden_states[-2]  # ë§ˆì§€ë§‰ì—ì„œ ë‘ ë²ˆì§¸ ë ˆì´ì–´

                # Shape ë§ì¶”ê¸°
                if face_embedding.dim() == 2:
                    face_embedding = face_embedding.unsqueeze(1)  # (1, 1, 512)

                # CFGìš© negative ì„ë² ë”©
                neg_face = torch.zeros_like(face_embedding)
                neg_clip = torch.zeros_like(clip_embeds)

                face_embedding_cfg = torch.cat([neg_face, face_embedding], dim=0)  # (2, 1, 512)
                clip_embeds_cfg = torch.cat([neg_clip, clip_embeds], dim=0)  # (2, 257, 1280)

                # Plus v2: CLIP ì„ë² ë”©ì€ 4D í•„ìš”: (batch, num_images, seq, hidden)
                clip_embeds_cfg = clip_embeds_cfg.unsqueeze(1)  # (2, 1, 257, 1280)

                # Plus v2: CLIP ì„ë² ë”©ì„ projection layerì— ì§ì ‘ ì„¤ì •
                self.pipeline.unet.encoder_hid_proj.image_projection_layers[0].clip_embeds = clip_embeds_cfg
                self.pipeline.unet.encoder_hid_proj.image_projection_layers[0].shortcut_scale = shortcut_scale
                print(f"      shortcut_scale: {shortcut_scale:.2f} (ë¨¸ë¦¬ìŠ¤íƒ€ì¼ ë°˜ì˜ ë¹„ìœ¨)")

                # ì–¼êµ´ ì„ë² ë”©ë§Œ ì „ë‹¬
                ip_adapter_kwargs["ip_adapter_image_embeds"] = [face_embedding_cfg]
                print(f"   FaceID Plus v2: ì„ë² ë”© ì¶”ì¶œ ì™„ë£Œ")
                print(f"      ì–¼êµ´ ì„ë² ë”©: {face_embedding_cfg.shape}")
                print(f"      CLIP ì„ë² ë”©: {clip_embeds_cfg.shape} (ë¨¸ë¦¬ìŠ¤íƒ€ì¼ í¬í•¨)")

            else:
                print("   FaceID Plus v2: ì–¼êµ´ ê²€ì¶œ ì‹¤íŒ¨, CLIP ì„ë² ë”©ìœ¼ë¡œ í´ë°±")
                # image_encoderê°€ Noneì´ë¯€ë¡œ ip_adapter_image ëŒ€ì‹  CLIP ì„ë² ë”© ì§ì ‘ ìƒì„±
                if self.clip_image_encoder is not None:
                    from transformers import CLIPImageProcessor
                    clip_processor = CLIPImageProcessor.from_pretrained("laion/CLIP-ViT-H-14-laion2B-s32B-b79K")
                    clip_input = clip_processor(images=source_face, return_tensors="pt").pixel_values.to(self.device, dtype=self.dtype)
                    clip_output = self.clip_image_encoder(clip_input, output_hidden_states=True)
                    clip_embeds = clip_output.hidden_states[-2]

                    # Zero face embedding (ì–¼êµ´ ê²€ì¶œ ì‹¤íŒ¨)
                    zero_face = torch.zeros(1, 1, 512, device=self.device, dtype=self.dtype)
                    neg_face = torch.zeros_like(zero_face)
                    neg_clip = torch.zeros_like(clip_embeds)

                    face_embedding_cfg = torch.cat([neg_face, zero_face], dim=0)
                    clip_embeds_cfg = torch.cat([neg_clip, clip_embeds], dim=0)
                    clip_embeds_cfg = clip_embeds_cfg.unsqueeze(1)

                    self.pipeline.unet.encoder_hid_proj.image_projection_layers[0].clip_embeds = clip_embeds_cfg
                    self.pipeline.unet.encoder_hid_proj.image_projection_layers[0].shortcut_scale = 1.0

                    ip_adapter_kwargs["ip_adapter_image_embeds"] = [face_embedding_cfg]
                else:
                    print("   [Warning] CLIP ì¸ì½”ë”ë„ ì—†ìŒ, IP-Adapter ì—†ì´ ì§„í–‰")

        elif self.use_faceid and self.face_id_extractor is not None:
            # FaceID (non-Plus): InsightFace 512-dim ì„ë² ë”© ì‚¬ìš©
            self.pipeline.set_ip_adapter_scale(face_strength)
            print("   FaceID: InsightFace ì„ë² ë”© ì¶”ì¶œ ì¤‘...")
            face_embedding = self.face_id_extractor.get_embedding_for_ip_adapter(
                source_face,
                dtype=self.dtype,
                device=self.device
            )

            if face_embedding is not None:
                # Shape ë³€í™˜: (1, 512) -> (1, 1, 512) for IP-Adapter
                if face_embedding.dim() == 2:
                    face_embedding = face_embedding.unsqueeze(1)  # (batch, 1, 512)

                # Classifier-free guidance: negative + positive embeddings
                # Shape: (1, 1, 512) -> (2, 1, 512)
                negative_embedding = torch.zeros_like(face_embedding)
                face_embedding_cfg = torch.cat([negative_embedding, face_embedding], dim=0)

                ip_adapter_kwargs["ip_adapter_image_embeds"] = [face_embedding_cfg]
                print(f"   FaceID: InsightFace ì„ë² ë”© ì¶”ì¶œ ì™„ë£Œ (shape: {face_embedding_cfg.shape})")

            else:
                print("   FaceID: ì–¼êµ´ ê²€ì¶œ ì‹¤íŒ¨, Standard ëª¨ë“œë¡œ í´ë°±")
                if self.pipeline.image_encoder is not None:
                    ip_adapter_kwargs["ip_adapter_image"] = source_face
                else:
                    print("   [Warning] image_encoder ì—†ìŒ, IP-Adapter ì—†ì´ ì§„í–‰")

        else:
            # Standard ëª¨ë“œ: ì´ë¯¸ì§€ ì§ì ‘ ì „ë‹¬ (CLIP ì¸ì½”ë”©)
            # ë¨¸ë¦¬ì¹´ë½ ì˜ì—­ì´ ìˆìœ¼ë©´ ë¸”ë Œë”©
            if hair_region is not None:
                face_array = np.array(source_face).astype(np.float32)
                hair_array = np.array(hair_region).astype(np.float32)

                # ë¨¸ë¦¬ì¹´ë½ ì˜ì—­ ë§ˆìŠ¤í¬ (íšŒìƒ‰ì´ ì•„ë‹Œ ë¶€ë¶„)
                hair_mask_arr = np.any(np.abs(hair_array - 128) > 10, axis=2).astype(np.float32)
                hair_mask_arr = hair_mask_arr[:, :, np.newaxis]

                # ì›ë³¸ ì–¼êµ´ + ë¨¸ë¦¬ì¹´ë½ ê°•ì¡° ë¸”ë Œë”©
                blended = face_array * (1 - hair_mask_arr * 0.3) + hair_array * (hair_mask_arr * 0.3)
                blended = np.clip(blended, 0, 255).astype(np.uint8)
                ip_adapter_input = Image.fromarray(blended)

                print("   Standard: ì–¼êµ´+ë¨¸ë¦¬ì¹´ë½ ë¸”ë Œë”© ì´ë¯¸ì§€ ì‚¬ìš©")

            else:
                ip_adapter_input = source_face
                print("   Standard: ì›ë³¸ ì–¼êµ´ ì´ë¯¸ì§€ ì‚¬ìš©")

            if self.pipeline.image_encoder is not None:
                ip_adapter_kwargs["ip_adapter_image"] = ip_adapter_input
            else:
                print("   [Warning] image_encoder ì—†ìŒ, IP-Adapter ì—†ì´ ì§„í–‰")

        print("\ní•©ì„± ì‹œì‘...")
        print("   ë°°ê²½ ìœ ì§€ + ìƒˆ ì–¼êµ´ í•©ì„± ì¤‘...")

        # 9. Inpainting ìˆ˜í–‰ (ê³ í•´ìƒë„ ìƒì„± í›„ ì›ë³¸ í¬ê¸°ë¡œ ì¶•ì†Œ)
        orig_width, orig_height = background_img.size

        # SDXL ìµœì  í•´ìƒë„ë¡œ ìŠ¤ì¼€ì¼ì—… (ìµœì†Œ 1024px, ë¹„ìœ¨ ìœ ì§€)
        min_size = 1024
        scale = max(min_size / orig_width, min_size / orig_height, 1.0)
        gen_width = int(orig_width * scale)
        gen_height = int(orig_height * scale)

        # 8ì˜ ë°°ìˆ˜ë¡œ ì¡°ì •
        gen_width = (gen_width // 8) * 8
        gen_height = (gen_height // 8) * 8

        # ìƒì„±ìš© ì´ë¯¸ì§€/ë§ˆìŠ¤í¬ ë¦¬ì‚¬ì´ì¦ˆ
        if scale > 1.0:
            bg_for_gen = background_img.resize((gen_width, gen_height), Image.Resampling.LANCZOS)
            mask_for_gen = face_mask.resize((gen_width, gen_height), Image.Resampling.LANCZOS)
            print(f"   ê³ í•´ìƒë„ ìƒì„±: {orig_width}x{orig_height} -> {gen_width}x{gen_height}")
        else:
            bg_for_gen = background_img
            mask_for_gen = face_mask

        print(f"ğŸ¨ ìƒì„± ì‹œì‘... (ì´ {num_inference_steps} ìŠ¤í…, Stop-at: {stop_at*100:.0f}%)")

        # íƒ€ì´ë° ì œì–´ìš© ì½œë°± í•¨ìˆ˜ ì •ì˜
        def step_callback(pipe, step_index, _timestep, callback_kwargs):
            # 1. í˜„ì¬ ìŠ¤í… ìˆ˜ ê³„ì‚° (í˜¸í™˜ì„± ì²˜ë¦¬)
            try:
                cur_step = step_index.item() if hasattr(step_index, "item") else step_index
            except:
                cur_step = step_index

            # 2. ì§„í–‰ë¥  ê³„ì‚°
            progress = cur_step / num_inference_steps

            # 3. Stop-At ë¡œì§ ì ìš© & ë¡œê·¸ ì¶œë ¥ (no_ip_adapter ëª¨ë“œë©´ ê±´ë„ˆë›°ê¸°)
            if self.no_ip_adapter:
                status_msg = "Simple Inpainting (IP-Adapter ì—†ìŒ)"
            elif progress > stop_at:
                # ì§€ì •ëœ êµ¬ê°„ì„ ë„˜ì—ˆì„ ë•Œ -> ì–¼êµ´ ë°˜ì˜ ë„ê¸°
                pipe.set_ip_adapter_scale(0.0)
                status_msg = f"ğŸ›‘ OFF (Scale: 0.0)"
            else:
                # êµ¬ê°„ ì•ˆì¼ ë•Œ -> ì–¼êµ´ ë°˜ì˜ ì¼œê¸°
                pipe.set_ip_adapter_scale(face_strength)
                status_msg = f"âœ… ON  (Scale: {face_strength})"

            # ë§¤ ìŠ¤í…ë§ˆë‹¤ ë¡œê·¸ ì¶œë ¥
            print(f"   [Step {cur_step:02d}/{num_inference_steps}] ì§„í–‰ë¥  {progress*100:.0f}% -> {status_msg}", flush=True)

            # 4. Preview ì´ë¯¸ì§€ ìƒì„± (5 ìŠ¤í…ë§ˆë‹¤)
            if hasattr(self, 'save_preview') and self.save_preview and cur_step > 0 and cur_step % 5 == 0:
                try:
                    latents = callback_kwargs.get("latents")
                    if latents is not None:
                        # VAEë¡œ latents ë””ì½”ë”©
                        latents_scaled = 1 / 0.18215 * latents
                        with torch.no_grad():
                            image_tensor = pipe.vae.decode(latents_scaled).sample

                        # Tensorë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜
                        image_tensor = (image_tensor / 2 + 0.5).clamp(0, 1)
                        image_np = image_tensor.cpu().permute(0, 2, 3, 1).float().numpy()[0]
                        image_np = (image_np * 255).round().astype("uint8")
                        preview_img = Image.fromarray(image_np)

                        # Preview ì €ì¥
                        preview_path = self.preview_path.replace('.png', f'_step{cur_step:03d}.png')
                        preview_img.save(preview_path)

                        # stdoutì— preview ê²½ë¡œ ì¶œë ¥ (ë°±ì—”ë“œê°€ íŒŒì‹±í•¨)
                        print(f"PREVIEW:{preview_path}", flush=True)

                        # ì¤‘ê°„ í…ì„œ ì •ë¦¬ (ë©”ëª¨ë¦¬ ëˆ„ì  ë°©ì§€)
                        del latents_scaled, image_tensor
                        torch.cuda.empty_cache()
                except Exception as e:
                    print(f"   Preview ìƒì„± ì‹¤íŒ¨ (Step {cur_step}): {e}")

            return callback_kwargs

        result = self.pipeline(
            prompt=full_prompt,
            negative_prompt=negative_prompt,
            image=bg_for_gen,
            mask_image=mask_for_gen,
            width=gen_width,
            height=gen_height,
            num_inference_steps=num_inference_steps,
            guidance_scale=guidance_scale,
            strength=actual_denoising,
            generator=generator,
            callback_on_step_end=step_callback,
            **ip_adapter_kwargs  # ip_adapter_image ë˜ëŠ” ip_adapter_image_embeds
        )

        output_image = result.images[0]

        # ì›ë³¸ í¬ê¸°ì™€ ë‹¤ë¥´ë©´ ë³µì›
        if output_image.size != (orig_width, orig_height):
            output_image = output_image.resize((orig_width, orig_height), Image.Resampling.LANCZOS)
            print(f"   ì¶œë ¥ í¬ê¸° ë³µì›: {gen_width}x{gen_height} -> {orig_width}x{orig_height}")

        # 10. Face Swap ì ìš© (ì„ íƒì )
        if apply_face_swap:
            # Face Swap ì „ ê²°ê³¼ ì €ì¥ (ë””ë²„ê¹…ìš©) - swap ì „ì— ì €ì¥!
            if save_mask and run_folder:
                pre_swap_path = os.path.join(run_folder, "5.5_result_before_swap.png")
                output_image.save(pre_swap_path)
                print(f"   Face Swap ì „ ê²°ê³¼ ì €ì¥: {os.path.basename(pre_swap_path)}")

            output_image = self._apply_face_swap(output_image, source_face, run_folder if save_mask else None)

            # 10.2. Face Swap Refinement ì ìš© (ì„ íƒì )
            if apply_swap_refinement:
                # Swap Refinement ì „ ì €ì¥ (ë””ë²„ê¹…ìš©)
                if save_mask and run_folder:
                    pre_refine_path = os.path.join(run_folder, "5.6_result_before_refinement.png")
                    output_image.save(pre_refine_path)
                    print(f"   Swap Refinement ì „ ê²°ê³¼ ì €ì¥: {os.path.basename(pre_refine_path)}")

                output_image = self._apply_swap_refinement(
                    output_image,
                    prompt=prompt,
                    denoising_strength=swap_refinement_strength,
                    guidance_scale=guidance_scale,
                    num_steps=max(15, num_inference_steps // 3),  # ë©”ì¸ ìŠ¤í…ì˜ 1/3 ì •ë„ ì‚¬ìš©
                    seed=seed,
                    run_folder=run_folder if save_mask else None
                )

        # 10.5. Face Enhance ì ìš© (ì„ íƒì  - GFPGAN)
        if apply_face_enhance:
            # Face Enhance ì „ ê²°ê³¼ ì €ì¥ (ë””ë²„ê¹…ìš©)
            if save_mask and run_folder:
                pre_enhance_path = os.path.join(run_folder, "5.7_result_before_enhance.png")
                output_image.save(pre_enhance_path)
                print(f"   Face Enhance ì „ ê²°ê³¼ ì €ì¥: {os.path.basename(pre_enhance_path)}")

            output_image = self._apply_face_enhance(
                output_image,
                strength=face_enhance_strength,
                run_folder=run_folder if save_mask else None
            )

        # 11. ì €ì¥
        output_image.save(output_path)
        print(f"\nâœ… ì™„ë£Œ! ì €ì¥ë¨: {output_path}")
        print("=" * 70)

        # 12. GPU ë©”ëª¨ë¦¬ ì •ë¦¬ (ë©”ëª¨ë¦¬ ëˆ„ì  ë°©ì§€)
        cleanup_gpu_memory()
        print("ğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")

        return output_image


def main():
    parser = argparse.ArgumentParser(
        description='ìë™ ì–¼êµ´ í•©ì„± (ë§ˆìŠ¤í¬ ìë™ ìƒì„±)',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ğŸ“– ì‚¬ìš© ë°©ë²• (ê°„ë‹¨ ë²„ì „!):

í•„ìš”í•œ ê²ƒ:
  1. ë°°ê²½ ì´ë¯¸ì§€ (ì¢‹ì€ ë°°ê²½/ì˜·ì˜ ì¦ëª…ì‚¬ì§„)
  2. í•©ì„±í•  ì–¼êµ´ (ì›ë³¸ ì–¼êµ´ ì´ë¯¸ì§€)

  âš ï¸ ë§ˆìŠ¤í¬ëŠ” ìë™ìœ¼ë¡œ ìƒì„±ë©ë‹ˆë‹¤!

ğŸ¯ ì‚¬ìš© ì˜ˆì‹œ:

# ê¸°ë³¸ (ê°€ì¥ ê°„ë‹¨)
python id_photo_face_composite_auto.py background.jpg face.jpg

# í”„ë¡¬í”„íŠ¸ ì¶”ê°€
python id_photo_face_composite_auto.py background.jpg face.jpg \\
    --prompt "young asian woman, natural smile"

# ê³ í’ˆì§ˆ ì„¤ì •
python id_photo_face_composite_auto.py background.jpg face.jpg \\
    --prompt "professional headshot" \\
    --face-strength 0.9 \\
    --steps 75 \\
    --seed 42 \\
    --output result.png

# ë§ˆìŠ¤í¬ë„ ì €ì¥í•˜ê³  ì‹¶ì„ ë•Œ
python id_photo_face_composite_auto.py background.jpg face.jpg \\
    --save-mask \\
    --output result.png

# FaceID ëª¨ë“œ (ì •ì²´ì„± ë³´ì¡´ í–¥ìƒ)
python id_photo_face_composite_auto.py background.jpg face.jpg \\
    --use-faceid \\
    --output result.png

ğŸ’¡ íŒŒë¼ë¯¸í„° ê°€ì´ë“œ:

--face-strength (ì–¼êµ´ ë°˜ì˜ ê°•ë„):
  0.75 ~ 0.85: ìì—°ìŠ¤ëŸ½ê²Œ (ê¸°ë³¸: 0.85)
  0.85 ~ 0.95: ì›ë³¸ê³¼ ë§¤ìš° ìœ ì‚¬í•˜ê²Œ

--denoising (ìƒì„± ê°•ë„):
  0.88 ~ 0.92: ë°°ê²½ ë§ì´ ìœ ì§€ (ê¸°ë³¸: 0.92)
  0.92 ~ 0.96: ë” ë§ì´ ë³€í˜•

--mask-expand (ë§ˆìŠ¤í¬ í™•ì¥):
  0.2 ~ 0.3: ì–¼êµ´ë§Œ (ê¸°ë³¸: 0.3)
  0.3 ~ 0.5: ì–¼êµ´ + ì£¼ë³€ ì¡°ê¸ˆ

--mask-blur (ë§ˆìŠ¤í¬ ë¸”ëŸ¬):
  10 ~ 15: ìì—°ìŠ¤ëŸ¬ìš´ ê²½ê³„ (ê¸°ë³¸: 15)
  15 ~ 25: ë” ë¶€ë“œëŸ¬ìš´ ê²½ê³„

ğŸ”§ ë¬¸ì œ í•´ê²°:

ì–¼êµ´ì„ ëª» ì°¾ì„ ë•Œ:
  - ì •ë©´ ì–¼êµ´ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”
  - ì–¼êµ´ì´ í¬ê³  ëª…í™•í•œ ì´ë¯¸ì§€ ì‚¬ìš©
  - --detection opencv ì˜µì…˜ ì‹œë„

ë°°ê²½ì´ ë„ˆë¬´ ë³€í•  ë•Œ:
  --denoising 0.88 (ë‚®ì¶”ê¸°)

ì–¼êµ´ì´ ì˜ ì•ˆë‚˜ì˜¬ ë•Œ:
  --face-strength 0.95 (ë†’ì´ê¸°)
  --steps 100 (ìŠ¤í… ì¦ê°€)

ì •ì²´ì„±ì´ ì•ˆ ë§ì„ ë•Œ:
  --use-faceid (InsightFace ê¸°ë°˜ FaceID ëª¨ë“œ)
  â€» pip install insightface onnxruntime í•„ìš”

ë¨¸ë¦¬ì¹´ë½ ìŠ¤íƒ€ì¼ë„ ì „ì´í•˜ê³  ì‹¶ì„ ë•Œ:
  --use-clip-blend (ê¶Œì¥! CLIP ì„ë² ë”© ë¸”ë Œë”©)
  â€» ì–¼êµ´ + ë¨¸ë¦¬ì¹´ë½ CLIP ì„ë² ë”© ê°€ì¤‘ì¹˜ ë¸”ë Œë”©
  â€» BiSeNetìœ¼ë¡œ ë¨¸ë¦¬ì¹´ë½ ì˜ì—­ ìë™ ì¶”ì¶œ
        """
    )

    parser.add_argument('background',
                       help='ë°°ê²½ ì´ë¯¸ì§€ (ì¦ëª…ì‚¬ì§„)')
    parser.add_argument('face',
                       help='í•©ì„±í•  ì–¼êµ´ ì´ë¯¸ì§€')

    parser.add_argument('--prompt', '-p',
                       default='professional portrait, natural expression',
                       help='í”„ë¡¬í”„íŠ¸')
    parser.add_argument('--auto-prompt', action='store_true',
                       help='Gemini Visionìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ ìë™ ìƒì„± (GEMINI_API_KEY í•„ìš”)')
    parser.add_argument('--output', '-o', default='output.png',
                       help='ì¶œë ¥ íŒŒì¼')
    parser.add_argument('--face-strength', type=float, default=0.85,
                       help='ì–¼êµ´ ë°˜ì˜ ê°•ë„ (ê¸°ë³¸: 0.85)')
    parser.add_argument('--denoising', type=float, default=0.92,
                       help='ìƒì„± ê°•ë„ (ê¸°ë³¸: 0.92)')
    parser.add_argument('--steps', type=int, default=50,
                       help='ìƒì„± ìŠ¤í… (ê¸°ë³¸: 50)')
    parser.add_argument('--guidance', type=float, default=7.5,
                       help='ê°€ì´ë˜ìŠ¤ (ê¸°ë³¸: 7.5)')
    parser.add_argument('--mask-expand', type=float, default=0.3,
                       help='ë§ˆìŠ¤í¬ í™•ì¥ ë¹„ìœ¨ (ê¸°ë³¸: 0.3)')
    parser.add_argument('--mask-blur', type=int, default=15,
                       help='ë§ˆìŠ¤í¬ ë¸”ëŸ¬ (ê¸°ë³¸: 15)')
    parser.add_argument('--mask-padding', type=int, default=0,
                       help='ë§ˆìŠ¤í¬ íŒ¨ë”© í”½ì…€ (ê¸°ë³¸: 0, ì–‘ìˆ˜=í™•ì¥, ìŒìˆ˜=ì¶•ì†Œ)')
    parser.add_argument('--seed', type=int, help='ëœë¤ ì‹œë“œ')
    parser.add_argument('--save-mask', action='store_true',
                       help='ë§ˆìŠ¤í¬ íŒŒì¼ë„ ì €ì¥')
    parser.add_argument('--use-background-size', action='store_true',
                       help='ë°°ê²½ ì´ë¯¸ì§€ í¬ê¸° ì‚¬ìš© (ê¸°ë³¸: ì›ë³¸ ì–¼êµ´ í¬ê¸°)')
    parser.add_argument('--detection', choices=['mediapipe', 'opencv'],
                       default='opencv',
                       help='ì–¼êµ´ ê°ì§€ ë°©ë²• (ê¸°ë³¸: opencv)')
    parser.add_argument('--no-hair', action='store_true',
                       help='ë¨¸ë¦¬ì¹´ë½ ì œì™¸ (ì–¼êµ´ë§Œ ë§ˆìŠ¤í‚¹)')
    parser.add_argument('--include-neck', action='store_true',
                       help='ëª© í¬í•¨ ë§ˆìŠ¤í‚¹ (ë ˆí¼ëŸ°ìŠ¤ ëª©ì´ ì´ìƒí•  ë•Œ ì‚¬ìš©)')
    parser.add_argument('--no-bisenet', action='store_true',
                       help='BiSeNet ë¹„í™œì„±í™” (íƒ€ì› ë§ˆìŠ¤í¬ë§Œ ì‚¬ìš©)')
    parser.add_argument('--no-gender-detect', action='store_true',
                       help='ì„±ë³„ ìë™ ê°ì§€ ë¹„í™œì„±í™”')
    parser.add_argument('--no-ip-adapter', action='store_true',
                       help='IP-Adapter ì—†ì´ ìˆœìˆ˜ ì¸í˜ì¸íŒ…ë§Œ ìˆ˜í–‰ (Pre-pasteì™€ í•¨ê»˜ ì‚¬ìš© ê¶Œì¥)')
    parser.add_argument('--use-faceid', action='store_true',
                       help='IP-Adapter FaceID ì‚¬ìš© (InsightFace ê¸°ë°˜, ì •ì²´ì„± ë³´ì¡´ í–¥ìƒ)')
    parser.add_argument('--use-faceid-plus', action='store_true',
                       help='IP-Adapter FaceID Plus v2 ì‚¬ìš© (ì–¼êµ´ ì •ì²´ì„± + ë¨¸ë¦¬ìŠ¤íƒ€ì¼ ë™ì‹œ ë°˜ì˜)')
    parser.add_argument('--use-dual-adapter', action='store_true',
                       help='Dual IP-Adapter ì‚¬ìš© (FaceID + CLIP, ì–¼êµ´ ì •ì²´ì„± + ë¨¸ë¦¬ì¹´ë½ ìŠ¤íƒ€ì¼ ì „ì´)')
    parser.add_argument('--use-clip-blend', action='store_true',
                       help='CLIP Blending ëª¨ë“œ (ì–¼êµ´+ë¨¸ë¦¬ì¹´ë½ CLIP ì„ë² ë”© ë¸”ë Œë”©, ê¶Œì¥)')
    parser.add_argument('--face-blend-weight', type=float, default=0.6,
                       help='CLIP Blending: ì–¼êµ´ ê°€ì¤‘ì¹˜ (ê¸°ë³¸: 0.6)')
    parser.add_argument('--hair-blend-weight', type=float, default=0.4,
                       help='CLIP Blending: ë¨¸ë¦¬ì¹´ë½ ê°€ì¤‘ì¹˜ (ê¸°ë³¸: 0.4)')
    parser.add_argument('--stop-at', type=float, default=1.0,
                       help='FaceID ì ìš© ì¤‘ë‹¨ ì‹œì  (0.0~1.0, ê¸°ë³¸: 1.0=ëê¹Œì§€)')
    parser.add_argument('--shortcut-scale', type=float, default=1.0,
                       help='FaceID Plus: CLIP ì´ë¯¸ì§€(ë¨¸ë¦¬ìŠ¤íƒ€ì¼) ë°˜ì˜ ë¹„ìœ¨ (0.0~1.0, ê¸°ë³¸: 1.0)')
    parser.add_argument('--save-preview', action='store_true',
                       help='ì¤‘ê°„ ìƒì„± ê³¼ì • preview ì´ë¯¸ì§€ ì €ì¥ (5 ìŠ¤í…ë§ˆë‹¤)')
    parser.add_argument('--use-pre-paste', action='store_true',
                       help='Pre-paste ëª¨ë“œ: ì†ŒìŠ¤ ì–¼êµ´ì„ ë°°ê²½ì— ë¯¸ë¦¬ ë¶™ì—¬ë„£ê¸° (ì–¼êµ´ ìœ„ì¹˜ ì •í™•ë„ í–¥ìƒ)')
    parser.add_argument('--pre-paste-denoising', type=float, default=0.65,
                       help='Pre-paste ì‹œ denoising strength (ê¸°ë³¸: 0.65)')
    parser.add_argument('--use-face-swap', action='store_true',
                       help='Face Swap ëª¨ë“œ: ìƒì„± í›„ ì–¼êµ´ êµì²´ (ìœ ì‚¬ë„ í–¥ìƒ)')
    parser.add_argument('--face-swap-model', type=str, default='insightface',
                       choices=['insightface', 'ghost'],
                       help='Face Swap ëª¨ë¸ ì„ íƒ: insightface (ë¹ ë¦„), ghost (ê³ í™”ì§ˆ, ê¸°ë³¸: insightface)')
    parser.add_argument('--use-face-enhance', action='store_true',
                       help='Face Enhance ëª¨ë“œ: GFPGANìœ¼ë¡œ ì–¼êµ´ í™”ì§ˆ ê°œì„ ')
    parser.add_argument('--face-enhance-strength', type=float, default=0.8,
                       help='Face Enhance ê°•ë„ (0.0~1.0, ê¸°ë³¸: 0.8)')
    parser.add_argument('--use-swap-refinement', action='store_true',
                       help='Face Swap Refinement: Face Swap í›„ ì–¼êµ´ ì˜ì—­ ê²½ë¯¸í•œ ì¸í˜ì¸íŒ…ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë¸”ë Œë”©')
    parser.add_argument('--swap-refinement-strength', type=float, default=0.3,
                       help='Swap Refinement ê°•ë„ (0.1~0.5, ê¸°ë³¸: 0.3, ë‚®ì„ìˆ˜ë¡ ì›ë³¸ ìœ ì§€)')
    parser.add_argument('--show', action='store_true',
                       help='ê²°ê³¼ í‘œì‹œ')

    args = parser.parse_args()

    # ì…ë ¥ ê²½ë¡œ ì²˜ë¦¬ (inputs/ í´ë” ìë™ í™•ì¸)
    background_path = get_input_path(args.background)
    face_path = get_input_path(args.face)

    # íŒŒì¼ í™•ì¸
    for path, name in [(background_path, 'ë°°ê²½'), (face_path, 'ì–¼êµ´')]:
        if not os.path.exists(path):
            print(f"âŒ {name} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {path}")
            print(f"   (inputs/ í´ë”ë„ í™•ì¸í–ˆìŠµë‹ˆë‹¤)")
            return

    # í•©ì„± ìˆ˜í–‰
    compositor = AutoIDPhotoCompositor(
        detection_method=args.detection,
        use_bisenet=not args.no_bisenet,
        use_faceid=args.use_faceid,
        use_faceid_plus=args.use_faceid_plus,
        use_dual_adapter=args.use_dual_adapter,
        use_clip_blend=args.use_clip_blend,
        use_pre_paste=args.use_pre_paste,
        use_face_swap=args.use_face_swap,
        use_face_enhance=args.use_face_enhance,
        use_swap_refinement=args.use_swap_refinement,
        no_ip_adapter=args.no_ip_adapter,
        face_swap_model=args.face_swap_model
    )

    # no_ip_adapter ëª¨ë“œê°€ ì•„ë‹ ë•Œë§Œ IP-Adapter ì²´í¬
    if not args.no_ip_adapter and not compositor.has_ip_adapter:
        print("\nIP-Adapter ë¡œë”© ì‹¤íŒ¨")
        print("   pip install diffusers transformers accelerate")
        return

    # ì‹¤í–‰ í´ë” ìƒì„± (outputs/run_name_timestamp/)
    run_folder = setup_run_folder(args.output)
    print(f"\nì‹¤í–‰ í´ë”: {run_folder}")

    # ì…ë ¥ ì´ë¯¸ì§€ ë³µì‚¬
    bg_copy_path = os.path.join(run_folder, "1_reference.png")
    face_copy_path = os.path.join(run_folder, "2_face.png")
    shutil.copy2(background_path, bg_copy_path)
    shutil.copy2(face_path, face_copy_path)
    print(f"   ì…ë ¥ ì´ë¯¸ì§€ ë³µì‚¬ ì™„ë£Œ")

    # ì‹œë“œ ì²˜ë¦¬ (ë¯¸ì§€ì •ì‹œ ëœë¤ ìƒì„±)
    actual_seed = args.seed if args.seed is not None else random.randint(0, 2**32 - 1)

    # í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬ (auto-prompt ë˜ëŠ” ìˆ˜ë™)
    if args.auto_prompt and HAS_PROMPT_GENERATOR:
        print("\nğŸ¤– Gemini Visionìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ ìƒì„± ì¤‘...")
        generated_prompt = generate_prompt_from_face_image(face_path)
        final_prompt = generated_prompt
        print(f"   ìƒì„±ëœ í”„ë¡¬í”„íŠ¸: {final_prompt}")
        print(f"GENERATED_PROMPT:{final_prompt}", flush=True)
    elif args.auto_prompt and not HAS_PROMPT_GENERATOR:
        print("\n   prompt_generator.pyë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        final_prompt = args.prompt
    else:
        final_prompt = args.prompt

    # ì‹¤í–‰ ëª…ë ¹ì–´ ê¸°ë¡
    command = ' '.join(sys.argv)

    # ì¶œë ¥ ê²½ë¡œ (í´ë” ë‚´ result.png)
    internal_output_path = os.path.join(run_folder, "5_result.png")

    result = compositor.composite_face_auto(
        background_path=background_path,
        source_face_path=face_path,
        prompt=final_prompt,
        output_path=internal_output_path,
        face_strength=args.face_strength,
        denoising_strength=args.denoising,
        num_inference_steps=args.steps,
        guidance_scale=args.guidance,
        mask_expand=args.mask_expand,
        mask_blur=args.mask_blur,
        seed=actual_seed,
        save_mask=True,  # í•­ìƒ ë§ˆìŠ¤í¬ ì €ì¥
        use_source_size=not args.use_background_size,
        include_hair=not args.no_hair,
        include_neck=args.include_neck,
        auto_detect_gender=not args.no_gender_detect,
        face_blend_weight=args.face_blend_weight,
        hair_blend_weight=args.hair_blend_weight,
        mask_padding=args.mask_padding,
        run_folder=run_folder,
        stop_at=args.stop_at,
        shortcut_scale=args.shortcut_scale,
        save_preview=args.save_preview,
        use_pre_paste=args.use_pre_paste,
        pre_paste_denoising=args.pre_paste_denoising,
        use_face_swap=args.use_face_swap,
        use_face_enhance=args.use_face_enhance,
        face_enhance_strength=args.face_enhance_strength,
        use_swap_refinement=args.use_swap_refinement,
        swap_refinement_strength=args.swap_refinement_strength
    )

    # íŒŒë¼ë¯¸í„° ì €ì¥
    save_run_params(run_folder, args, command, actual_seed, background_path, face_path, final_prompt)

    if result and args.show:
        result.show()


if __name__ == "__main__":
    import sys

    if len(sys.argv) > 1:
        main()
    else:
        # ëŒ€í™”í˜• ëª¨ë“œ
        print("=" * 70)
        print("ìë™ ì–¼êµ´ í•©ì„± (ë§ˆìŠ¤í¬ ìë™ ìƒì„±)")
        print("=" * 70)
        print("\nğŸ’¡ í•„ìš”í•œ ê²ƒ: ë°°ê²½ ì´ë¯¸ì§€ + ì–¼êµ´ ì´ë¯¸ì§€ (2ê°œë§Œ!)\n")

        background = input("ë°°ê²½ ì´ë¯¸ì§€ ê²½ë¡œ: ").strip()
        face = input("í•©ì„±í•  ì–¼êµ´ ê²½ë¡œ: ").strip()

        if not os.path.exists(background):
            print(f"âŒ ë°°ê²½ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {background}")
            sys.exit(1)
        if not os.path.exists(face):
            print(f"âŒ ì–¼êµ´ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {face}")
            sys.exit(1)

        prompt = input("í”„ë¡¬í”„íŠ¸ (Enter=ê¸°ë³¸ê°’): ").strip()
        if not prompt:
            prompt = "professional portrait, natural expression"

        compositor = AutoIDPhotoCompositor()

        if compositor.has_ip_adapter:
            result = compositor.composite_face_auto(
                background_path=background,
                source_face_path=face,
                prompt=prompt,
                save_mask=True
            )
            if result:
                result.show()
